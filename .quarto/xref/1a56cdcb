{"entries":[],"headings":["the-problem-with-modeling-long-sequences","capturing-data-dependencies-with-attention-mechanisms","attending-to-different-parts-of-the-input-with-self-attention","a-simple-self-attention-mechanism-without-trainable-weights","computing-attention-weights-for-all-input-tokens","generalize-to-all-input-sequence-tokens","implementing-self-attention-with-trainable-weights","computing-the-attention-weights-step-by-step","implementing-a-compact-selfattention-class","hiding-future-words-with-causal-attention","applying-a-causal-attention-mask","masking-additional-attention-weights-with-dropout","implementing-a-compact-causal-self-attention-class","extending-single-head-attention-to-multi-head-attention","stacking-multiple-single-head-attention-layers","implementing-multi-head-attention-with-weight-splits","summary-and-takeaways"]}