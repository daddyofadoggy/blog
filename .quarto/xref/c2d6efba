{"entries":[],"headings":["introduction","what-is-distributed-data-parallelism","the-core-idea","the-math-behind-it","setting-up-the-environment","auto-imported-variables","the-get-utility","building-ddp-from-scratch","step-1-the-constructor---ensuring-model-synchronization","step-2-adding-forward-pass-methods","step-3-the-heart-of-ddp---gradient-synchronization","putting-it-all-together-performance-comparison","single-gpu-baseline","ddp-with-2-gpus","key-insight","advanced-feature-gradient-accumulation","the-challenge-with-ddp","the-solution-conditional-syncing","using-gradient-accumulation","dataset-characteristics","profiling","key-observations","bottlenecks-and-solutions","code-walkthrough","imports-and-setup-lines-1-14","the-simpledistributeddataparallelism-class-lines-16-42","initialization-init-lines-17-27","gradient-synchronization-lines-29-33","helper-methods-lines-35-42","data-preparation-lines-44-68","data-sharding---the-critical-part-lines-87-98","model-setup-lines-109-112","profiler-setup-lines-114-136","understanding-the-profiler-schedule","understanding-the-profile-configuration","visualizing-profiler-data-with-tensorboard-remote-setup","the-training-loop-lines-138-161","step-1-move-data-to-gpu-lines-143-144","step-2-forward-pass-lines-146-147","step-3-backward-pass-lines-148-149","step-4-synchronize-gradients-lines-151-152","step-5-update-model-lines-154-158","cleanup-lines-160-163","the-big-picture-how-ddp-works","the-ddp-workflow","why-ddp-is-powerful","key-concepts-recap","summary"]}