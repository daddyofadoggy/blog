{"entries":[],"headings":["set-up","create-a-virtual-environment","working-with-text-data","understanding-word-embeddings","tokenizing-text","converting-tokens-into-token-ids","adding-special-context-tokens","bytepair-encoding","data-sampling-with-a-sliding-window","creating-token-embeddings","encoding-word-positions","summary-and-takeaways","coding-attention-mechanisms","the-problem-with-modeling-long-sequences","capturing-data-dependencies-with-attention-mechanisms","attending-to-different-parts-of-the-input-with-self-attention","a-simple-self-attention-mechanism-without-trainable-weights","computing-attention-weights-for-all-input-tokens","generalize-to-all-input-sequence-tokens","implementing-self-attention-with-trainable-weights","computing-the-attention-weights-step-by-step","implementing-a-compact-selfattention-class","hiding-future-words-with-causal-attention","applying-a-causal-attention-mask","masking-additional-attention-weights-with-dropout","implementing-a-compact-causal-self-attention-class","extending-single-head-attention-to-multi-head-attention","stacking-multiple-single-head-attention-layers","implementing-multi-head-attention-with-weight-splits","summary-and-takeaways-1","implementing-a-gpt-model-from-scratch-to-generate-text","coding-an-llm-architecture","normalizing-activations-with-layer-normalization","implementing-a-feed-forward-network-with-gelu-activations","adding-shortcut-connections","connecting-attention-and-linear-layers-in-a-transformer-block","coding-the-gpt-model","generating-text","summary-and-takeaways-2","pretraining-on-unlabeled-data","chapter-5-pretraining-on-unlabeled-data","evaluating-generative-text-models","using-gpt-to-generate-text","calculating-the-text-generation-loss-cross-entropy-and-perplexity","calculating-the-training-and-validation-set-losses","training-an-llm","decoding-strategies-to-control-randomness","temperature-scaling","top-k-sampling","modifying-the-text-generation-function","loading-and-saving-model-weights-in-pytorch","loading-pretrained-weights-from-openai","summary-and-takeaways-3","finetuning-for-text-classification","different-categories-of-finetuning","preparing-the-dataset","creating-data-loaders","initializing-a-model-with-pretrained-weights","adding-a-classification-head","calculating-the-classification-loss-and-accuracy","finetuning-the-model-on-supervised-data","using-the-llm-as-a-spam-classifier","summary-and-takeaways-4","finetuning-to-follow-instruction","introduction-to-instruction-finetuning","preparing-a-dataset-for-supervised-instruction-finetuning","organizing-data-into-training-batches","creating-data-loaders-for-an-instruction-dataset","loading-a-pretrained-llm","finetuning-the-llm-on-instruction-data","extracting-and-saving-responses","evaluating-the-finetuned-llm","conclusions","whats-next","staying-up-to-date-in-a-fast-moving-field","final-words","summary-and-takeaways-5","whats-next-1","dpo-for-llm-alignment","deployment-in-hf-hub-using-gradio","fine-tuned-gpt-model-demo","try-the-demo-above-or-visit-the-full-hugging-face-space-for-the-best-experience.","key-technical-achievements","references"]}