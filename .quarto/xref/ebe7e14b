{"entries":[],"headings":["introduction","challenges-in-distributed-training-vs.-single-gpu-training","memory-constraints","single-gpu-training","distributed-training-challenges","communication-vs.-computation-trade-off","gradient-accumulation-and-synchronization","single-gpu-approach","distributed-training-synchronization","memory-sharding-strategies","distributed-training-approaches","baseline-single-gpu-training","data-parallel-ddp","concept","implementation","key-implementation-detail-gradient-synchronization-with-no_sync","communication-pattern","performance-metrics","advantages-limitations","fully-sharded-data-parallel---full-sharding-fsdp-full-zero-3","concept-1","implementation-1","communication-pattern-1","memory-breakdown-per-gpu-2-gpus-total","performance-metrics-1","advantages-trade-offs","fully-sharded-data-parallel---gradient-sharding-fsdp-grad-zero-2","concept-2","implementation-2","why-fsdp-grad-excels","communication-pattern-2","memory-breakdown-per-gpu-2-gpus-total-1","performance-metrics-2","advantages-trade-offs-1","performance-analysis","throughput-analysis","single-gpu-batch-size-scaling","distributed-training-speedup","memory-profiling-analysis","memory-usage-patterns-baseline-single-gpu","active-memory-timeline-memory-usage-across-training","active-cache-segment-timeline-memory-fragmentation-view","allocator-state-history-detailed-fragmentation-analysis","cross-analysis-memory-usage-vs-communication-overhead","profiling-results-compute-vs.-communication-breakdown","summary-comparison-table","key-metrics","overhead-vs.-baseline","communication-computation-overlap-analysis","overlap-percentage","why-fsdp_grad-achieves-98-overlap","operator-level-profiling-what-changes-between-strategies","baseline-vs.-ddp-adding-gradient-synchronization","ddp-vs.-fsdp_full-switching-to-parameter-sharding","ddp-vs.-fsdp_grad-minimal-communication-pattern","communication-pattern-summary","recommendations","key-findings","strategy-selection-guide","choose-fsdp-grad-zero-2-when","choose-fsdp-full-zero-3-when","choose-ddp-when","stay-with-single-gpu-when","performance-optimization-insights","scaling-beyond-2-gpus","expected-scaling-behavior","final-recommendations","for-this-experiment-gpt-2-large-2x-h100","monitoring-metrics","configuration-checklist","fsdp1-vs-fsdp2-what-changed","fsdp1-legacy-api","fsdp2-new-api","fsdp1-to-fsdp2-migration-mapping","key-migration-steps","conclusion","appendix-running-the-experiments","single-gpu-baseline","ddp-training","fsdp-training"]}