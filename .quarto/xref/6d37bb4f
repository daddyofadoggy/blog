{"entries":[],"headings":["coding-an-llm-architecture","normalizing-activations-with-layer-normalization","implementing-a-feed-forward-network-with-gelu-activations","adding-shortcut-connections","connecting-attention-and-linear-layers-in-a-transformer-block","coding-the-gpt-model","generating-text","summary-and-takeaways"]}