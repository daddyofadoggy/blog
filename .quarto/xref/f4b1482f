{"entries":[],"headings":["introduction-the-memory-wall-problem","the-paradox-of-model-training","the-hidden-memory-costs","why-current-solutions-fall-short","enter-zero-zero-redundancy-optimizer","what-youll-learn-in-this-blog","background-where-does-memory-go-in-deep-learning","model-states-the-primary-memory-consumer","mixed-precision-training-primer","memory-breakdown-with-adam-optimizer","concrete-example-our-2.3b-parameter-model","residual-states-the-secondary-memory-consumers","activations-the-hidden-giant","temporary-buffers-communication-overhead","memory-fragmentation-the-silent-killer","total-memory-picture","our-experimental-setup-a-reproducible-testbed","the-redundancy-problem-in-data-parallelism","zero-foundations-three-stages-of-optimization","mathematical-framework-memory-savings","visual-understanding-memory-consumption-across-stages","zero-1-optimizer-state-partitioning-p_os","how-it-works","communication-pattern","our-experimental-results-zero-1","zero-2-gradient-partitioning-p_osg","how-it-works-1","the-reduce-scatter-operation","implementation-detail-gradient-hooks","our-experimental-results-zero-2","when-zero-2-shines","zero-3-parameter-partitioning-p_osgp","how-it-works-2","parameter-lifecycle","implementation-zero3parammanager","hook-registration","our-experimental-results-zero-3","profiler-deep-dive-understanding-zero-through-execution-traces","zero-1-profiler-analysis-baseline-vs-optimizer-sharding","overview","operator-breakdown","kernel-execution","peak-memory-timeline","memory-operator-view","zero-2-profiler-analysis-gradient-sharding-impact","overview-1","operator-breakdown-1","kernel-execution-1","peak-memory-timeline-1","memory-operator-view-1","zero-3-profiler-analysis-full-sharding-under-the-hood","overview-2","operator-breakdown-2","kernel-execution-2","peak-memory-timeline-2","memory-operator-view-2","comparative-profiler-insights","communication-pattern-summary","comparative-analysis-choosing-the-right-zero-stage","memory-savings-comparison","our-experimental-results-2.3b-params-2-gpus","communication-overhead-analysis","communication-patterns","communication-overhead-vs-model-size","scalability-comparison","memory-scaling-with-number-of-gpus","maximum-trainable-model-size","why-zero-2-can-be-a-free-lunch-and-why-you-should-start-there","the-communication-volume-paradox","understanding-the-48.6-overhead","when-zero-2-becomes-free","the-free-lunch-argument","why-start-with-zero-2-not-zero-1","the-new-recommendation","theoretical-foundation","practical-validation","decision-framework-which-stage-should-you-use","based-on-model-size","based-on-hardware-configuration","based-on-batch-size-constraints","quick-start-recommendation","implementation-deep-dive","project-structure","zero-1-optimizer-state-partitioning","the-shardedoptimizer-class","parameter-sharding-strategy","removing-non-local-parameters","the-training-step","profiling-integration","zero-2-adding-gradient-sharding","gradient-hooks","registering-hooks","reduce-scatter-for-gradients","why-48.6-communication-overhead","zero-3-full-parameter-sharding","the-zero3parammanager","materialize-gathering-shards","release-keeping-only-local-shard","forward-and-backward-hooks","parameter-initialization-with-shards","gradient-all-reduce","memory-tracking-utilities","calculating-memory-usage","optimizer-state-memory","complete-memory-report","distributed-training-helpers","reproducibility","distributed-context-helper","training-loop-anatomy","setup-phase","warmup-step","memory-profiling-loop","results-reporting","key-implementation-patterns","pattern-1-wrapping-native-optimizers","pattern-2-lazy-materialization-zero-3","pattern-3-communication-timing","pattern-4-gradient-hooks-for-memory-management","common-pitfalls-and-solutions","pitfall-1-forgetting-torch.cuda.synchronize","pitfall-2-hooks-with-lambda-closures","pitfall-3-materialize-without-release-zero-3","pitfall-4-incorrect-shard-ownership-calculation","code-comparison-across-zero-stages","extending-the-code","extension-1-activation-checkpointing","extension-2-mixed-precision-training","extension-3-offloading-to-cpu","performance-optimization-tips","tip-1-overlap-communication-with-computation","tip-2-fused-optimizers","tip-3-bucketing-gradients","debugging-distributed-training","technique-1-enable-nccl-debug-logs","technique-2-rank-specific-logging","technique-3-gradient-verification","summary-from-theory-to-practice","running-your-own-experiments","prerequisites","hardware-requirements","software-requirements","network-requirements","environment-setup","clone-the-repository","create-virtual-environment","install-dependencies","verify-installation","running-zero-1","basic-execution","expected-output","viewing-profiler-traces","comparing-all-three-stages","run-all-stages-in-sequence","extracting-results","advanced-experiments","measuring-bandwidth-utilization","profiling-with-different-profiler-settings","testing-with-real-models","experiment-ideas","scaling-study","communication-vs.-computation-trade-off","next-steps","validation-checklist","summary","findings-and-conclusion","key-findings","memory-efficiency-achievements","communication-overhead-trade-offs","profiler-insights","memory-consumption-fundamentals","when-to-use-each-zero-stage","practical-recommendations","implementation-best-practices","hardware-requirements-1","performance-optimization","broader-impact","conclusion","references","primary-literature","implementation-code","related-work-background","tools-frameworks"]}