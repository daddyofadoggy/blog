{"title":"Introduction","markdown":{"headingText":"Introduction","containsRefs":false,"markdown":"\n\nWelcome! In this blog, we'll explore Distributed Data Parallelism (DDP), a powerful technique for training deep learning models faster by using multiple GPUs. If you've ever trained a large model and wished it could go faster, DDP is one of the best tools to achieve that speedup.\n\nDon't worry if you're new to distributed training - we'll break everything down step by step, starting from the core concepts and building up to a working implementation of Pytorch's `DistributedDataParallel`\n\n\n## What is Distributed Data Parallelism?\n\nBefore diving into the code, let's understand the fundamental concept. In distributed training, device refers to GPU and host refers to CPU.\n\n### The Core Idea\n\nImagine you're a teacher grading 100 homework assignments. You could\n\n- **Option A**: Grade all 100 assignments yourself (slow!)\n- **Option B**: Split the assignments among 4 teaching assistants, each grades 25 assignments (4x faster!)\n\nIn the First Phase, Distributed Data Parallel begins with the entire batch of data being divided into equal partitions across devices. Each partition is processed independently by identical model replicas running on separate GPUs, with each performing its own forward pass computation. Following the forward pass, each model calculates its own loss value based solely on its data partition, which then initiates the backward pass where gradients are computed independently on each device.\n\nAfter local gradient computation, DDP executes its most critical operation—the all-reduce synchronization—where gradients from all devices are averaged, ensuring each model receives the same update signal as if it had processed the entire batch. With synchronized gradients in hand, each model's optimizer applies identical parameter updates, maintaining perfect weight consistency across all replicas. This coordinated update completes one training iteration, and the process repeats with new data partitions in the next step, preserving model equivalence throughout training. To illustrate the DDP process I have attached a diagram below. I borrowed it from Zach's Scratch to Scale cohort and one of the best diagrams I've ever seen on DDP\n\n<div style=\"text-align: center;\">\n\n![DDP Architecture Diagram (ref: Scratch to Scale)](assets/ddp_diagram.png)\n\n</div>\n\nDistributed Data Parallel delivers remarkable efficiency through its balanced approach to parallelism, offering near-linear scaling with increasing GPU count while maintaining mathematical equivalence to single-GPU training. The communication overhead is minimized by exchanging only gradients rather than activations or weights, utilizing highly optimized all-reduce operations that leverage ring-based algorithms. DDP's elegant simplicity makes it the preferred parallelization strategy for most deep learning tasks, providing substantial speedups without the complexity of model parallelism approaches.\n\n**Key Points:**\n\n- We **DON'T** split the model across GPUs (the model stays whole)\n- We **DO** split the training data across GPUs\n- Each GPU has a complete copy of the model\n- Each GPU processes a different subset of data\n- At the end of each step, we average the gradients from all GPUs\n\n\n### The Math Behind It\n\nGiven `n` GPUs, here's what happens:\n\n```\nB_i = B/n     → Each GPU gets a mini-batch of size B/n\ng = (1/n) Σ g_i   → Gradients from all GPUs are averaged\nθ_i = θ_i - g     → Each GPU updates its model using the averaged gradient\n```\n\nIn plain English:\n1. Split your batch of data across all GPUs\n2. Each GPU computes gradients on its portion\n3. Average all the gradients together\n4. Update the model parameters on each GPU\n\nThe beauty of DDP is that it only requires **one communication step** - the gradient averaging. This makes it very efficient!\n\n\n## Setting Up the Environment\n\n### Auto-imported Variables\n\nThe environment automatically provides:\n- `rank` - The ID of the current process (0 or 1)\n- `world_size` - Total number of processes (2 in this case)\n- `gpu_id` - The specific GPU assigned to this process\n- `device` - The PyTorch device object for this GPU\n\n### The `get()` Utility\n\nWe have introduced a handy utility function `get()` for accessing distributed information:\n\n```python\nget(\"ws\")      # → world_size (number of GPUs)\nget(\"rank\")    # → current process rank\nget(\"grank\")   # → global rank\nget(\"lrank\")   # → local rank\n```\n\nTo understand `get`, we need to dig into `cache_mesh` Class - A Function Decorator with State.\n\n```python \nclass cache_mesh:\n    def __init__(self, func):\n        self.func = func        # Store the decorated function\n        self._mesh = None       # Initialize mesh cache as None\n\n    def __call__(self, str, dm: dist.device_mesh.DeviceMesh = None):\n        mesh = self._mesh if dm is None else dm     # If no device mesh (dm) is provided, it uses the cached mesh (self._mesh)\n        return self.func(str, mesh)                 # It calls the original function with the string argument and the determined mesh\n\n    def register_mesh(self, mesh: dist.device_mesh.DeviceMesh):\n        self._mesh = mesh\n        return self\n\n```\nNow we are going to declare the `get` function is decorated with @cache_mesh, transforming it into an instance of the cache_mesh class. This allows it to use a cached device mesh when none is provided.\n\n```python\n@cache_mesh\ndef get(str, dm: dist.device_mesh.DeviceMesh = None):\n    \"\"\"\n    Applies a func to get whatever is requested.\n\n    `ws` -> dist.get_world_size(pg)\n    `pg` -> dist.get_process_group()\n    `rank` -> dist.get_rank(pg) # global\n    `grank` -> dist.get_rank(pg) # global\n    `lrank` -> local_rank\n    \"\"\"\n\n    pg = dm.get_group() if dm else None\n\n    match str:\n        case \"ws\":\n            return dist.get_world_size(pg)\n        case \"pg\":\n            return pg\n        case \"rank\" | \"grank\":\n            return dist.get_rank(pg)\n        case \"lrank\":\n            return dm.get_local_rank() if dm else int(os.environ.get(\"LOCAL_RANK\", 0))\n        case _:\n            raise ValueError(f\"Invalid string: {str}\")\n\n```\nHere is an example of how to use it in practice \n\n```python\n# In setup code, register a mesh once\ndevice_mesh = dist.DeviceMesh(\"cuda\", [[0, 1, 2, 3]])  # Create a mesh with 4 GPUs\nget.register_mesh(device_mesh)\n\n# Later, easily access distributed info without passing the mesh each time\nworld_size = get(\"ws\")       # Uses cached mesh\nmy_rank = get(\"rank\")        # Uses cached mesh\nlocal_rank = get(\"lrank\")    # Uses cached mesh\n\n# Or override with a specific mesh when needed\nspecific_mesh = dist.DeviceMesh(\"cuda\", [[0, 1]])\nother_world_size = get(\"ws\", specific_mesh)  # Uses specific mesh\n```\nAlternatively, we can use `nbdistributed` [plugin] (https://muellerzr.github.io/scratch-to-scale/01_intro_to_jupyter.html ) and then \n\n```python\n%load_ext nbdistributed\n```\n\n```python\n%dist_init --num-processes 2 --gpu-ids 1,2\n```\nThis creates:\n- **Rank 0** → Worker on GPU 1\n- **Rank 1** → Worker on GPU 2\n\nEach \"rank\" is essentially a separate process handling one GPU.\n\n## Building DDP from Scratch\n\nNow comes the exciting part - implementing DDP ourselves to understand how it works!\n\n### Step 1: The Constructor - Ensuring Model Synchronization\n\nThe first challenge: we need to ensure all GPUs start with the **exact same model**. If they don't, the training will diverge and produce incorrect results.\n\n```python\nclass SimpleDistributedDataParallelism:\n    def __init__(self, model: torch.nn.Module):\n        self.model = model\n\n        # Verify all GPUs have identical model parameters\n        for param in model.parameters():\n            rank0_param = param.data.clone()\n            dist.broadcast(rank0_param, src=0)  # Broadcast from rank 0\n            if not torch.equal(param.data, rank0_param):\n                raise ValueError(\n                    \"Expected model parameters to be identical during `__init__`, \"\n                    \"but this is not true. Make sure to set the seeds before creating your model\"\n                )\n```\n\n**What's happening here?**\n\n1. For each parameter in the model:\n   - Rank 0 broadcasts its parameter value to all other ranks\n   - Each rank compares its local parameter to rank 0's parameter\n   - If there's any mismatch, we raise an error\n\n2. Why do we need this?\n   - Random initialization could give different starting weights on each GPU\n   - Solution: Set the same random seed on all GPUs before creating the model\n\n**Testing the verification:**\n\nThe notebook demonstrates this by intentionally setting different seeds:\n\n```python\n%%rank [0]\nset_seed(43)  # Rank 0 uses seed 43\n\n# Rank 1 still uses default seed\n# Result: ValueError when trying to create DDP model!\n```\n\nAfter fixing by setting the same seed on all ranks:\n\n```python\nset_seed(43)  # Same seed on all ranks\nmodel = SimpleDistributedDataParallelism(model)  # Success!\n```\n\n### Step 2: Adding Forward Pass Methods\n\nWe need to make our wrapper behave like a normal PyTorch model:\n\n```python\ndef __call__(self, *args, **kwargs):\n    return self.model(*args, **kwargs)\n\ndef train(self):\n    self.model.train()\n\ndef eval(self):\n    self.model.eval()\n```\n\nThese methods simply delegate to the wrapped model, making our DDP class transparent to use.\n\n### Step 3: The Heart of DDP - Gradient Synchronization\n\nThis is where the magic happens! After computing gradients on each GPU's subset of data, we need to average them across all GPUs.\n\n**The Problem Without Synchronization:**\n\nThe notebook shows what happens if we train without syncing:\n\n```python\n# Each GPU processes different data\nitem = dataset[get(\"rank\")]  # Rank 0 gets item 0, Rank 1 gets item 1\n\n# Train without syncing\noutput = model(**item)\noutput.loss.backward()\noptimizer.step()\n\n# Check if parameters match across GPUs\n# Result: ValueError - parameters are different!\n# Max difference: 0.00390625\n```\n\nThe GPUs diverged because they updated their models differently!\n\n**The Solution - `sync_gradients()` Method:**\n\n```python\ndef sync_gradients(self):\n    \"\"\"\n    Should be called after the backward pass.\n    Averages gradients across all GPUs.\n    \"\"\"\n    for param in self.model.parameters():\n        if param.grad is not None:\n            # Sum gradients from all GPUs\n            dist.all_reduce(param.grad, op=dist.ReduceOp.SUM)\n            # Divide by number of GPUs to get average\n            param.grad /= dist.get_world_size()\n```\n\n**How it works:**\n\n1. `dist.all_reduce()` - A collective communication operation that:\n   - Gathers the gradient tensor from all GPUs\n   - Applies an operation (SUM in our case)\n   - Returns the result to all GPUs\n\n2. We divide by `world_size` to convert the sum into an average\n\n3. After this, all GPUs have the **same averaged gradient** and will update identically\n\n**The Corrected Training Loop:**\n\n```python\noutput = model(**item)\noutput.loss.backward()\nddp_model.sync_gradients()  # Critical step!\noptimizer.step()\noptimizer.zero_grad()\n\n# Verify parameters match across GPUs\n# Result: Success - all parameters identical!\n```\n\n## Putting It All Together: Performance Comparison\n\nHere is how a simple DDP class looks like \n\n```python\nclass SimpleDistributedDataParallelism:\n    def __init__(self, model:torch.nn.Module):\n        self.model = model\n\n        for param in model.parameters():\n            rank0_param = param.data.clone()\n            dist.broadcast(rank0_param, src=0)\n            if not torch.equal(param.data, rank0_param):\n                raise ValueError(\n                    \"Expected model parameters to be identical during `__init__`, but this is not true. \"\n                    \"Make sure to set the seeds before creating your model\"\n                )\n\n    def sync_gradients(self):\n        \"\"\"\n        Should be called before the backward pass, iterates \n        through all params, and:\n        1. Check if it is `None` (not trainable)\n        2. If trainable, will perform an `all_reduce` using `SUM`\n        (aka: take the global average of all grads)\n        \"\"\"\n        for param in self.model.parameters():\n            if param.grad is not None:\n                dist.all_reduce(param.grad, op=dist.ReduceOp.SUM)\n                param.grad /= dist.get_world_size()\n    \n    def __call__(self, *args, **kwargs):\n        return self.model(*args, **kwargs)\n    \n    def train(self):\n        self.model.train()\n    \n    def eval(self):\n        self.model.eval()\n```\n\nNow let's see the speedup in action!\n\n### Single GPU Baseline\n\nTraining on a single GPU (Rank 0 only):\n```python\nper_device_batch_size = 16  # Batch size of 16\n\n# Results:\n# Total training time: 1.58 seconds\n# Average time per batch: 0.0751 seconds\n```\n\n### DDP with 2 GPUs\n\nNow let's distribute the training:\n\n1. **Data Sharding**: Split the dataset across GPUs\n   ```python\n   ds_length_per_rank = len(dataset) // world_size\n   rank = get(\"rank\")\n   start = rank * ds_length_per_rank\n   end = start + ds_length_per_rank\n   train_shard = dataset.select(range(start, end))\n   ```\n\n2. **Smaller per-device batch size**: Since we're using 2 GPUs\n   ```python\n   per_device_batch_size = 8  # 8 per GPU = 16 total (same as single GPU)\n   ```\n\n3. **Results:**\n   ```\n   Rank 0: 1.13 seconds, 0.0540 seconds/batch\n   Rank 1: 1.16 seconds, 0.0551 seconds/batch\n   ```\n\n### Key Insight\n\nWith 2 GPUs, we can train with an **effective global batch size of 16** (8 per GPU) in approximately **the same time** it took to train with batch size 8 on a single GPU!\n\nThis means:\n- We effectively **doubled our throughput**\n- The communication overhead (gradient averaging) is minimal\n- We could even increase to a global batch size of 32 (16 per GPU) for even faster training\n\n## Advanced Feature: Gradient Accumulation\n\nSometimes you want to train with a very large batch size, but it won't fit in GPU memory. The solution is **gradient accumulation** - accumulate gradients over multiple micro-batches before updating.\n\n### The Challenge with DDP\n\nWith gradient accumulation, we don't want to sync gradients after every micro-batch - that would be wasteful! We only want to sync when we're ready to actually update the model.\n\n### The Solution: Conditional Syncing\n\n```python\nclass SimpleDistributedDataParallelism:\n    def __init__(self, model: torch.nn.Module):\n        self.model = model\n        self.enable_grad_sync()  # Start with syncing enabled\n        # ... (initialization code)\n\n    def sync_gradients(self):\n        if not self.do_sync:\n            return  # Skip syncing if disabled\n        # ... (sync code)\n\n    def enable_grad_sync(self):\n        self._do_sync = True\n\n    def disable_grad_sync(self):\n        self._do_sync = False\n\n    @contextmanager\n    def no_sync(self):\n        \"\"\"Context manager to temporarily disable gradient syncing.\"\"\"\n        prev = self.do_sync\n        self.disable_grad_sync()\n        try:\n            yield\n        finally:\n            self._do_sync = prev\n```\n\n### Using Gradient Accumulation\n\n```python\ngrad_accum_steps = 4  # Accumulate over 4 micro-batches\n\nfor i, batch in enumerate(dataloader):\n    # Only sync on the last accumulation step\n    if i % grad_accum_steps == 0:\n        ddp_model.enable_grad_sync()\n    else:\n        ddp_model.disable_grad_sync()\n\n    output = ddp_model(batch)\n    output.loss.backward()\n\n    # Only update when syncing is enabled\n    if ddp_model.do_sync:\n        ddp_model.sync_gradients()\n        optimizer.step()\n        optimizer.zero_grad()\n```\n\nThis way, we only communicate gradients every 4 steps instead of every step, reducing communication overhead!\n\n## Dataset Characteristics\nFor dataset, we used GLUE MRPC Dataset. Here is a brief description of the dataset\n\n- **Task Type:** Sentence pair classification (paraphrase identification)\n- **Description:** The MRPC dataset contains pairs of sentences automatically extracted from online news sources with human annotations indicating whether they are semantically equivalent (paraphrases) or not\n- **Size:**\n    - Training set: 3,668 sentence pairs\n    - Validation set: 408 sentence pairs\n    - Test set: 1,725 sentence pairs\n- **Labels:** Binary classification\n    - 0: not_equivalent\n    - 1: equivalent\n\nHere's an example from the dataset:\n```python\n{\n  'idx': 0,\n  'label': 1,\n  'sentence1': 'Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence.',\n  'sentence2': 'Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence.'\n}\n```\nFor model, we used a small 360M `HuggingFaceTB/SmolLM2-360M-Instruct` model.\n\n\n## Profiling \nWe used `torch.profiler` to check traces, kernel and memory footprint . We ran the distributed module using a 4 H100 SXM5 GPU instatance in `LambdaLab`.  \n\n<div style=\"text-align: center;\">\n\n![Profiier Execution Summary](assets/profiler_overview_1.png)\n\n</div>\n\n<div style=\"text-align: center;\">\n\n![Profiier Operator Summary](assets/profiler_operator.png)\n\n</div>\n\n<div style=\"text-align: center;\">\n\n![Profiier Kernel Summary](assets/profiler_kernel.png)\n\n</div>\n\n\n<div style=\"text-align: center;\">\n\n![Profiier Traces ](assets/profiler_trace.png)\n\n</div>\n\n### Key Observations\n\n- **Very low GPU utilization (15.19%)** - This is extremely low for H100 GPUs, indicating significant inefficiency\n- **SM Efficiency (11.08%)** - This suggests your kernels aren't fully utilizing the streaming multiprocessors\n- **Occupancy (28.73%)** - The low occupancy indicates your kernels aren't keeping the GPU busy\n- **CPU Execution dominates (61.1%)** of the step time\n- **Kernel execution (15.2%)** is relatively small\n- **Communication overhead (20.9%)** is significant but expected in DDP\n\n### Bottlenecks and Solutions\n\n- The AllReduce operation (42.2%) dominates kernel time, which is expected in DDP but appears to be taking too much relative time\n    - Solution: Gradient Accumulation\n- Unused Tensorcore as we see it in the Profiler\n    - Solution: Mixed Precision Training to enable tensorcore \n- We can try to Increase batch size until memory limits to increase throughput\n\nWe have experimented with Gradient Accumulation with step size 2 and AllReduce operation  reduced to 25% in Kermel profiler.\n\n<div style=\"text-align: center;\">\n\n![Profiier Kernel Summary after Gradient Accumulation](assets/profiler_kernel_grad_ac.png)\n\n</div>\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../../styles.css"],"output-file":"report_ddp.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":["cosmo","brand"],"title-block-banner":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}