{"title":"6.1 Different categories of finetuning","markdown":{"headingText":"6.1 Different categories of finetuning","containsRefs":false,"markdown":"\n<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/chapter-overview.webp\" width=500px>\n\n\n- No code in this section\n\n- The most common ways to finetune language models are instruction-finetuning and classification finetuning\n- Instruction-finetuning, depicted below, is the topic of the next chapter\n\n<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/instructions.webp\" width=500px>\n\n- Classification finetuning, the topic of this chapter, is a procedure you may already be familiar with if you have a background in machine learning -- it's similar to training a convolutional network to classify handwritten digits, for example\n- In classification finetuning, we have a specific number of class labels (for example, \"spam\" and \"not spam\") that the model can output\n- A classification finetuned model can only predict classes it has seen during training (for example, \"spam\" or \"not spam\"), whereas an instruction-finetuned model can usually perform many tasks\n- We can think of a classification-finetuned model as a very specialized model; in practice, it is much easier to create a specialized model than a generalist model that performs well on many different tasks\n\n<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/spam-non-spam.webp\" width=500px>\n\n## 6.2 Preparing the dataset\n\n<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/overview-1.webp\" width=500px>\n\n- This section prepares the dataset we use for classification finetuning\n- We use a dataset consisting of spam and non-spam text messages to finetune the LLM to classify them\n- First, we download and unzip the dataset\n\n- The dataset is saved as a tab-separated text file, which we can load into a pandas DataFrame\n\n- When we check the class distribution, we see that the data contains \"ham\" (i.e., \"not spam\") much more frequently than \"spam\"\n\n- For simplicity, and because we prefer a small dataset for educational purposes anyway (it will make it possible to finetune the LLM faster), we subsample (undersample) the dataset so that it contains 747 instances from each class\n- (Next to undersampling, there are several other ways to deal with class balances, but they are out of the scope of a book on LLMs; you can find examples and more information in the [`imbalanced-learn` user guide](https://imbalanced-learn.org/stable/user_guide.html))\n\n- Next, we change the string class labels \"ham\" and \"spam\" into integer class labels 0 and 1:\n\n- Let's now define a function that randomly divides the dataset into training, validation, and test subsets\n\n## 6.3 Creating data loaders\n\n- Note that the text messages have different lengths; if we want to combine multiple training examples in a batch, we have to either\n  1. truncate all messages to the length of the shortest message in the dataset or batch\n  2. pad all messages to the length of the longest message in the dataset or batch\n\n- We choose option 2 and pad all messages to the longest message in the dataset\n- For that, we use `<|endoftext|>` as a padding token, as discussed in chapter 2\n\n<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/pad-input-sequences.webp?123\" width=500px>\n\n- The `SpamDataset` class below identifies the longest sequence in the training dataset and adds the padding token to the others to match that sequence length\n\n- We also pad the validation and test set to the longest training sequence\n- Note that validation and test set samples that are longer than the longest training example are being truncated via `encoded_text[:self.max_length]` in the `SpamDataset` code\n- This behavior is entirely optional, and it would also work well if we set `max_length=None` in both the validation and test set cases\n\n- Next, we use the dataset to instantiate the data loaders, which is similar to creating the data loaders in previous chapters\n\n<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/batch.webp\" width=500px>\n\n- As a verification step, we iterate through the data loaders and ensure that the batches contain 8 training examples each, where each training example consists of 120 tokens\n\n- Lastly, let's print the total number of batches in each dataset\n\n## 6.4 Initializing a model with pretrained weights\n\n- In this section, we initialize the pretrained model we worked with in the previous chapter\n\n<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/overview-2.webp\" width=500px>\n\n- To ensure that the model was loaded correctly, let's double-check that it generates coherent text\n\n- Before we finetune the model as a classifier, let's see if the model can perhaps already classify spam messages via prompting\n\n- As we can see, the model is not very good at following instructions\n- This is expected, since it has only been pretrained and not instruction-finetuned (instruction finetuning will be covered in the next chapter)\n\n## 6.5 Adding a classification head\n\n<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/lm-head.webp\" width=500px>\n\n- In this section, we are modifying the pretrained LLM to make it ready for classification finetuning\n- Let's take a look at the model architecture first\n\n- Above, we can see the architecture we implemented in chapter 4 neatly laid out\n- The goal is to replace and finetune the output layer\n- To achieve this, we first freeze the model, meaning that we make all layers non-trainable\n\n- Then, we replace the output layer (`model.out_head`), which originally maps the layer inputs to 50,257 dimensions (the size of the vocabulary)\n- Since we finetune the model for binary classification (predicting 2 classes, \"spam\" and \"not spam\"), we can replace the output layer as shown below, which will be trainable by default\n- Note that we use `BASE_CONFIG[\"emb_dim\"]` (which is equal to 768 in the `\"gpt2-small (124M)\"` model) to keep the code below more general\n\n- Technically, it's sufficient to only train the output layer\n- However, as I found in [Finetuning Large Language Models](https://magazine.sebastianraschka.com/p/finetuning-large-language-models), experiments show that finetuning additional layers can noticeably improve the performance\n- So, we are also making the last transformer block and the final `LayerNorm` module connecting the last transformer block to the output layer trainable\n\n<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/trainable.webp\" width=500px>\n\n- We can still use this model similar to before in previous chapters\n- For example, let's feed it some text input\n\n- What's different compared to previous chapters is that it now has two output dimensions instead of 50,257\n\n- As discussed in previous chapters, for each input token, there's one output vector\n- Since we fed the model a text sample with 4 input tokens, the output consists of 4 2-dimensional output vectors above\n\n<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/input-and-output.webp\" width=500px>\n\n- In chapter 3, we discussed the attention mechanism, which connects each input token to each other input token\n- In chapter 3, we then also introduced the causal attention mask that is used in GPT-like models; this causal mask lets a current token only attend to the current and previous token positions\n- Based on this causal attention mechanism, the 4th (last) token contains the most information among all tokens because it's the only token that includes information about all other tokens\n- Hence, we are particularly interested in this last token, which we will finetune for the spam classification task\n\n<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/attention-mask.webp\" width=200px>\n\n## 6.6 Calculating the classification loss and accuracy\n\n<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/overview-3.webp?1\" width=500px>\n\n- Before explaining the loss calculation, let's have a brief look at how the model outputs are turned into class labels\n\n<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/class-argmax.webp\" width=600px>\n\n- Similar to chapter 5, we convert the outputs (logits) into probability scores via the `softmax` function and then obtain the index position of the largest probability value via the `argmax` function\n\n- Note that the softmax function is optional here, as explained in chapter 5, because the largest outputs correspond to the largest probability scores\n\n- We can apply this concept to calculate the so-called classification accuracy, which computes the percentage of correct predictions in a given dataset\n- To calculate the classification accuracy, we can apply the preceding `argmax`-based prediction code to all examples in a dataset and calculate the fraction of correct predictions as follows:\n\n- Let's apply the function to calculate the classification accuracies for the different datasets:\n\n- As we can see, the prediction accuracies are not very good, since we haven't finetuned the model, yet\n\n- Before we can start finetuning (/training), we first have to define the loss function we want to optimize during training\n- The goal is to maximize the spam classification accuracy of the model; however, classification accuracy is not a differentiable function\n- Hence, instead, we minimize the cross-entropy loss as a proxy for maximizing the classification accuracy (you can learn more about this topic in lecture 8 of my freely available [Introduction to Deep Learning](https://sebastianraschka.com/blog/2021/dl-course.html#l08-multinomial-logistic-regression--softmax-regression) class)\n\n- The `calc_loss_batch` function is the same here as in chapter 5, except that we are only interested in optimizing the last token `model(input_batch)[:, -1, :]` instead of all tokens `model(input_batch)`\n\nThe `calc_loss_loader` is exactly the same as in chapter 5\n\n- Using the `calc_closs_loader`, we compute the initial training, validation, and test set losses before we start training\n\n- In the next section, we train the model to improve the loss values and consequently the classification accuracy\n\n## 6.7 Finetuning the model on supervised data\n\n- In this section, we define and use the training function to improve the classification accuracy of the model\n- The `train_classifier_simple` function below is practically the same as the `train_model_simple` function we used for pretraining the model in chapter 5\n- The only two differences are that we now \n  1. track the number of training examples seen (`examples_seen`) instead of the number of tokens seen\n  2. calculate the accuracy after each epoch instead of printing a sample text after each epoch\n\n<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/training-loop.webp?1\" width=500px>\n\n- The `evaluate_model` function used in the `train_classifier_simple` is the same as the one we used in chapter 5\n\n- The training takes about 5 minutes on a M3 MacBook Air laptop computer and less than half a minute on a V100 or A100 GPU\n\n- Similar to chapter 5, we use matplotlib to plot the loss function for the training and validation set\n\n- Above, based on the downward slope, we see that the model learns well\n- Furthermore, the fact that the training and validation loss are very close indicates that the model does not tend to overfit the training data\n- Similarly, we can plot the accuracy below\n\n- Based on the accuracy plot above, we can see that the model achieves a relatively high training and validation accuracy after epochs 4 and 5\n- However, we have to keep in mind that we specified `eval_iter=5` in the training function earlier, which means that we only estimated the training and validation set performances\n- We can compute the training, validation, and test set performances over the complete dataset as follows below\n\n- We can see that the training and validation set performances are practically identical\n- However, based on the slightly lower test set performance, we can see that the model overfits the training data to a very small degree, as well as the validation data that has been used for tweaking some of the hyperparameters, such as the learning rate\n- This is normal, however, and this gap could potentially be further reduced by increasing the model's dropout rate (`drop_rate`) or the `weight_decay` in the optimizer setting\n\n## 6.8 Using the LLM as a spam classifier\n\n<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/overview-4.webp\" width=500px>\n\n- Finally, let's use the finetuned GPT model in action\n- The `classify_review` function below implements the data preprocessing steps similar to the `SpamDataset` we implemented earlier\n- Then, the function returns the predicted integer class label from the model and returns the corresponding class name\n\n- Let's try it out on a few examples below\n\n- Finally, let's save the model in case we want to reuse the model later without having to train it again\n\n- Then, in a new session, we could load the model as follows\n\n## Summary and takeaways\n\n- See the [./gpt_class_finetune.py](./gpt_class_finetune.py) script, a self-contained script for classification finetuning\n- You can find the exercise solutions in [./exercise-solutions.ipynb](./exercise-solutions.ipynb)\n- In addition, interested readers can find an introduction to parameter-efficient training with low-rank adaptation (LoRA) in [appendix E](../../appendix-E)\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../../styles.css"],"output-file":"ch06.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":["cosmo","brand"],"title-block-banner":false,"listing":false,"search":false},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}