{"title":"Day One with DGX Spark: From Setup to Running Local LLMs","markdown":{"yaml":{"title":"Day One with DGX Spark: From Setup to Running Local LLMs","author":"Dipankar Baisya","date":"2025-11-25","categories":["GPU-computing","nvidia-dgx","LLM","deployment","ollama","open-webui"]},"headingText":"Introduction: Meet the DGX Spark","containsRefs":false,"markdown":"\n\n\nThe NVIDIA DGX Spark, launched in October 2025, represents a significant leap in making enterprise-grade AI infrastructure accessible to individual developers and researchers. This compact AI supercomputer packs an impressive 1 petaFLOP of AI performance into a desktop form factor, powered by the NVIDIA GB10 Grace Blackwell Superchip. At its heart, the system features a 20-core Arm processor (10 Cortex-X925 performance cores and 10 Cortex-A725 efficiency cores) paired with 128GB of unified memory that's seamlessly shared between the CPU and GPU. This unified memory architecture is particularly powerful for AI workloads, allowing the system to run inference on models with up to 200 billion parameters and fine-tune models up to 70 billion parameters locally. With 4TB of solid-state storage and dual QSFP Ethernet ports providing 200 Gb/s of aggregate bandwidth, the DGX Spark at $3,999 brings what was once exclusive to data centers right to your desk.\n\n## A Personal Note: From GTX 1080 Ti to Grace Blackwell\n\n My journey with GPUs began back in October 2017, when I was setting up my lab's computer with an NVIDIA GTX 1080 Ti. It was time I was persuing a PhD (1st year). That experience introduced me to the world of GPU-accelerated computing and sparked my interest in leveraging specialized hardware for computational tasks. Fast forward eight years, and the DGX Spark represents my second foray into GPU ownership—though calling it just a \"GPU\" would be a significant understatement. While the GTX 1080 Ti was a powerful graphics card in its time, the DGX Spark is an entirely different beast: a complete AI supercomputer that integrates CPU, GPU, and unified memory into a cohesive system designed specificallyfor AI workloads. The evolution from a single GPU card to this integrated Grace Blackwell architecture reflects not just technological progress, but also the democratization of AI infrastructure that was once accessible only to large research institutions and tech giants\n\n::: {layout-ncol=2 layout-valign=\"top\"}\n![NVIDIA DGX Spark - A complete AI supercomputer featuring the Grace Blackwell architecture, representing eight years of technological evolution](assets/DGX.jpg){height=400}\n\n![NVIDIA GTX 1080 Ti - My first GPU from October 2017, a powerful graphics card that introduced me to GPU-accelerated computing](assets/GTX1080.jpg){height=400}\n\n:::\n\n## Evolution of GPU Computing: A Side-by-Side Comparison\n\nThe eight-year gap between these two systems tells a remarkable story of technological advancement, particularly in the shift from graphics-focused GPUs to AI-specialized computing platforms.\n\n| **Specification** | **GTX 1080 Ti (2017)** | **DGX Spark (2025)** | **Evolution** |\n|-------------------|------------------------|----------------------|---------------|\n| **Architecture** | Pascal (16nm) | Grace Blackwell GB10 (3nm) | Integrated CPU-GPU superchip design |\n| **CPU Cores** | Host system dependent | 20 Arm cores (10 X925 + 10 A725) | Integrated high-performance CPU |\n| **CUDA Cores** | 3,584 | 6,144 | **71% increase** in CUDA cores |\n| **Tensor Cores** | None | 192 (5th generation) | AI-optimized matrix operations |\n| **Memory Capacity** | 11 GB GDDR5X | 128 GB LPDDR5x Unified | **11.6× increase** in capacity |\n| **Memory Architecture** | Dedicated GPU memory | CPU-GPU unified coherent memory | Seamless sharing eliminates data transfer overhead |\n| **Memory Bandwidth** | 484 GB/s | 273 GB/s unified | Coherent memory access across CPU-GPU |\n| **FP32 Performance** | 11.5 TFLOPS | 31 TFLOPS | **2.7× increase** in traditional compute |\n| **AI Performance** | ~11.5 TFLOPS (FP32 only) | 1,000 TOPS (FP4)<br>1 PETAFLOP | **~87× increase** with AI-optimized precision |\n| **Precision Support** | FP32, FP16 | FP32, FP16, FP8, NVFP4, MXFP8 | Multi-precision for optimal AI inference |\n| **Model Capacity** | Limited by 11GB | 200B parameters (inference)<br>70B parameters (fine-tuning) | Native support for frontier models |\n| **Power Consumption** | 250W | 140W TDP (240W with accessories) | Dramatically more efficient |\n| **Primary Use Case** | Gaming & Graphics | AI Development & Inference | Purpose-built for AI/ML workflows |\n| **Price at Launch** | ~$699 | $3,999 | Premium for integrated AI platform |\n| **Form Factor** | PCIe Graphics Card | Complete Desktop System | Self-contained AI workstation |\n\n### Key Architectural Breakthroughs\n\n**Unified Memory Revolution**: The most significant advancement is the shift from discrete GPU memory to unified coherent memory. The GTX 1080 Ti required explicit data transfers between system RAM and GPU memory, creating bottlenecks. The DGX Spark's 128GB unified memory is seamlessly accessible to both CPU and GPU, eliminating these transfers and enabling efficient processing of models that were impossible on traditional GPUs.\n\n**Fifth-Generation Tensor Cores**: Perhaps the most transformative feature is the inclusion of 192 fifth-generation Tensor Cores—technology completely absent from the GTX 1080 Ti. Tensor Cores are specialized processing units designed specifically for the matrix multiplication operations that dominate neural network training and inference. Each Tensor Core can perform multiple operations per clock cycle on matrices, dramatically accelerating AI workloads compared to traditional CUDA cores.\n\nWhat makes the fifth-generation Tensor Cores in the DGX Spark particularly powerful is their tight integration with 256KB of Tensor Memory (TMEM) per Streaming Multiprocessor (SM). This keeps frequently accessed data close to the compute units, minimizing memory latency and maximizing throughput. The Blackwell architecture features four Tensor Cores per SM, optimized specifically for transformer-based models that have become the foundation of modern AI.\n\n**Transformer Engine and Multi-Precision Support**: The DGX Spark includes NVIDIA's second-generation Transformer Engine, a game-changing feature for LLM inference and fine-tuning. While the GTX 1080 Ti was limited to FP32 and FP16 precision, the DGX Spark supports a range of precision formats optimized for different AI tasks:\n\n- **FP32 (32-bit)**: Traditional floating-point for general computing (31 TFLOPS)\n- **FP16 (16-bit)**: Half-precision for training and inference\n- **FP8 (8-bit)**: Introduced with H100, using E4M3 and E5M2 variants for efficient AI operations\n- **MXFP8**: Blackwell's microscaling FP8 format with block-level scaling factors for improved accuracy\n- **NVFP4 (4-bit)**: Blackwell's proprietary 4-bit floating-point format using two-level scaling, achieving near-FP8 accuracy while reducing memory footprint by 1.8× and enabling 1 PETAFLOP of AI performance\n\nThe Transformer Engine dynamically selects the optimal precision for each layer during inference, balancing accuracy and performance. For LLM inference, FP4 precision delivers massive throughput gains—enabling the 87× performance advantage over FP32-only systems—while maintaining acceptable accuracy for most use cases. This is why the DGX Spark can handle 200B parameter models that would be impossible on the GTX 1080 Ti's 11GB of memory.\n\n**Integration vs. Component**: The GTX 1080 Ti was a component requiring a host system, while the DGX Spark is a complete, integrated platform with CPU, GPU, storage, and networking designed to work in harmony for AI workloads. The NVLink-C2C chip-to-chip interconnect provides high-bandwidth, low-latency communication between the Grace CPU and Blackwell GPU, enabling the unified memory architecture that eliminates traditional PCIe bottlenecks.\n\n## Setting up DGX Spark after Unpacking\n\nAfter unboxing DGX Spark, first thing stands out to me its portability. Its compact and simple to set up and then paired with a Mac book (or any other computer). Before setting up, we need to see the ports of DGX Spark. Lets have a look at the ports of DGX Spark in the diagram below:\n\n::: {style=\"text-align: center;\"}\n![NVIDIA DGX Spark - Ports and Connectivity](assets/DGX_ports.jpg){height=400}\n:::\n\nFirst step is to connect the one end of the power adapter to the Power port and other end to power supply (socket). Click the the On/Off button and that's it. One thing I need to mention that there is no led light indicating if the device is turned on or off. The only way to find out if its on is by detecting the Hotspot in the Wifi . The info of the Hotspot can be found on the cover of Quick Start Guide.\n\n::: {style=\"text-align: center;\"}\n![NVIDIA DGX Spark - Quick Start Guide](assets/Quick_start.jpg){height=400}\n:::\n\nOnce connected with the HotSpot the set up is inititated and just need to follow the the instruction . At some point it will identify the orginal Wifi you are connected in . Then it may initiate a few updates of the firmware and reboots . No human interventions needed .\n\n::: {style=\"text-align: center;\"}\n![NVIDIA DGX Spark - Set up completed](assets/set_up.jpg){height=300}\n:::\n\nAfter the complition of the set up its going to direct you to the [spark page](https://build.nvidia.com/spark)\n\n## Connecting to DGX Spark on Your Local Network\n\nBefore diving into running LLMs, it's essential to establish reliable network access to your DGX Spark. While the initial setup happens over the device's WiFi hotspot, for day-to-day work you'll want to connect to your DGX Spark over your local network via SSH. This gives you command-line access and the ability to tunnel ports for accessing web applications.\n\n### Prerequisites\n\nFirst, verify that you have an SSH client installed. Most modern operating systems (Linux, macOS, Windows 10+) include SSH by default:\n\n```bash\nssh -V\n```\n\nYou'll need the following information:\n- **Username**: Your DGX Spark account name (created during initial setup)\n- **Password**: Your account password\n- **Hostname**: Your device's mDNS hostname (typically `spark-xxxx.local`)\n- **IP Address**: As a backup if mDNS doesn't work on your network\n\n### Finding Your DGX Spark on the Network\n\nThe DGX Spark uses mDNS (multicast DNS) for easy discovery on local networks. Your hostname format is typically `spark-xxxx.local` where `xxxx` is a unique identifier.\n\nTest if mDNS resolution works on your network:\n\n```bash\nping spark-abcd.local\n```\n\nReplace `spark-abcd` with your actual hostname. If you see ping responses with IP addresses and latency measurements, mDNS is working correctly.\n\nIf you get \"Cannot resolve hostname\" or \"Unknown host\" errors, your network doesn't support mDNS (common in corporate networks). In this case, you'll need to find your DGX Spark's IP address through your router's admin panel or by connecting a display directly to the device.\n\n### Establishing SSH Connection\n\nOnce you have your hostname or IP address, connect via SSH:\n\n**Using mDNS hostname:**\n```bash\nssh <username>@<spark-hostname>.local\n```\n\n**Using IP address (if mDNS fails):**\n```bash\nssh <username>@<ip_address>\n```\n\nFor example:\n```bash\nssh dipankar@spark-a1b2.local\n```\n\nOn your first connection, you'll see a host fingerprint warning. Type `yes` to accept and add the host to your known hosts file, then enter your password.\n\n### Verifying Your Connection\n\nOnce connected, verify you're on the correct device:\n\n```bash\nhostname        # Should show your DGX Spark hostname\nuname -a        # Shows system information\nnvidia-smi      # Check GPU status (if available)\n```\n\nType `exit` to close the SSH session.\n\n### SSH Port Forwarding for Web Applications\n\nOne of the most powerful features of SSH is port forwarding, which allows you to access web applications running on your DGX Spark as if they were running on your local machine. This is crucial for accessing services like Open WebUI, Jupyter notebooks, or monitoring dashboards.\n\nFor example, to access a web service running on port 8080 on your DGX Spark:\n\n```bash\nssh -L 8080:localhost:8080 <username>@<spark-hostname>.local\n```\n\nNow, opening `http://localhost:8080` in your browser will connect to the service running on your DGX Spark. The `-L` flag creates a local port forward, tunneling traffic from your local port 8080 to the DGX Spark's port 8080.\n\nYou can forward multiple ports in a single SSH session:\n\n```bash\nssh -L 8080:localhost:8080 -L 11000:localhost:11000 <username>@<spark-hostname>.local\n```\n\nThis is particularly useful when running Open WebUI (typically on port 8080) alongside other monitoring or development tools.\n\n### Troubleshooting Network Access\n\n**mDNS not working?**\n- Check if your router supports mDNS/Bonjour\n- Try connecting from a different network segment\n- Use the IP address directly instead\n- On corporate networks, consult with IT about mDNS availability\n\n**Can't find the IP address?**\n- Check your router's DHCP client list\n- Connect a monitor and keyboard directly to the DGX Spark\n- Use network scanning tools like `nmap` or `arp-scan`\n\n**Connection refused or timeout?**\n- Verify the DGX Spark is powered on and connected to WiFi\n- Check firewall settings on both machines\n- Ensure you're on the same network subnet\n\nWith network access established, you're ready to start deploying AI workloads on your DGX Spark.\n\n## Running Local LLMs: Setting Up Open WebUI and Ollama\n\nWith the DGX Spark hardware setup complete, it's time to put this AI supercomputer to work. The real power of the DGX Spark lies in its ability to run large language models locally, giving you full control over your AI infrastructure without relying on external APIs. In this section, I'll walk through setting up [Open WebUI](https://build.nvidia.com/spark/open-webui/instructions) with Ollama using Docker, and then demonstrate how to interact with these models programmatically using Python.\n\n### Why Open WebUI and Ollama?\n\n**Ollama** provides a simple, efficient way to run large language models locally. It handles model management, optimization, and inference, making it easy to work with models ranging from small 7B parameter models to the 200B parameter models that the DGX Spark can handle.\n\n**Open WebUI** offers a clean, ChatGPT-like interface for interacting with Ollama models. More importantly, it exposes an OpenAI-compatible API, which means you can use familiar tools and libraries to interact with your local models as if they were OpenAI's GPT models.\n\n### Step 1: Setting Up Docker Access\n\nBefore we begin, we need to ensure Docker is properly configured. First, check if you have Docker access:\n\n```bash\ndocker ps\n```\n\nIf you encounter permission errors, add your user to the docker group:\n\n```bash\nsudo usermod -aG docker $USER\nnewgrp docker\n```\n\n### Step 2: Deploying Open WebUI with Ollama\n\nThe beauty of this setup is its simplicity. Open WebUI provides a container image with Ollama integrated, eliminating the need for separate installations.\n\nPull the container image:\n\n```bash\ndocker pull ghcr.io/open-webui/open-webui:ollama\n```\n\nLaunch the container with GPU support and persistent storage:\n\n```bash\ndocker run -d -p 8080:8080 --gpus=all \\\n  -v open-webui:/app/backend/data \\\n  -v open-webui-ollama:/root/.ollama \\\n  --name open-webui ghcr.io/open-webui/open-webui:ollama\n```\n\nThe `--gpus=all` flag is crucial—it gives the container access to the DGX Spark's powerful Grace Blackwell GPU. The two volume mounts ensure that your application data and downloaded models persist across container restarts.\n\nOnce running, navigate to `http://<spark-ip>:8080` in your browser. You'll be greeted with a setup screen where you can create your administrator account.\n\n### Step 3: Downloading Your First Model\n\nThrough the Open WebUI interface, you can access Ollama's extensive model library. For my first test, I downloaded the `gpt-oss:20b` model—a 20 billion parameter open-source model that showcases the DGX Spark's capability to handle frontier-scale models locally.\n\nThe download process happens directly on your DGX Spark, leveraging the 4TB of storage. Depending on the model size and your network speed, this can take several minutes, but the unified 128GB memory architecture means these large models can run entirely in RAM without swapping to disk.\n\n### Step 4: Interacting with Models via Python\n\nWhile the web interface is great for interactive chat, the real power comes from programmatic access. I created a Python client to interact with the Ollama models running on the DGX Spark through Open WebUI's OpenAI-compatible API.\n\nHere's the setup process I followed (you can find the complete code in my [DGX_ollama repository](https://github.com/daddyofadoggy/DGX_ollama)):\n\n**1. Clone and set up the environment:**\n\n```bash\ngit clone https://github.com/daddyofadoggy/DGX_ollama.git\ncd DGX_ollama\npython3 -m venv myvenv\nsource myvenv/bin/activate\npip install -r requirements.txt\n```\n\n**2. Configure authentication:**\n\nCreate a `.env` file with your API key (generated from the Open WebUI settings):\n\n```bash\nOLLAMA_KEY=your_api_key_here\n```\n\n**3. The Python client:**\n\nThe implementation is remarkably simple thanks to the OpenAI-compatible API:\n\n```python\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclient = OpenAI(\n    base_url=\"http://10.0.0.194:8080/ollama/v1\",\n    api_key=os.getenv(\"OLLAMA_KEY\")\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-oss:20b\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Explain the benefits of unified memory architecture for LLM inference\"}\n    ],\n)\n\nprint(response.choices[0].message.content)\n```\n\nThis code looks identical to how you'd interact with OpenAI's API, but it's hitting your local DGX Spark instead. The model runs entirely on your hardware, leveraging those 192 Tensor Cores and the unified memory architecture we discussed earlier.\n\n### What Makes This Setup Powerful\n\nThe combination of DGX Spark's hardware capabilities and this software stack creates a remarkably powerful local AI development environment:\n\n1. **Privacy and Control**: Your data never leaves your machine. For sensitive applications or proprietary data, this is invaluable.\n\n2. **No API Costs**: Once you've invested in the DGX Spark, there are no per-token charges. Run as many inferences as you want.\n\n3. **Customization**: You can fine-tune models on your own data, experiment with different quantization levels, and optimize for your specific use cases.\n\n4. **Low Latency**: No network round trips to external APIs. With the unified memory architecture, inference happens at local GPU speeds.\n\n5. **OpenAI-Compatible Interface**: Existing code using OpenAI's SDK works with minimal modifications—just change the base URL and API key.\n\nIn my initial experiments, the DGX Spark handled the 20B parameter model effortlessly, with inference times that rival cloud-based solutions. The Transformer Engine's dynamic precision selection and those fifth-generation Tensor Cores make a noticeable difference in real-world performance.\n\n### Cleaning Up After Experiments\n\nWhen you're done experimenting or want to free up resources, it's important to properly clean up your Docker containers and volumes. Here's how to do it systematically:\n\n**1. Stop the running container:**\n\n```bash\ndocker stop open-webui\n```\n\n**2. Remove the container:**\n\n```bash\ndocker rm open-webui\n```\n\n**3. (Optional) Remove the volumes:**\n\nIf you want to completely start fresh and remove all data including downloaded models, you can delete the volumes. **Warning**: This will delete all your settings, chat history, and downloaded models.\n\n```bash\ndocker volume rm open-webui\ndocker volume rm open-webui-ollama\n```\n\n**4. (Optional) Remove the Docker image:**\n\nTo free up disk space, you can also remove the Docker image itself:\n\n```bash\ndocker rmi ghcr.io/open-webui/open-webui:ollama\n```\n\n**5. Verify cleanup:**\n\nCheck that everything has been removed:\n\n```bash\n# Check for running containers\ndocker ps -a | grep open-webui\n\n# Check for volumes\ndocker volume ls | grep open-webui\n\n# Check for images\ndocker images | grep open-webui\n```\n\n**Pro Tip**: If you want to preserve your models but clean up the application data, you can selectively remove only the `open-webui` volume while keeping `open-webui-ollama`. This way, you won't need to re-download large models if you decide to set up Open WebUI again later.\n\n## References\n\n### Official NVIDIA Documentation\n\n1. **NVIDIA DGX Spark Product Page**\n   [https://www.nvidia.com/en-us/products/workstations/dgx-spark/](https://www.nvidia.com/en-us/products/workstations/dgx-spark/)\n\n2. **NVIDIA DGX Spark Arrives for World's AI Developers | NVIDIA Newsroom**\n   [https://nvidianews.nvidia.com/news/nvidia-dgx-spark-arrives-for-worlds-ai-developers](https://nvidianews.nvidia.com/news/nvidia-dgx-spark-arrives-for-worlds-ai-developers)\n\n3. **Hardware Overview — DGX Spark User Guide**\n   [https://docs.nvidia.com/dgx/dgx-spark/hardware.html](https://docs.nvidia.com/dgx/dgx-spark/hardware.html)\n\n4. **NVIDIA Blackwell Architecture**\n   [https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/](https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/)\n\n5. **Inside NVIDIA Blackwell Ultra | NVIDIA Technical Blog**\n   [https://developer.nvidia.com/blog/inside-nvidia-blackwell-ultra-the-chip-powering-the-ai-factory-era/](https://developer.nvidia.com/blog/inside-nvidia-blackwell-ultra-the-chip-powering-the-ai-factory-era/)\n\n6. **Using FP8 and FP4 with Transformer Engine — NVIDIA Documentation**\n   [https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html)\n\n### Hardware Specifications\n\n7. **NVIDIA DGX Spark features 6144 CUDA cores | VideoCardz**\n   [https://videocardz.com/newz/nvidia-dgx-spark-features-6144-cuda-cores-just-as-many-as-rtx-5070](https://videocardz.com/newz/nvidia-dgx-spark-features-6144-cuda-cores-just-as-many-as-rtx-5070)\n\n8. **NVIDIA Dissects GB10 Superchip | WCCFtech**\n   [https://wccftech.com/nvidia-gb10-superchip-soc-3nm-20-arm-v9-2-cpu-cores-nvfp4-blackwell-gpu-lpddr5x-9400-memory-140w-tdp/](https://wccftech.com/nvidia-gb10-superchip-soc-3nm-20-arm-v9-2-cpu-cores-nvfp4-blackwell-gpu-lpddr5x-9400-memory-140w-tdp/)\n\n9. **NVIDIA GeForce GTX 1080 Ti Specs | TechPowerUp**\n   [https://www.techpowerup.com/gpu-specs/geforce-gtx-1080-ti.c2877](https://www.techpowerup.com/gpu-specs/geforce-gtx-1080-ti.c2877)\n\n10. **Official nVidia GTX 1080 Ti Specs | GamersNexus**\n    [https://gamersnexus.net/news-pc/2820-official-nvidia-gtx-1080-ti-specs-announced](https://gamersnexus.net/news-pc/2820-official-nvidia-gtx-1080-ti-specs-announced)\n\n### Setup Guides and Tools\n\n11. **Open WebUI with Ollama Setup Guide for DGX Spark**\n    [https://build.nvidia.com/spark/open-webui/instructions](https://build.nvidia.com/spark/open-webui/instructions)\n\n12. **Connect to Your DGX Spark via SSH**\n    [https://build.nvidia.com/spark/connect-to-your-spark/manual-ssh](https://build.nvidia.com/spark/connect-to-your-spark/manual-ssh)\n\n13. **DGX Ollama Python Client | GitHub Repository**\n    [https://github.com/daddyofadoggy/DGX_ollama](https://github.com/daddyofadoggy/DGX_ollama)\n\n14. **Setting Up Open WebUI on DGX Spark | YouTube Tutorial**\n    [https://www.youtube.com/watch?v=yOgNv4HrYZ4](https://www.youtube.com/watch?v=yOgNv4HrYZ4)\n\n### Reviews and Analysis\n\n15. **NVIDIA DGX Spark In-Depth Review | LMSYS Org**\n    [https://lmsys.org/blog/2025-10-13-nvidia-dgx-spark/](https://lmsys.org/blog/2025-10-13-nvidia-dgx-spark/)\n\n16. **NVIDIA DGX Spark Review | IntuitionLabs**\n    [https://intuitionlabs.ai/articles/nvidia-dgx-spark-review](https://intuitionlabs.ai/articles/nvidia-dgx-spark-review)","srcMarkdownNoYaml":"\n\n## Introduction: Meet the DGX Spark\n\nThe NVIDIA DGX Spark, launched in October 2025, represents a significant leap in making enterprise-grade AI infrastructure accessible to individual developers and researchers. This compact AI supercomputer packs an impressive 1 petaFLOP of AI performance into a desktop form factor, powered by the NVIDIA GB10 Grace Blackwell Superchip. At its heart, the system features a 20-core Arm processor (10 Cortex-X925 performance cores and 10 Cortex-A725 efficiency cores) paired with 128GB of unified memory that's seamlessly shared between the CPU and GPU. This unified memory architecture is particularly powerful for AI workloads, allowing the system to run inference on models with up to 200 billion parameters and fine-tune models up to 70 billion parameters locally. With 4TB of solid-state storage and dual QSFP Ethernet ports providing 200 Gb/s of aggregate bandwidth, the DGX Spark at $3,999 brings what was once exclusive to data centers right to your desk.\n\n## A Personal Note: From GTX 1080 Ti to Grace Blackwell\n\n My journey with GPUs began back in October 2017, when I was setting up my lab's computer with an NVIDIA GTX 1080 Ti. It was time I was persuing a PhD (1st year). That experience introduced me to the world of GPU-accelerated computing and sparked my interest in leveraging specialized hardware for computational tasks. Fast forward eight years, and the DGX Spark represents my second foray into GPU ownership—though calling it just a \"GPU\" would be a significant understatement. While the GTX 1080 Ti was a powerful graphics card in its time, the DGX Spark is an entirely different beast: a complete AI supercomputer that integrates CPU, GPU, and unified memory into a cohesive system designed specificallyfor AI workloads. The evolution from a single GPU card to this integrated Grace Blackwell architecture reflects not just technological progress, but also the democratization of AI infrastructure that was once accessible only to large research institutions and tech giants\n\n::: {layout-ncol=2 layout-valign=\"top\"}\n![NVIDIA DGX Spark - A complete AI supercomputer featuring the Grace Blackwell architecture, representing eight years of technological evolution](assets/DGX.jpg){height=400}\n\n![NVIDIA GTX 1080 Ti - My first GPU from October 2017, a powerful graphics card that introduced me to GPU-accelerated computing](assets/GTX1080.jpg){height=400}\n\n:::\n\n## Evolution of GPU Computing: A Side-by-Side Comparison\n\nThe eight-year gap between these two systems tells a remarkable story of technological advancement, particularly in the shift from graphics-focused GPUs to AI-specialized computing platforms.\n\n| **Specification** | **GTX 1080 Ti (2017)** | **DGX Spark (2025)** | **Evolution** |\n|-------------------|------------------------|----------------------|---------------|\n| **Architecture** | Pascal (16nm) | Grace Blackwell GB10 (3nm) | Integrated CPU-GPU superchip design |\n| **CPU Cores** | Host system dependent | 20 Arm cores (10 X925 + 10 A725) | Integrated high-performance CPU |\n| **CUDA Cores** | 3,584 | 6,144 | **71% increase** in CUDA cores |\n| **Tensor Cores** | None | 192 (5th generation) | AI-optimized matrix operations |\n| **Memory Capacity** | 11 GB GDDR5X | 128 GB LPDDR5x Unified | **11.6× increase** in capacity |\n| **Memory Architecture** | Dedicated GPU memory | CPU-GPU unified coherent memory | Seamless sharing eliminates data transfer overhead |\n| **Memory Bandwidth** | 484 GB/s | 273 GB/s unified | Coherent memory access across CPU-GPU |\n| **FP32 Performance** | 11.5 TFLOPS | 31 TFLOPS | **2.7× increase** in traditional compute |\n| **AI Performance** | ~11.5 TFLOPS (FP32 only) | 1,000 TOPS (FP4)<br>1 PETAFLOP | **~87× increase** with AI-optimized precision |\n| **Precision Support** | FP32, FP16 | FP32, FP16, FP8, NVFP4, MXFP8 | Multi-precision for optimal AI inference |\n| **Model Capacity** | Limited by 11GB | 200B parameters (inference)<br>70B parameters (fine-tuning) | Native support for frontier models |\n| **Power Consumption** | 250W | 140W TDP (240W with accessories) | Dramatically more efficient |\n| **Primary Use Case** | Gaming & Graphics | AI Development & Inference | Purpose-built for AI/ML workflows |\n| **Price at Launch** | ~$699 | $3,999 | Premium for integrated AI platform |\n| **Form Factor** | PCIe Graphics Card | Complete Desktop System | Self-contained AI workstation |\n\n### Key Architectural Breakthroughs\n\n**Unified Memory Revolution**: The most significant advancement is the shift from discrete GPU memory to unified coherent memory. The GTX 1080 Ti required explicit data transfers between system RAM and GPU memory, creating bottlenecks. The DGX Spark's 128GB unified memory is seamlessly accessible to both CPU and GPU, eliminating these transfers and enabling efficient processing of models that were impossible on traditional GPUs.\n\n**Fifth-Generation Tensor Cores**: Perhaps the most transformative feature is the inclusion of 192 fifth-generation Tensor Cores—technology completely absent from the GTX 1080 Ti. Tensor Cores are specialized processing units designed specifically for the matrix multiplication operations that dominate neural network training and inference. Each Tensor Core can perform multiple operations per clock cycle on matrices, dramatically accelerating AI workloads compared to traditional CUDA cores.\n\nWhat makes the fifth-generation Tensor Cores in the DGX Spark particularly powerful is their tight integration with 256KB of Tensor Memory (TMEM) per Streaming Multiprocessor (SM). This keeps frequently accessed data close to the compute units, minimizing memory latency and maximizing throughput. The Blackwell architecture features four Tensor Cores per SM, optimized specifically for transformer-based models that have become the foundation of modern AI.\n\n**Transformer Engine and Multi-Precision Support**: The DGX Spark includes NVIDIA's second-generation Transformer Engine, a game-changing feature for LLM inference and fine-tuning. While the GTX 1080 Ti was limited to FP32 and FP16 precision, the DGX Spark supports a range of precision formats optimized for different AI tasks:\n\n- **FP32 (32-bit)**: Traditional floating-point for general computing (31 TFLOPS)\n- **FP16 (16-bit)**: Half-precision for training and inference\n- **FP8 (8-bit)**: Introduced with H100, using E4M3 and E5M2 variants for efficient AI operations\n- **MXFP8**: Blackwell's microscaling FP8 format with block-level scaling factors for improved accuracy\n- **NVFP4 (4-bit)**: Blackwell's proprietary 4-bit floating-point format using two-level scaling, achieving near-FP8 accuracy while reducing memory footprint by 1.8× and enabling 1 PETAFLOP of AI performance\n\nThe Transformer Engine dynamically selects the optimal precision for each layer during inference, balancing accuracy and performance. For LLM inference, FP4 precision delivers massive throughput gains—enabling the 87× performance advantage over FP32-only systems—while maintaining acceptable accuracy for most use cases. This is why the DGX Spark can handle 200B parameter models that would be impossible on the GTX 1080 Ti's 11GB of memory.\n\n**Integration vs. Component**: The GTX 1080 Ti was a component requiring a host system, while the DGX Spark is a complete, integrated platform with CPU, GPU, storage, and networking designed to work in harmony for AI workloads. The NVLink-C2C chip-to-chip interconnect provides high-bandwidth, low-latency communication between the Grace CPU and Blackwell GPU, enabling the unified memory architecture that eliminates traditional PCIe bottlenecks.\n\n## Setting up DGX Spark after Unpacking\n\nAfter unboxing DGX Spark, first thing stands out to me its portability. Its compact and simple to set up and then paired with a Mac book (or any other computer). Before setting up, we need to see the ports of DGX Spark. Lets have a look at the ports of DGX Spark in the diagram below:\n\n::: {style=\"text-align: center;\"}\n![NVIDIA DGX Spark - Ports and Connectivity](assets/DGX_ports.jpg){height=400}\n:::\n\nFirst step is to connect the one end of the power adapter to the Power port and other end to power supply (socket). Click the the On/Off button and that's it. One thing I need to mention that there is no led light indicating if the device is turned on or off. The only way to find out if its on is by detecting the Hotspot in the Wifi . The info of the Hotspot can be found on the cover of Quick Start Guide.\n\n::: {style=\"text-align: center;\"}\n![NVIDIA DGX Spark - Quick Start Guide](assets/Quick_start.jpg){height=400}\n:::\n\nOnce connected with the HotSpot the set up is inititated and just need to follow the the instruction . At some point it will identify the orginal Wifi you are connected in . Then it may initiate a few updates of the firmware and reboots . No human interventions needed .\n\n::: {style=\"text-align: center;\"}\n![NVIDIA DGX Spark - Set up completed](assets/set_up.jpg){height=300}\n:::\n\nAfter the complition of the set up its going to direct you to the [spark page](https://build.nvidia.com/spark)\n\n## Connecting to DGX Spark on Your Local Network\n\nBefore diving into running LLMs, it's essential to establish reliable network access to your DGX Spark. While the initial setup happens over the device's WiFi hotspot, for day-to-day work you'll want to connect to your DGX Spark over your local network via SSH. This gives you command-line access and the ability to tunnel ports for accessing web applications.\n\n### Prerequisites\n\nFirst, verify that you have an SSH client installed. Most modern operating systems (Linux, macOS, Windows 10+) include SSH by default:\n\n```bash\nssh -V\n```\n\nYou'll need the following information:\n- **Username**: Your DGX Spark account name (created during initial setup)\n- **Password**: Your account password\n- **Hostname**: Your device's mDNS hostname (typically `spark-xxxx.local`)\n- **IP Address**: As a backup if mDNS doesn't work on your network\n\n### Finding Your DGX Spark on the Network\n\nThe DGX Spark uses mDNS (multicast DNS) for easy discovery on local networks. Your hostname format is typically `spark-xxxx.local` where `xxxx` is a unique identifier.\n\nTest if mDNS resolution works on your network:\n\n```bash\nping spark-abcd.local\n```\n\nReplace `spark-abcd` with your actual hostname. If you see ping responses with IP addresses and latency measurements, mDNS is working correctly.\n\nIf you get \"Cannot resolve hostname\" or \"Unknown host\" errors, your network doesn't support mDNS (common in corporate networks). In this case, you'll need to find your DGX Spark's IP address through your router's admin panel or by connecting a display directly to the device.\n\n### Establishing SSH Connection\n\nOnce you have your hostname or IP address, connect via SSH:\n\n**Using mDNS hostname:**\n```bash\nssh <username>@<spark-hostname>.local\n```\n\n**Using IP address (if mDNS fails):**\n```bash\nssh <username>@<ip_address>\n```\n\nFor example:\n```bash\nssh dipankar@spark-a1b2.local\n```\n\nOn your first connection, you'll see a host fingerprint warning. Type `yes` to accept and add the host to your known hosts file, then enter your password.\n\n### Verifying Your Connection\n\nOnce connected, verify you're on the correct device:\n\n```bash\nhostname        # Should show your DGX Spark hostname\nuname -a        # Shows system information\nnvidia-smi      # Check GPU status (if available)\n```\n\nType `exit` to close the SSH session.\n\n### SSH Port Forwarding for Web Applications\n\nOne of the most powerful features of SSH is port forwarding, which allows you to access web applications running on your DGX Spark as if they were running on your local machine. This is crucial for accessing services like Open WebUI, Jupyter notebooks, or monitoring dashboards.\n\nFor example, to access a web service running on port 8080 on your DGX Spark:\n\n```bash\nssh -L 8080:localhost:8080 <username>@<spark-hostname>.local\n```\n\nNow, opening `http://localhost:8080` in your browser will connect to the service running on your DGX Spark. The `-L` flag creates a local port forward, tunneling traffic from your local port 8080 to the DGX Spark's port 8080.\n\nYou can forward multiple ports in a single SSH session:\n\n```bash\nssh -L 8080:localhost:8080 -L 11000:localhost:11000 <username>@<spark-hostname>.local\n```\n\nThis is particularly useful when running Open WebUI (typically on port 8080) alongside other monitoring or development tools.\n\n### Troubleshooting Network Access\n\n**mDNS not working?**\n- Check if your router supports mDNS/Bonjour\n- Try connecting from a different network segment\n- Use the IP address directly instead\n- On corporate networks, consult with IT about mDNS availability\n\n**Can't find the IP address?**\n- Check your router's DHCP client list\n- Connect a monitor and keyboard directly to the DGX Spark\n- Use network scanning tools like `nmap` or `arp-scan`\n\n**Connection refused or timeout?**\n- Verify the DGX Spark is powered on and connected to WiFi\n- Check firewall settings on both machines\n- Ensure you're on the same network subnet\n\nWith network access established, you're ready to start deploying AI workloads on your DGX Spark.\n\n## Running Local LLMs: Setting Up Open WebUI and Ollama\n\nWith the DGX Spark hardware setup complete, it's time to put this AI supercomputer to work. The real power of the DGX Spark lies in its ability to run large language models locally, giving you full control over your AI infrastructure without relying on external APIs. In this section, I'll walk through setting up [Open WebUI](https://build.nvidia.com/spark/open-webui/instructions) with Ollama using Docker, and then demonstrate how to interact with these models programmatically using Python.\n\n### Why Open WebUI and Ollama?\n\n**Ollama** provides a simple, efficient way to run large language models locally. It handles model management, optimization, and inference, making it easy to work with models ranging from small 7B parameter models to the 200B parameter models that the DGX Spark can handle.\n\n**Open WebUI** offers a clean, ChatGPT-like interface for interacting with Ollama models. More importantly, it exposes an OpenAI-compatible API, which means you can use familiar tools and libraries to interact with your local models as if they were OpenAI's GPT models.\n\n### Step 1: Setting Up Docker Access\n\nBefore we begin, we need to ensure Docker is properly configured. First, check if you have Docker access:\n\n```bash\ndocker ps\n```\n\nIf you encounter permission errors, add your user to the docker group:\n\n```bash\nsudo usermod -aG docker $USER\nnewgrp docker\n```\n\n### Step 2: Deploying Open WebUI with Ollama\n\nThe beauty of this setup is its simplicity. Open WebUI provides a container image with Ollama integrated, eliminating the need for separate installations.\n\nPull the container image:\n\n```bash\ndocker pull ghcr.io/open-webui/open-webui:ollama\n```\n\nLaunch the container with GPU support and persistent storage:\n\n```bash\ndocker run -d -p 8080:8080 --gpus=all \\\n  -v open-webui:/app/backend/data \\\n  -v open-webui-ollama:/root/.ollama \\\n  --name open-webui ghcr.io/open-webui/open-webui:ollama\n```\n\nThe `--gpus=all` flag is crucial—it gives the container access to the DGX Spark's powerful Grace Blackwell GPU. The two volume mounts ensure that your application data and downloaded models persist across container restarts.\n\nOnce running, navigate to `http://<spark-ip>:8080` in your browser. You'll be greeted with a setup screen where you can create your administrator account.\n\n### Step 3: Downloading Your First Model\n\nThrough the Open WebUI interface, you can access Ollama's extensive model library. For my first test, I downloaded the `gpt-oss:20b` model—a 20 billion parameter open-source model that showcases the DGX Spark's capability to handle frontier-scale models locally.\n\nThe download process happens directly on your DGX Spark, leveraging the 4TB of storage. Depending on the model size and your network speed, this can take several minutes, but the unified 128GB memory architecture means these large models can run entirely in RAM without swapping to disk.\n\n### Step 4: Interacting with Models via Python\n\nWhile the web interface is great for interactive chat, the real power comes from programmatic access. I created a Python client to interact with the Ollama models running on the DGX Spark through Open WebUI's OpenAI-compatible API.\n\nHere's the setup process I followed (you can find the complete code in my [DGX_ollama repository](https://github.com/daddyofadoggy/DGX_ollama)):\n\n**1. Clone and set up the environment:**\n\n```bash\ngit clone https://github.com/daddyofadoggy/DGX_ollama.git\ncd DGX_ollama\npython3 -m venv myvenv\nsource myvenv/bin/activate\npip install -r requirements.txt\n```\n\n**2. Configure authentication:**\n\nCreate a `.env` file with your API key (generated from the Open WebUI settings):\n\n```bash\nOLLAMA_KEY=your_api_key_here\n```\n\n**3. The Python client:**\n\nThe implementation is remarkably simple thanks to the OpenAI-compatible API:\n\n```python\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclient = OpenAI(\n    base_url=\"http://10.0.0.194:8080/ollama/v1\",\n    api_key=os.getenv(\"OLLAMA_KEY\")\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-oss:20b\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Explain the benefits of unified memory architecture for LLM inference\"}\n    ],\n)\n\nprint(response.choices[0].message.content)\n```\n\nThis code looks identical to how you'd interact with OpenAI's API, but it's hitting your local DGX Spark instead. The model runs entirely on your hardware, leveraging those 192 Tensor Cores and the unified memory architecture we discussed earlier.\n\n### What Makes This Setup Powerful\n\nThe combination of DGX Spark's hardware capabilities and this software stack creates a remarkably powerful local AI development environment:\n\n1. **Privacy and Control**: Your data never leaves your machine. For sensitive applications or proprietary data, this is invaluable.\n\n2. **No API Costs**: Once you've invested in the DGX Spark, there are no per-token charges. Run as many inferences as you want.\n\n3. **Customization**: You can fine-tune models on your own data, experiment with different quantization levels, and optimize for your specific use cases.\n\n4. **Low Latency**: No network round trips to external APIs. With the unified memory architecture, inference happens at local GPU speeds.\n\n5. **OpenAI-Compatible Interface**: Existing code using OpenAI's SDK works with minimal modifications—just change the base URL and API key.\n\nIn my initial experiments, the DGX Spark handled the 20B parameter model effortlessly, with inference times that rival cloud-based solutions. The Transformer Engine's dynamic precision selection and those fifth-generation Tensor Cores make a noticeable difference in real-world performance.\n\n### Cleaning Up After Experiments\n\nWhen you're done experimenting or want to free up resources, it's important to properly clean up your Docker containers and volumes. Here's how to do it systematically:\n\n**1. Stop the running container:**\n\n```bash\ndocker stop open-webui\n```\n\n**2. Remove the container:**\n\n```bash\ndocker rm open-webui\n```\n\n**3. (Optional) Remove the volumes:**\n\nIf you want to completely start fresh and remove all data including downloaded models, you can delete the volumes. **Warning**: This will delete all your settings, chat history, and downloaded models.\n\n```bash\ndocker volume rm open-webui\ndocker volume rm open-webui-ollama\n```\n\n**4. (Optional) Remove the Docker image:**\n\nTo free up disk space, you can also remove the Docker image itself:\n\n```bash\ndocker rmi ghcr.io/open-webui/open-webui:ollama\n```\n\n**5. Verify cleanup:**\n\nCheck that everything has been removed:\n\n```bash\n# Check for running containers\ndocker ps -a | grep open-webui\n\n# Check for volumes\ndocker volume ls | grep open-webui\n\n# Check for images\ndocker images | grep open-webui\n```\n\n**Pro Tip**: If you want to preserve your models but clean up the application data, you can selectively remove only the `open-webui` volume while keeping `open-webui-ollama`. This way, you won't need to re-download large models if you decide to set up Open WebUI again later.\n\n## References\n\n### Official NVIDIA Documentation\n\n1. **NVIDIA DGX Spark Product Page**\n   [https://www.nvidia.com/en-us/products/workstations/dgx-spark/](https://www.nvidia.com/en-us/products/workstations/dgx-spark/)\n\n2. **NVIDIA DGX Spark Arrives for World's AI Developers | NVIDIA Newsroom**\n   [https://nvidianews.nvidia.com/news/nvidia-dgx-spark-arrives-for-worlds-ai-developers](https://nvidianews.nvidia.com/news/nvidia-dgx-spark-arrives-for-worlds-ai-developers)\n\n3. **Hardware Overview — DGX Spark User Guide**\n   [https://docs.nvidia.com/dgx/dgx-spark/hardware.html](https://docs.nvidia.com/dgx/dgx-spark/hardware.html)\n\n4. **NVIDIA Blackwell Architecture**\n   [https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/](https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/)\n\n5. **Inside NVIDIA Blackwell Ultra | NVIDIA Technical Blog**\n   [https://developer.nvidia.com/blog/inside-nvidia-blackwell-ultra-the-chip-powering-the-ai-factory-era/](https://developer.nvidia.com/blog/inside-nvidia-blackwell-ultra-the-chip-powering-the-ai-factory-era/)\n\n6. **Using FP8 and FP4 with Transformer Engine — NVIDIA Documentation**\n   [https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html)\n\n### Hardware Specifications\n\n7. **NVIDIA DGX Spark features 6144 CUDA cores | VideoCardz**\n   [https://videocardz.com/newz/nvidia-dgx-spark-features-6144-cuda-cores-just-as-many-as-rtx-5070](https://videocardz.com/newz/nvidia-dgx-spark-features-6144-cuda-cores-just-as-many-as-rtx-5070)\n\n8. **NVIDIA Dissects GB10 Superchip | WCCFtech**\n   [https://wccftech.com/nvidia-gb10-superchip-soc-3nm-20-arm-v9-2-cpu-cores-nvfp4-blackwell-gpu-lpddr5x-9400-memory-140w-tdp/](https://wccftech.com/nvidia-gb10-superchip-soc-3nm-20-arm-v9-2-cpu-cores-nvfp4-blackwell-gpu-lpddr5x-9400-memory-140w-tdp/)\n\n9. **NVIDIA GeForce GTX 1080 Ti Specs | TechPowerUp**\n   [https://www.techpowerup.com/gpu-specs/geforce-gtx-1080-ti.c2877](https://www.techpowerup.com/gpu-specs/geforce-gtx-1080-ti.c2877)\n\n10. **Official nVidia GTX 1080 Ti Specs | GamersNexus**\n    [https://gamersnexus.net/news-pc/2820-official-nvidia-gtx-1080-ti-specs-announced](https://gamersnexus.net/news-pc/2820-official-nvidia-gtx-1080-ti-specs-announced)\n\n### Setup Guides and Tools\n\n11. **Open WebUI with Ollama Setup Guide for DGX Spark**\n    [https://build.nvidia.com/spark/open-webui/instructions](https://build.nvidia.com/spark/open-webui/instructions)\n\n12. **Connect to Your DGX Spark via SSH**\n    [https://build.nvidia.com/spark/connect-to-your-spark/manual-ssh](https://build.nvidia.com/spark/connect-to-your-spark/manual-ssh)\n\n13. **DGX Ollama Python Client | GitHub Repository**\n    [https://github.com/daddyofadoggy/DGX_ollama](https://github.com/daddyofadoggy/DGX_ollama)\n\n14. **Setting Up Open WebUI on DGX Spark | YouTube Tutorial**\n    [https://www.youtube.com/watch?v=yOgNv4HrYZ4](https://www.youtube.com/watch?v=yOgNv4HrYZ4)\n\n### Reviews and Analysis\n\n15. **NVIDIA DGX Spark In-Depth Review | LMSYS Org**\n    [https://lmsys.org/blog/2025-10-13-nvidia-dgx-spark/](https://lmsys.org/blog/2025-10-13-nvidia-dgx-spark/)\n\n16. **NVIDIA DGX Spark Review | IntuitionLabs**\n    [https://intuitionlabs.ai/articles/nvidia-dgx-spark-review](https://intuitionlabs.ai/articles/nvidia-dgx-spark-review)"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":["cosmo","brand"],"title-block-banner":true,"title":"Day One with DGX Spark: From Setup to Running Local LLMs","author":"Dipankar Baisya","date":"2025-11-25","categories":["GPU-computing","nvidia-dgx","LLM","deployment","ollama","open-webui"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}