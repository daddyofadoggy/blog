<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>fp8_blog – My Blogs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">My Blogs</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block"></header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>As large language models continue to grow in size and complexity, pre-training them efficiently has become a critical challenge for researchers and practitioners. Traditional training with 32-bit (FP32) or even 16-bit (BF16) precision requires substantial computational resources and memory. Low-precision training, particularly with 8-bit floating point (FP8) format, has emerged as a promising solution to reduce both memory footprint and training time while maintaining model quality.</p>
<p>This blog post presents a comprehensive exploration of FP8 training, from theoretical foundations to practical implementation, culminating in detailed benchmark results comparing FP8 and BF16 training across multiple model architectures on NVIDIA’s latest B200 (Blackwell) GPUs. We’ll walk through the implementation using PyTorch’s torchao library and HuggingFace Accelerate, and analyze empirical findings that reveal when and why FP8 training excels.</p>
<hr>
</section>
<section id="understanding-low-precision-training" class="level2">
<h2 class="anchored" data-anchor-id="understanding-low-precision-training">Understanding Low-Precision Training</h2>
<section id="what-is-low-precision-training" class="level3">
<h3 class="anchored" data-anchor-id="what-is-low-precision-training">What is Low-Precision Training?</h3>
<p>Low-precision training refers to using reduced numerical precision (fewer bits) for representing numbers during neural network training. Instead of standard 32-bit floating point (FP32), models can be trained using 16-bit (FP16/BF16) or even 8-bit (FP8) formats. The key insight is that <strong>compute happens in low precision, but results are upcast and accumulated in higher precision</strong> to maintain numerical stability.</p>
</section>
<section id="comparison-of-low-precision-methods" class="level3">
<h3 class="anchored" data-anchor-id="comparison-of-low-precision-methods">Comparison of Low-Precision Methods</h3>
<p>According to <a href="https://huggingface.co/docs/accelerate/v1.10.0/en/concept_guides/low_precision_training">HuggingFace Accelerate documentation</a>, different low-precision training methods offer varying trade-offs between memory usage, computation speed, and accuracy. Here’s a comprehensive comparison:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th>Optimization Level</th>
<th>Computation (GEMM)</th>
<th>Comm</th>
<th>Weight</th>
<th>Master Weight</th>
<th>Weight Gradient</th>
<th>Optimizer States</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>FP16 AMP</strong></td>
<td>FP16</td>
<td>FP32</td>
<td>FP32</td>
<td>N/A</td>
<td>FP32</td>
<td>FP32+FP32</td>
</tr>
<tr class="even">
<td><strong>Nvidia TE</strong></td>
<td>FP8</td>
<td>FP32</td>
<td>FP32</td>
<td>N/A</td>
<td>FP32</td>
<td>FP32+FP32</td>
</tr>
<tr class="odd">
<td><strong>MS-AMP O1</strong></td>
<td>FP8</td>
<td>FP8</td>
<td>FP16</td>
<td>N/A</td>
<td>FP8</td>
<td>FP32+FP32</td>
</tr>
<tr class="even">
<td><strong>MS-AMP O2</strong></td>
<td>FP8</td>
<td>FP8</td>
<td>FP16</td>
<td>N/A</td>
<td>FP8</td>
<td>FP8+FP16</td>
</tr>
<tr class="odd">
<td><strong>MS-AMP O3</strong></td>
<td>FP8</td>
<td>FP8</td>
<td>FP8</td>
<td>FP16</td>
<td>FP8</td>
<td>FP8+FP16</td>
</tr>
</tbody>
</table>
<p><strong>Key observations:</strong></p>
<ul>
<li><strong>FP16 AMP</strong> (Automatic Mixed Precision): The baseline mixed-precision approach, computing in FP16 while keeping weights and optimizer states in FP32</li>
<li><strong>Nvidia TransformersEngine (TE)</strong>: Converts matrix multiplications to FP8 while keeping other operations in FP32, providing maximum stability with minimal accuracy loss</li>
<li><strong>MS-AMP O1</strong>: Extends FP8 to communication operations, reducing distributed training bandwidth by ~50%</li>
<li><strong>MS-AMP O2</strong>: Further reduces optimizer states to mixed FP8/FP16, balancing memory savings and numerical stability</li>
<li><strong>MS-AMP O3</strong>: Most aggressive approach with full FP8 except FP16 master weights, maximizing memory reduction</li>
</ul>
</section>
<section id="the-core-principle-compute-vs-storage" class="level3">
<h3 class="anchored" data-anchor-id="the-core-principle-compute-vs-storage">The Core Principle: Compute vs Storage</h3>
<p>The fundamental principle of low-precision training is:</p>
<pre><code>Storage (High Precision) → Cast → Compute (Low Precision) → Upcast → Accumulate (High Precision)</code></pre>
<p><strong>Why this works:</strong></p>
<ul>
<li>✅ <strong>Fast computation</strong> in low precision (FP8/FP16) on modern GPU tensor cores</li>
<li>✅ <strong>Numerical stability</strong> by accumulating in high precision (BF16/FP32)</li>
<li>✅ <strong>Memory savings</strong> during computation (parameters and activations)</li>
<li>✅ <strong>Training stability</strong> maintained across many gradient updates</li>
</ul>
<p><strong>Example: FP8 Forward Pass</strong></p>
<pre><code>1. Parameters stored in BF16
2. Cast weights and activations to FP8
3. Matrix multiplication: FP8 × FP8 (fast!)
4. Upcast result to BF16
5. Store activations in BF16 for backward pass</code></pre>
<p>This prevents <strong>accumulation errors</strong> that would occur if all operations remained in FP8, while still gaining the computational speedup from low-precision arithmetic.</p>
<hr>
</section>
</section>
<section id="float8-fp8-format-technical-deep-dive" class="level2">
<h2 class="anchored" data-anchor-id="float8-fp8-format-technical-deep-dive">Float8 (FP8) Format: Technical Deep Dive</h2>
<section id="what-is-fp8" class="level3">
<h3 class="anchored" data-anchor-id="what-is-fp8">What is FP8?</h3>
<p>Float8 (FP8) is an 8-bit floating-point format that represents numbers using only 8 bits, compared to 32 bits for FP32 or 16 bits for FP16/BF16. According to the <a href="https://pytorch.org/blog/training-using-float8-fsdp2/">PyTorch blog on FP8 training</a>, FP8 provides a crucial balance between memory efficiency and computational precision for large-scale training.</p>
</section>
<section id="fp8-format-structure" class="level3">
<h3 class="anchored" data-anchor-id="fp8-format-structure">FP8 Format Structure</h3>
<p>FP8 typically uses the following bit allocation:</p>
<ul>
<li><strong>1 sign bit</strong>: Positive or negative</li>
<li><strong>4-5 exponent bits</strong>: Determines the range of representable values</li>
<li><strong>2-3 mantissa bits</strong>: Determines precision within that range</li>
</ul>
<p><strong>Precision Comparison Table:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 10%">
<col style="width: 15%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th>Precision</th>
<th>Total Bits</th>
<th>Exponent</th>
<th>Mantissa</th>
<th>Range</th>
<th>Precision</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>FP32</strong></td>
<td>32</td>
<td>8 bits</td>
<td>23 bits</td>
<td>±3.4e38</td>
<td>~7 decimal digits</td>
<td>Master weights, accumulation</td>
</tr>
<tr class="even">
<td><strong>BF16</strong></td>
<td>16</td>
<td>8 bits</td>
<td>7 bits</td>
<td>±3.4e38</td>
<td>~3 decimal digits</td>
<td>Training (good range)</td>
</tr>
<tr class="odd">
<td><strong>FP16</strong></td>
<td>16</td>
<td>5 bits</td>
<td>10 bits</td>
<td>±65,504</td>
<td>~3 decimal digits</td>
<td>Training (limited range)</td>
</tr>
<tr class="even">
<td><strong>FP8</strong></td>
<td>8</td>
<td>4-5 bits</td>
<td>2-3 bits</td>
<td>±57,344</td>
<td>~2 decimal digits</td>
<td>Computation only</td>
</tr>
</tbody>
</table>
</section>
<section id="key-characteristics" class="level3">
<h3 class="anchored" data-anchor-id="key-characteristics">Key Characteristics</h3>
<p><strong>Memory Efficiency:</strong></p>
<ul>
<li>75% reduction compared to FP32</li>
<li>50% reduction compared to FP16/BF16</li>
<li>Critical for training billion-parameter models</li>
</ul>
<p><strong>Computational Performance:</strong></p>
<ul>
<li>2x faster matrix multiplications vs BF16</li>
<li>4x faster vs FP32</li>
<li>Leverages modern GPU tensor cores (NVIDIA H100, B200)</li>
</ul>
<p><strong>Precision Trade-off:</strong></p>
<ul>
<li>Limited precision (~2 significant decimal digits)</li>
<li>Requires dynamic scaling to maximize representable range</li>
<li><strong>Must upcast for accumulation</strong> to avoid compounding errors</li>
</ul>
</section>
<section id="fp8-variants" class="level3">
<h3 class="anchored" data-anchor-id="fp8-variants">FP8 Variants</h3>
<p>There are two main FP8 formats defined in the <a href="https://www.opencompute.org/documents/ocp-8-bit-floating-point-specification-ofp8-revision-1-0-2023-06-20-pdf">OCP (Open Compute Project) FP8 specification</a>:</p>
<ol type="1">
<li><p><strong>E4M3FN (4 exponent, 3 mantissa)</strong>: Better precision, smaller range</p>
<ul>
<li>Range: ±448</li>
<li>Precision: 3 mantissa bits ≈ 0.1% relative error</li>
<li><strong>Typical use: Forward pass</strong> (weights and activations)</li>
</ul></li>
<li><p><strong>E5M2 (5 exponent, 2 mantissa)</strong>: Larger range, less precision</p>
<ul>
<li>Range: ±57,344</li>
<li>Precision: 2 mantissa bits ≈ 1% relative error</li>
<li><strong>Typical use: Backward pass</strong> (gradients)</li>
</ul></li>
</ol>
<p><strong>Why this assignment?</strong></p>
<p><strong>Forward Pass (E4M3):</strong></p>
<ul>
<li>Activations and weights have <strong>moderate, predictable ranges</strong></li>
<li>Need <strong>higher precision</strong> to preserve information through layers</li>
<li>E4M3’s 3 mantissa bits provide 2x better precision than E5M2</li>
<li>Smaller range (±448) is sufficient for well-normalized networks</li>
<li>Example: Layer outputs typically in range [-10, 10] after normalization</li>
</ul>
<p><strong>Backward Pass (E5M2):</strong></p>
<ul>
<li>Gradients have <strong>wide, unpredictable dynamic range</strong></li>
<li>Can span from 1e-7 (tiny gradients in early layers) to 100+ (large gradients near loss)</li>
<li>Need <strong>larger range</strong> to avoid overflow/underflow</li>
<li>E5M2’s 5 exponent bits provide 128x larger range than E4M3</li>
<li>Precision is less critical (gradients are noisy estimates anyway)</li>
<li>Example: Gradient magnitudes can vary by 5-6 orders of magnitude</li>
</ul>
<p><strong>Practical example:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward pass: E4M3</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>activation <span class="op">=</span> layer(<span class="bu">input</span>)  <span class="co"># Values in [-10, 10]</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>activation_fp8 <span class="op">=</span> to_e4m3(activation)  <span class="co"># Precise quantization</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Backward pass: E5M2</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>gradient <span class="op">=</span> compute_gradient(loss)  <span class="co"># Values in [1e-6, 100]</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>gradient_fp8 <span class="op">=</span> to_e5m2(gradient)  <span class="co"># Wide range captured</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Modern implementations:</strong></p>
<ul>
<li>NVIDIA H100/B200 GPUs support both formats in hardware</li>
<li>TorchAO and TransformersEngine automatically select appropriate format</li>
<li>Some implementations use E4M3 for both passes with careful scaling</li>
</ul>
</section>
<section id="dynamic-scaling-in-fp8" class="level3">
<h3 class="anchored" data-anchor-id="dynamic-scaling-in-fp8">Dynamic Scaling in FP8</h3>
<p>FP8’s limited range requires <strong>dynamic scaling</strong> to maximize precision:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Conceptual FP8 scaling mechanism</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>max_val <span class="op">=</span> <span class="bu">max</span>(<span class="bu">abs</span>(tensor))</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>scale <span class="op">=</span> FP8_MAX_VALUE <span class="op">/</span> max_val</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale and quantize</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>tensor_fp8 <span class="op">=</span> quantize((tensor <span class="op">*</span> scale).clip(<span class="op">-</span>FP8_MAX, FP8_MAX))</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># During computation, apply inverse scaling</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> (tensor_fp8_A <span class="op">@</span> tensor_fp8_B) <span class="op">/</span> (scale_A <span class="op">*</span> scale_B)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This ensures values use the full FP8 range, minimizing quantization errors.</p>
</section>
<section id="detailed-fp8-training-flow-with-fsdp2" class="level3">
<h3 class="anchored" data-anchor-id="detailed-fp8-training-flow-with-fsdp2">Detailed FP8 Training Flow with FSDP2</h3>
<p>Let’s examine the complete precision management flow in FP8 training with FSDP2, as implemented in our benchmark.</p>
<section id="forward-pass-flow" class="level4">
<h4 class="anchored" data-anchor-id="forward-pass-flow">Forward Pass Flow</h4>
<pre><code>┌─────────────────────────────────────────────────────────────┐
│ Step 1: Parameter Storage (BF16, sharded across GPUs)      │
│         • Each GPU stores 1/N of model parameters           │
│         • Base dtype: BF16 (16 bits per parameter)          │
└─────────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────────┐
│ Step 2: All-Gather in FP8 (FSDP2 communication)            │
│         • Parameters gathered from all GPUs in FP8          │
│         • Saves 2x bandwidth vs BF16                        │
│         • enable_fsdp_float8_all_gather=True                │
│         • 8 bits/param vs 16 bits/param                     │
└─────────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────────┐
│ Step 3: Upcast FP8 → BF16                                   │
│         • Parameters converted to BF16 after gathering      │
│         • Ensures numerical stability for computation       │
└─────────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────────┐
│ Step 4: Matrix Multiply in FP8                              │
│         • Weights: BF16 → FP8 (cast to 8-bit)               │
│         • Activations: BF16 → FP8 (cast to 8-bit)           │
│         • Computation: FP8 × FP8 (fast tensor cores!)       │
│         • 2x speedup vs BF16 × BF16                         │
└─────────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────────┐
│ Step 5: Upcast Results FP8 → BF16                           │
│         • Critical for numerical stability                  │
│         • Prevents accumulation errors                      │
│         • Result has full BF16 precision                    │
└─────────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────────┐
│ Step 6: Store Activations (BF16)                            │
│         • Needed for backward pass                          │
│         • Higher precision for gradient computation         │
└─────────────────────────────────────────────────────────────┘</code></pre>
</section>
<section id="backward-pass-flow" class="level4">
<h4 class="anchored" data-anchor-id="backward-pass-flow">Backward Pass Flow</h4>
<pre><code>┌─────────────────────────────────────────────────────────────┐
│ Step 1: Compute Gradients in BF16                           │
│         • Uses stored BF16 activations                      │
│         • Chain rule applied in higher precision            │
└─────────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────────┐
│ Step 2: Cast Gradients BF16 → FP8                           │
│         • For storage and communication                     │
│         • Reduces memory footprint by 2x                    │
└─────────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────────┐
│ Step 3: Reduce-Scatter in FP8                               │
│         • Gradients averaged across GPUs                    │
│         • Communicated in FP8 (saves bandwidth)             │
│         • Each GPU receives its gradient shard              │
└─────────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────────┐
│ Step 4: Upcast to BF16 for Optimizer                        │
│         • Optimizer needs higher precision                  │
│         • Ensures stable parameter updates                  │
└─────────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────────┐
│ Step 5: Update Parameters (BF16)                            │
│         • AdamW updates master weights in BF16              │
│         • Maintains numerical stability over many steps     │
└─────────────────────────────────────────────────────────────┘</code></pre>
</section>
</section>
<section id="the-accumulation-problem-why-upcasting-is-essential" class="level3">
<h3 class="anchored" data-anchor-id="the-accumulation-problem-why-upcasting-is-essential">The Accumulation Problem: Why Upcasting is Essential</h3>
<p><strong>The core challenge:</strong> FP8 has very limited precision (~3-4 significant decimal digits). When you accumulate many small values, errors compound catastrophically.</p>
<p><strong>Example: Accumulation in FP8 (Bad!)</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulated FP8 accumulation - DO NOT DO THIS!</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> fp8(<span class="fl">0.0</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    small_value <span class="op">=</span> fp8(<span class="fl">0.001</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    result <span class="op">+=</span> small_value  <span class="co"># Each addition loses precision!</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Expected result: 1.0</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Actual result: 0.87 or worse (accumulated rounding errors)</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Error: ~13% due to precision loss at each step</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Why this fails:</strong></p>
<ul>
<li>Each FP8 addition introduces ~0.0001-0.001 rounding error</li>
<li>1000 additions → errors accumulate</li>
<li>Final result is significantly wrong</li>
</ul>
<p><strong>Solution: Compute in FP8, Accumulate in BF16 (Good!)</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Correct approach: upcast before accumulating</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> bf16(<span class="fl">0.0</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    small_value <span class="op">=</span> fp8(<span class="fl">0.001</span>)       <span class="co"># Compute in FP8</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    result <span class="op">+=</span> bf16(small_value)    <span class="co"># Upcast before accumulating</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Expected result: 1.0</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Actual result: 0.999 (accurate!)</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Error: ~0.1% - acceptable for training</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Why this works:</strong></p>
<ul>
<li>BF16’s 7-bit mantissa preserves precision during accumulation</li>
<li>Only the initial computation uses FP8 (fast)</li>
<li>Accumulation uses BF16 (stable)</li>
<li>Best of both worlds: speed + stability</li>
</ul>
<p><strong>Real training example:</strong></p>
<p>Consider a gradient update in a transformer:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Wrong: accumulate gradients in FP8</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> model.layers:</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    grad_fp8 <span class="op">=</span> compute_gradient_fp8(layer)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    total_grad_fp8 <span class="op">+=</span> grad_fp8  <span class="co"># Error accumulates!</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Right: accumulate gradients in BF16</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> model.layers:</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    grad_fp8 <span class="op">=</span> compute_gradient_fp8(layer)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    total_grad_bf16 <span class="op">+=</span> grad_fp8.to(bf16)  <span class="co"># Stable accumulation</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="operation-level-precision-strategy" class="level3">
<h3 class="anchored" data-anchor-id="operation-level-precision-strategy">Operation-Level Precision Strategy</h3>
<p>Different operations in neural network training have different precision requirements. Here’s the optimal strategy used in our benchmark:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 26%">
<col style="width: 26%">
<col style="width: 26%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th>Operation</th>
<th>Precision</th>
<th>Rationale</th>
<th>Impact</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Matrix Multiply</strong></td>
<td><strong>FP8</strong></td>
<td>Bulk of computation; 2-4x speedup on modern GPUs</td>
<td>60-80% of training time</td>
</tr>
<tr class="even">
<td><strong>Activation Functions</strong></td>
<td><strong>BF16</strong></td>
<td>Non-linear ops benefit from higher precision</td>
<td>Small overhead, better accuracy</td>
</tr>
<tr class="odd">
<td><strong>Result Accumulation</strong></td>
<td><strong>BF16</strong></td>
<td>Prevents compounding rounding errors</td>
<td>Critical for stability</td>
</tr>
<tr class="even">
<td><strong>Gradient Computation</strong></td>
<td><strong>BF16</strong></td>
<td>Maintains gradient accuracy for backprop</td>
<td>Essential for convergence</td>
</tr>
<tr class="odd">
<td><strong>Parameter Updates</strong></td>
<td><strong>BF16/FP32</strong></td>
<td>Critical for long-term training stability</td>
<td>Optimizer needs precision</td>
</tr>
<tr class="even">
<td><strong>Communication (FSDP)</strong></td>
<td><strong>FP8</strong></td>
<td>Reduces network bandwidth by 2x</td>
<td>Speeds up multi-GPU training</td>
</tr>
<tr class="odd">
<td><strong>Parameter Storage</strong></td>
<td><strong>BF16</strong></td>
<td>Master weights for optimizer</td>
<td>Memory vs precision balance</td>
</tr>
<tr class="even">
<td><strong>Normalization (LayerNorm)</strong></td>
<td><strong>BF16</strong></td>
<td>Statistics computation needs precision</td>
<td>Prevents numerical instability</td>
</tr>
<tr class="odd">
<td><strong>Residual Connections</strong></td>
<td><strong>BF16</strong></td>
<td>Direct addition benefits from precision</td>
<td>Maintains gradient flow</td>
</tr>
</tbody>
</table>
<p><strong>Performance impact breakdown:</strong></p>
<p>For a Llama 3.1 8B model:</p>
<ul>
<li>Matrix multiplications: ~75% of FLOPs → <strong>FP8 gives 2x speedup here</strong></li>
<li>Other operations: ~25% of FLOPs → <strong>Stay in BF16 for stability</strong></li>
<li>Overall speedup: ~1.5x (0.75 × 2x + 0.25 × 1x = 1.5x)</li>
</ul>
<p>This explains why we see 10-15% TFLOPs improvement rather than 2x in our benchmarks.</p>
</section>
<section id="traditional-mixed-precision-training-fp16bf16---historical-context" class="level3">
<h3 class="anchored" data-anchor-id="traditional-mixed-precision-training-fp16bf16---historical-context">Traditional Mixed Precision Training (FP16/BF16) - Historical Context</h3>
<p>Before FP8, the standard was FP16/BF16 mixed precision training:</p>
<p><strong>Flow:</strong></p>
<pre><code>1. Master Weights: Stored in FP32 (high precision)
   ↓
2. Cast to FP16/BF16 for forward pass
   ↓
3. Compute: Matrix multiplications in FP16/BF16 (2x faster than FP32)
   ↓
4. Activations: Stored in FP16/BF16 (50% memory vs FP32)
   ↓
5. Backward Pass: Gradients computed in FP16/BF16
   ↓
6. Upcast: Gradients converted to FP32 before optimizer
   ↓
7. Optimizer: Updates master weights in FP32</code></pre>
<p><strong>Key insight:</strong> Even with FP16 computation, optimizer maintains FP32 master copy to prevent precision loss over thousands of gradient updates.</p>
<p><strong>FP8 extends this principle:</strong></p>
<ul>
<li>Compute: FP8 (even lower precision, 2x faster than BF16)</li>
<li>Accumulate: BF16 (sufficient precision for stability)</li>
<li>Master weights: BF16 (good enough for billion-parameter models)</li>
</ul>
<p>This hierarchical precision strategy is the foundation of modern efficient training.</p>
<hr>
</section>
</section>
<section id="torchaos-convert_to_float8_training-enabling-fp8-at-scale" class="level2">
<h2 class="anchored" data-anchor-id="torchaos-convert_to_float8_training-enabling-fp8-at-scale">TorchAO’s convert_to_float8_training: Enabling FP8 at Scale</h2>
<section id="overview" class="level3">
<h3 class="anchored" data-anchor-id="overview">Overview</h3>
<p>The <a href="https://github.com/pytorch/ao/tree/main/torchao/float8">torchao library</a> provides <code>convert_to_float8_training</code>, a function that seamlessly converts <code>torch.nn.Linear</code> modules to FP8-enabled <code>Float8Linear</code> modules for efficient training.</p>
</section>
<section id="basic-usage" class="level3">
<h3 class="anchored" data-anchor-id="basic-usage">Basic Usage</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchao.float8 <span class="im">import</span> convert_to_float8_training, Float8LinearConfig</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create model</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">8192</span>, <span class="dv">4096</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">4096</span>, <span class="dv">128</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>).bfloat16().cuda()</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Configure FP8 recipe</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> Float8LinearConfig.from_recipe_name(<span class="st">"tensorwise"</span>)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert eligible linear modules to FP8</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>convert_to_float8_training(model, config<span class="op">=</span>config)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Enable torch.compile for best performance</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> torch.<span class="bu">compile</span>(model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="configuration-recipes" class="level3">
<h3 class="anchored" data-anchor-id="configuration-recipes">Configuration Recipes</h3>
<p>TorchAO provides three FP8 recipes with different speed/accuracy trade-offs:</p>
<p><strong>1. “tensorwise”</strong> - Fastest but least accurate</p>
<ul>
<li>Scales entire tensors by a single factor</li>
<li>Minimal overhead</li>
<li>Best for throughput-critical applications</li>
</ul>
<p><strong>2. “rowwise”</strong> - Balanced performance and accuracy</p>
<ul>
<li>Scales each row independently</li>
<li>Better numerical properties</li>
<li>Recommended for most use cases</li>
</ul>
<p><strong>3. “rowwise_with_gw_hp”</strong> - Most accurate</p>
<ul>
<li>Row-wise scaling with high-precision gradients</li>
<li>Maintains gradient accuracy</li>
<li>Best for quality-critical training</li>
</ul>
</section>
<section id="optional-module-filtering" class="level3">
<h3 class="anchored" data-anchor-id="optional-module-filtering">Optional Module Filtering</h3>
<p>You can selectively convert modules using a filter function:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> module_filter_fn(mod: torch.nn.Module, fqn: <span class="bu">str</span>):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Skip first and last layers (common practice)</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> fqn <span class="kw">in</span> [<span class="st">"0"</span>, <span class="st">"model.layers.-1"</span>]:</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Only convert layers with dimensions divisible by 16</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(mod, torch.nn.Linear):</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mod.in_features <span class="op">%</span> <span class="dv">16</span> <span class="op">!=</span> <span class="dv">0</span> <span class="kw">or</span> mod.out_features <span class="op">%</span> <span class="dv">16</span> <span class="op">!=</span> <span class="dv">0</span>:</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">True</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>convert_to_float8_training(</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    model,</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    config<span class="op">=</span>config,</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    module_filter_fn<span class="op">=</span>module_filter_fn</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Why skip first/last layers?</strong></p>
<ul>
<li>Input embeddings and output layers are often more sensitive to precision</li>
<li>Keeping them in higher precision improves model quality with minimal cost</li>
</ul>
</section>
<section id="performance-impact" class="level3">
<h3 class="anchored" data-anchor-id="performance-impact">Performance Impact</h3>
<p>According to torchao benchmarks on NVIDIA H100 with 8 GPUs:</p>
<ul>
<li><strong>Tensorwise scaling</strong>: ~25% speedup over BF16 baseline</li>
<li><strong>Rowwise scaling</strong>: ~10% speedup with better accuracy</li>
<li><strong>E2E training speedups</strong>: Up to 1.5x at 512 GPU / 405B parameter scale</li>
</ul>
</section>
<section id="integration-with-pytorch-ecosystem" class="level3">
<h3 class="anchored" data-anchor-id="integration-with-pytorch-ecosystem">Integration with PyTorch Ecosystem</h3>
<p><code>convert_to_float8_training</code> seamlessly composes with:</p>
<ul>
<li>✅ <code>torch.compile</code> for kernel fusion</li>
<li>✅ FSDP2 for distributed training</li>
<li>✅ DTensor-based distributed APIs</li>
<li>✅ PyTorch activation checkpointing</li>
</ul>
<hr>
</section>
</section>
<section id="fp8-with-ddp-huggingface-accelerate-baseline" class="level2">
<h2 class="anchored" data-anchor-id="fp8-with-ddp-huggingface-accelerate-baseline">FP8 with DDP: HuggingFace Accelerate Baseline</h2>
<section id="the-train_baseline-function" class="level3">
<h3 class="anchored" data-anchor-id="the-train_baseline-function">The train_baseline() Function</h3>
<p>HuggingFace provides a <a href="https://github.com/huggingface/accelerate/blob/main/benchmarks/fp8/torchao/ddp.py">reference implementation</a> showing how to use FP8 with DistributedDataParallel (DDP).</p>
</section>
<section id="implementation-walkthrough" class="level3">
<h3 class="anchored" data-anchor-id="implementation-walkthrough">Implementation Walkthrough</h3>
<p><strong>Step 1: Identify Linear Layers</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_baseline():</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    set_seed(<span class="dv">42</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler <span class="op">=</span> get_training_utilities(MODEL_NAME)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Find first and last linear layers</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    first_linear <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    last_linear <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, module <span class="kw">in</span> model.named_modules():</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(module, torch.nn.Linear):</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> first_linear <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>                first_linear <span class="op">=</span> name</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>            last_linear <span class="op">=</span> name</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Why identify first/last layers?</strong> The first and last linear layers are typically excluded from FP8 conversion for numerical stability:</p>
<ul>
<li><strong>First layer</strong>: Processes input embeddings, which can have wide dynamic range</li>
<li><strong>Last layer</strong>: Produces final logits for loss computation, where precision matters</li>
</ul>
<p><strong>Step 2: Create Filter Function</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>    func <span class="op">=</span> partial(</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>        filter_linear_layers,</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>        first_layer_name<span class="op">=</span>first_linear,</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>        last_layer_name<span class="op">=</span>last_linear</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This creates a filtering function that excludes boundary layers from FP8 conversion.</p>
<p><strong>Step 3: Apply FP8 Conversion</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>    convert_to_float8_training(model, module_filter_fn<span class="op">=</span>func)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>All eligible <code>nn.Linear</code> layers are now replaced with <code>Float8Linear</code> modules.</p>
<p><strong>Step 4: Wrap with DDP</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>    device_ids <span class="op">=</span> [accelerator.local_process_index]</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    output_device <span class="op">=</span> accelerator.local_process_index</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> DDP(</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>        device_ids<span class="op">=</span>device_ids,</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>        output_device<span class="op">=</span>output_device</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The FP8 model is wrapped with PyTorch’s DistributedDataParallel for multi-GPU training.</p>
<p><strong>Step 5: Training Loop with Autocast</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> train_dataloader:</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.autocast(device_type<span class="op">=</span><span class="st">"cuda"</span>, dtype<span class="op">=</span>torch.bfloat16):</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(<span class="op">**</span>batch)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> outputs.loss</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Key points:</strong></p>
<ul>
<li><strong>Autocast context</strong>: Ensures non-FP8 operations use BF16</li>
<li><strong>DDP gradient synchronization</strong>: Gradients are all-reduced across GPUs automatically</li>
<li><strong>Mixed precision</strong>: FP8 for linear layers, BF16 for other operations</li>
</ul>
</section>
<section id="ddp-vs-fsdp-when-to-use-each" class="level3">
<h3 class="anchored" data-anchor-id="ddp-vs-fsdp-when-to-use-each">DDP vs FSDP: When to Use Each</h3>
<p><strong>Use DDP when:</strong></p>
<ul>
<li>Model fits in single GPU memory</li>
<li>Simple multi-GPU setup needed</li>
<li>Maximum per-GPU throughput desired</li>
</ul>
<p><strong>Use FSDP when:</strong></p>
<ul>
<li>Model too large for single GPU</li>
<li>Need to scale to 100+ GPUs</li>
<li>Memory efficiency is critical</li>
</ul>
<hr>
</section>
</section>
<section id="fp8-with-fsdp2-production-scale-training" class="level2">
<h2 class="anchored" data-anchor-id="fp8-with-fsdp2-production-scale-training">FP8 with FSDP2: Production-Scale Training</h2>
<section id="fsdp2-overview" class="level3">
<h3 class="anchored" data-anchor-id="fsdp2-overview">FSDP2 Overview</h3>
<p>FSDP2 (Fully Sharded Data Parallel 2) is PyTorch’s latest distributed training framework that <strong>shards model parameters, gradients, and optimizer states across GPUs</strong>. This enables training models that wouldn’t fit on a single GPU.</p>
</section>
<section id="float8linearconfig-for-fsdp2" class="level3">
<h3 class="anchored" data-anchor-id="float8linearconfig-for-fsdp2">Float8LinearConfig for FSDP2</h3>
<p>The <a href="https://github.com/huggingface/accelerate/blob/main/examples/torch_native_parallelism/fsdp2_fp8.py">HuggingFace FSDP2 example</a> shows how to configure FP8 with FSDP2:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchao.float8 <span class="im">import</span> Float8LinearConfig</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> accelerate.utils <span class="im">import</span> (</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    AORecipeKwargs,</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    FullyShardedDataParallelPlugin</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create FSDP2 plugin</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>fsdp2_plugin <span class="op">=</span> FullyShardedDataParallelPlugin(</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    fsdp_version<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>    cpu_ram_efficient_loading<span class="op">=</span><span class="va">False</span>,  <span class="co"># Incompatible with FP8 torchao</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    auto_wrap_policy<span class="op">=</span><span class="st">"transformer_based_wrap"</span>,</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    transformer_cls_names_to_wrap<span class="op">=</span>[<span class="st">"LlamaDecoderLayer"</span>],</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>fsdp2_plugin.set_mixed_precision(args.precision)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Configure FP8 settings</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>fp8_config <span class="op">=</span> Float8LinearConfig(</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>    enable_fsdp_float8_all_gather<span class="op">=</span><span class="va">True</span>,  <span class="co"># Key optimization!</span></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Pass FP8 config to Accelerator</span></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>kwargs <span class="op">=</span> []</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> args.precision <span class="op">==</span> <span class="st">"fp8"</span>:</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>    kwargs <span class="op">=</span> [AORecipeKwargs(config<span class="op">=</span>fp8_config)]</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>accelerator <span class="op">=</span> Accelerator(</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>    fsdp_plugin<span class="op">=</span>fsdp2_plugin,</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>    dynamo_plugin<span class="op">=</span>dynamo_plugin,</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>    kwargs_handlers<span class="op">=</span>kwargs,</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Later: prepare the model</span></span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>model, optimizer, dataloader <span class="op">=</span> accelerator.prepare(model, optimizer, dataloader)</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a><span class="co"># ↑ convert_to_float8_training() is called HERE under the hood!</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="under-the-hood-how-accelerate-applies-fp8" class="level3">
<h3 class="anchored" data-anchor-id="under-the-hood-how-accelerate-applies-fp8">Under the Hood: How Accelerate Applies FP8</h3>
<p><strong>Key difference from DDP approach:</strong></p>
<p>In the <strong>DDP example</strong> (Section 4), we explicitly called:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>convert_to_float8_training(model, module_filter_fn<span class="op">=</span>func)  <span class="co"># Explicit call</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> DDP(model, ...)  <span class="co"># Then wrap with DDP</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>In the <strong>FSDP2 approach</strong>, we don’t see <code>convert_to_float8_training()</code> in user code, but <strong>Accelerate calls it automatically</strong> during <code>accelerator.prepare()</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># What happens inside accelerator.prepare(model)</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prepare(<span class="va">self</span>, model):</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Apply AO (torchao) recipe if provided</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="va">self</span>.kwargs_handlers contains AORecipeKwargs:</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>        config <span class="op">=</span> kwargs_handler.config  <span class="co"># Float8LinearConfig</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Accelerate internally calls:</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>        convert_to_float8_training(model, config<span class="op">=</span>config)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Then wrap with FSDP2</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> FSDP(model, ...)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Execution order:</strong></p>
<ol type="1">
<li>User creates <code>Float8LinearConfig</code> with settings</li>
<li>User passes it via <code>AORecipeKwargs</code> to <code>Accelerator</code></li>
<li>User calls <code>accelerator.prepare(model)</code></li>
<li><strong>Accelerate calls <code>convert_to_float8_training(model, config=fp8_config)</code></strong> internally</li>
<li>Accelerate then wraps the FP8 model with FSDP2</li>
<li>Returns the prepared model</li>
</ol>
<p><strong>Why this design?</strong></p>
<p>The FSDP2 approach lets Accelerate manage the order of operations:</p>
<ul>
<li>✅ Ensures FP8 conversion happens <strong>before</strong> FSDP wrapping</li>
<li>✅ Prevents user errors (wrong order of operations)</li>
<li>✅ Cleaner API (one call to <code>prepare()</code> does everything)</li>
<li>✅ Handles edge cases (e.g., certain layers shouldn’t be converted)</li>
</ul>
<p><strong>Verification:</strong></p>
<p>You can verify this by inspecting the model after <code>prepare()</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_config(...)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">type</span>(model.model.layers[<span class="dv">0</span>].mlp.gate_proj))</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Output: &lt;class 'torch.nn.Linear'&gt;</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> accelerator.prepare(model)  <span class="co"># With AORecipeKwargs</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">type</span>(model.model.layers[<span class="dv">0</span>].mlp.gate_proj))</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Output: &lt;class 'torchao.float8.float8_linear.Float8Linear'&gt;</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="co">#         ↑ Linear layers converted to Float8Linear!</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="key-configuration-enable_fsdp_float8_all_gather" class="level3">
<h3 class="anchored" data-anchor-id="key-configuration-enable_fsdp_float8_all_gather">Key Configuration: enable_fsdp_float8_all_gather</h3>
<p>The critical optimization is <code>enable_fsdp_float8_all_gather=True</code>:</p>
<p><strong>What it does:</strong></p>
<ul>
<li>Parameters are <strong>gathered in FP8 format</strong> during FSDP’s all-gather operation</li>
<li>After gathering, parameters are <strong>upcast to BF16</strong> for computation</li>
<li>This saves <strong>2x communication bandwidth</strong> vs gathering in BF16</li>
</ul>
</section>
<section id="fsdp2-sharding-mechanism" class="level3">
<h3 class="anchored" data-anchor-id="fsdp2-sharding-mechanism">FSDP2 Sharding Mechanism</h3>
<p><strong>How FSDP2 shards the model:</strong></p>
<ol type="1">
<li><strong>Parameter Sharding</strong>: Each GPU stores 1/N of the model parameters</li>
<li><strong>All-Gather</strong>: During forward pass, GPUs gather needed parameters from others</li>
<li><strong>Computation</strong>: Full parameters are used for computation</li>
<li><strong>Free</strong>: Parameters are freed after use to save memory</li>
<li><strong>Gradient Reduction</strong>: Gradients are reduced (averaged) across GPUs</li>
<li><strong>Reduce-Scatter</strong>: Each GPU receives only its gradient shard</li>
</ol>
<p><strong>Memory savings with 4 GPUs:</strong></p>
<ul>
<li>Each GPU stores ~25% of parameters</li>
<li>Temporarily gathers full parameters for computation</li>
<li>Peak memory is much lower than replicating full model</li>
</ul>
</section>
<section id="auto-wrap-policy" class="level3">
<h3 class="anchored" data-anchor-id="auto-wrap-policy">Auto-Wrap Policy</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>auto_wrap_policy<span class="op">=</span><span class="st">"transformer_based_wrap"</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>transformer_cls_names_to_wrap<span class="op">=</span>[<span class="st">"LlamaDecoderLayer"</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What this does:</strong></p>
<ul>
<li>Each transformer decoder layer becomes a separate FSDP unit</li>
<li>Parameters are sharded at the layer level</li>
<li>Provides good balance between:
<ul>
<li>Communication efficiency (fewer all-gathers)</li>
<li>Memory efficiency (fine-grained sharding)</li>
</ul></li>
</ul>
</section>
<section id="why-cpu_ram_efficient_loadingfalse" class="level3">
<h3 class="anchored" data-anchor-id="why-cpu_ram_efficient_loadingfalse">Why cpu_ram_efficient_loading=False?</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>cpu_ram_efficient_loading<span class="op">=</span><span class="va">False</span>  <span class="co"># Incompatible with FP8 torchao</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>CPU-efficient loading creates the model on CPU first, then transfers to GPU. This is <strong>incompatible with torchao’s FP8 conversion</strong>, which must happen on GPU. Setting this to <code>False</code> ensures the model is created directly on GPU.</p>
<hr>
</section>
</section>
<section id="our-implementation-code-highlights" class="level2">
<h2 class="anchored" data-anchor-id="our-implementation-code-highlights">Our Implementation: Code Highlights</h2>
<p>Our benchmark implementation (<code>fp8_benchmark.py</code>) builds on these concepts to create a comprehensive FP8 vs BF16 comparison framework. Let’s examine key highlights from the codebase.</p>
<section id="architecture-detection" class="level3">
<h3 class="anchored" data-anchor-id="architecture-detection">Architecture Detection</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Lines 62-90: Determine transformer layer class</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="st">"Qwen"</span> <span class="kw">in</span> args.model_name:</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    layer <span class="op">=</span> <span class="st">"Qwen3DecoderLayer"</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="cf">elif</span> <span class="st">"mistral"</span> <span class="kw">in</span> args.model_name.lower():</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    layer <span class="op">=</span> <span class="st">"MistralDecoderLayer"</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="cf">elif</span> <span class="st">"phi"</span> <span class="kw">in</span> args.model_name.lower():</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    layer <span class="op">=</span> <span class="st">"Phi3DecoderLayer"</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="cf">elif</span> <span class="st">"gemma"</span> <span class="kw">in</span> args.model_name.lower():</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    layer <span class="op">=</span> <span class="st">"GemmaDecoderLayer"</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="cf">elif</span> <span class="st">"gpt"</span> <span class="kw">in</span> args.model_name.lower():</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"gpt-oss"</span> <span class="kw">in</span> args.model_name.lower():</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>        layer <span class="op">=</span> <span class="st">"GPT2Block"</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="st">"gpt-neo"</span> <span class="kw">in</span> args.model_name.lower():</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>        layer <span class="op">=</span> <span class="st">"GPTNeoBlock"</span></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... more GPT variants</span></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>    layer <span class="op">=</span> <span class="st">"LlamaDecoderLayer"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Why this matters:</strong> Different model architectures use different layer class names. FSDP2’s auto-wrap policy needs the correct class name to shard the model properly. Supporting multiple architectures allows comprehensive benchmarking across model families.</p>
</section>
<section id="fsdp2-fp8-integration" class="level3">
<h3 class="anchored" data-anchor-id="fsdp2-fp8-integration">FSDP2 + FP8 Integration</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Lines 92-111: Configure FSDP2 with FP8</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>fsdp2_plugin <span class="op">=</span> FullyShardedDataParallelPlugin(</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    fsdp_version<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    cpu_ram_efficient_loading<span class="op">=</span><span class="va">False</span>,  <span class="co"># Critical for FP8</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    auto_wrap_policy<span class="op">=</span><span class="st">"transformer_based_wrap"</span>,</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    transformer_cls_names_to_wrap<span class="op">=</span>[layer],</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>fsdp2_plugin.set_mixed_precision(args.precision)</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>fp8_config <span class="op">=</span> Float8LinearConfig(</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    enable_fsdp_float8_all_gather<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>kwargs <span class="op">=</span> []</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> args.precision <span class="op">==</span> <span class="st">"fp8"</span>:</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>    kwargs <span class="op">=</span> [AORecipeKwargs(config<span class="op">=</span>fp8_config)]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Integration flow:</strong></p>
<ol type="1">
<li>FSDP2 plugin configured for transformer-based wrapping</li>
<li>Mixed precision set to “fp8” or “bf16”</li>
<li>FP8 config enables optimized all-gather</li>
<li>Config passed to Accelerator via kwargs_handlers</li>
</ol>
</section>
<section id="model-initialization-strategy" class="level3">
<h3 class="anchored" data-anchor-id="model-initialization-strategy">Model Initialization Strategy</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Lines 124-127: Random initialization for benchmarking</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_config(</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    AutoConfig.from_pretrained(args.model_name, use_cache<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    torch_dtype<span class="op">=</span>torch.bfloat16,</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Key observation:</strong> We use <code>from_config()</code> instead of <code>from_pretrained()</code>, creating models with <strong>random weights</strong>. This is intentional for benchmarking:</p>
<p>✅ <strong>Advantages:</strong></p>
<ul>
<li>Much faster initialization (no weight loading)</li>
<li>Sufficient for performance testing</li>
<li>Loss values still meaningful for convergence comparison</li>
</ul>
<p>❌ <strong>Not suitable for:</strong></p>
<ul>
<li>Fine-tuning tasks</li>
<li>Evaluating model quality</li>
<li>Production training</li>
</ul>
<p><strong>This is a pre-training benchmark</strong>, not actual pre-training. We run only 50-1000 steps to measure performance, not the billions of steps needed for real pre-training.</p>
</section>
<section id="performance-tracking" class="level3">
<h3 class="anchored" data-anchor-id="performance-tracking">Performance Tracking</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Lines 143-157: Training loop with metrics</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(<span class="op">**</span>batch)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> outputs.loss</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    accelerator.backward(loss)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Track performance metrics</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    metrics <span class="op">=</span> performance_tracker.step(</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>        batch[<span class="st">"input_ids"</span>].shape[<span class="dv">1</span>],</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>        model_flops_per_token</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Tracked metrics:</strong></p>
<ul>
<li><strong>Tokens/second</strong>: Total tokens processed per second</li>
<li><strong>Steps/second</strong>: Training iterations per second</li>
<li><strong>TFLOPs</strong>: Teraflops (trillion floating-point operations per second)</li>
<li><strong>MFU</strong>: Model FLOPs Utilization (% of theoretical peak)</li>
<li><strong>GPU memory</strong>: Active, allocated, and reserved memory</li>
</ul>
</section>
<section id="loss-function" class="level3">
<h3 class="anchored" data-anchor-id="loss-function">Loss Function</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The loss is automatically computed inside the model</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model(<span class="op">**</span>batch)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> outputs.loss  <span class="co"># Cross-entropy loss</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>When <code>labels</code> are provided to a HuggingFace causal language model, it automatically computes <strong>cross-entropy loss</strong> for next-token prediction:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Internal computation (conceptual)</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> F.cross_entropy(</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    logits.view(<span class="op">-</span><span class="dv">1</span>, vocab_size),  <span class="co"># Predicted token probabilities</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    labels.view(<span class="op">-</span><span class="dv">1</span>),               <span class="co"># Actual next tokens</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    ignore_index<span class="op">=-</span><span class="dv">100</span>              <span class="co"># Ignore padding tokens</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This measures how well the model predicts the next token given previous context.</p>
</section>
<section id="fsdp-communication-pattern" class="level3">
<h3 class="anchored" data-anchor-id="fsdp-communication-pattern">FSDP Communication Pattern</h3>
<p>During training, FSDP2 follows this communication pattern:</p>
<p><strong>Forward Pass:</strong></p>
<pre><code>1. All-gather parameters in FP8 (if enabled) or BF16
2. Upcast to BF16 after gathering
3. Compute forward pass
4. Free gathered parameters
5. Store activations for backward pass</code></pre>
<p><strong>Backward Pass:</strong></p>
<pre><code>1. All-gather parameters again
2. Compute gradients
3. Reduce-scatter gradients (average across GPUs)
4. Each GPU receives its gradient shard
5. Free gathered parameters
6. Update local parameter shard</code></pre>
<p>This pattern enables training models larger than single-GPU memory while minimizing communication overhead through FP8 compression.</p>
<hr>
</section>
</section>
<section id="experimental-setup-benchmarking-on-nvidia-b200" class="level2">
<h2 class="anchored" data-anchor-id="experimental-setup-benchmarking-on-nvidia-b200">Experimental Setup: Benchmarking on NVIDIA B200</h2>
<section id="hardware-configuration" class="level3">
<h3 class="anchored" data-anchor-id="hardware-configuration">Hardware Configuration</h3>
<p>Our experiments were conducted on a Lambda Cloud instance with:</p>
<p><strong>GPUs:</strong> 4× NVIDIA B200 (180GB SXM6) - Blackwell Architecture</p>
<p><strong>Peak Theoretical Performance:</strong></p>
<ul>
<li><strong>FP8</strong>: 9000 TFLOPs per GPU (Tensor Cores)</li>
<li><strong>BF16</strong>: 4500 TFLOPs per GPU (Tensor Cores)</li>
<li><strong>FP32</strong>: ~600 TFLOPs per GPU</li>
</ul>
<p><strong>Memory:</strong></p>
<ul>
<li>180GB per GPU (720GB total)</li>
<li>SXM6 form factor (enables direct NVLink connectivity)</li>
</ul>
<p><strong>Interconnect: NVLink (Critical for Performance)</strong></p>
<p>This instance uses <strong>NVLink</strong> for GPU-to-GPU communication, NOT standard PCIe. This is a critical architectural advantage:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 32%">
<col style="width: 37%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>NVLink (Our Setup)</th>
<th>PCIe 5.0 (Alternative)</th>
<th>Impact</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Bandwidth per GPU</strong></td>
<td>900 GB/s bidirectional</td>
<td>~128 GB/s bidirectional</td>
<td><strong>7x higher</strong></td>
</tr>
<tr class="even">
<td><strong>Latency</strong></td>
<td>~1-2 µs</td>
<td>~5-10 µs</td>
<td><strong>5x lower</strong></td>
</tr>
<tr class="odd">
<td><strong>Topology</strong></td>
<td>Direct GPU-GPU mesh</td>
<td>Through CPU/PCIe switch</td>
<td>Direct vs indirect</td>
</tr>
<tr class="even">
<td><strong>Communication overhead</strong></td>
<td>Minimal</td>
<td>Significant</td>
<td>FSDP efficiency</td>
</tr>
<tr class="odd">
<td><strong>Multi-GPU scaling</strong></td>
<td>Near-linear</td>
<td>Sublinear</td>
<td>Training throughput</td>
</tr>
</tbody>
</table>
<p><strong>Why NVLink Matters for This Benchmark:</strong></p>
<ol type="1">
<li><p><strong>FSDP2 Communication Efficiency</strong></p>
<ul>
<li>All-gather operations: Gather parameters from all GPUs</li>
<li>Reduce-scatter operations: Average and distribute gradients</li>
<li>With NVLink: 900 GB/s per GPU × 4 GPUs = 3.6 TB/s aggregate</li>
<li>With PCIe: 128 GB/s per GPU × 4 GPUs = 512 GB/s aggregate</li>
<li><strong>Result: 7x faster parameter/gradient communication</strong></li>
</ul></li>
<li><p><strong>FP8 Communication Bandwidth Savings Amplified</strong></p>
<ul>
<li>FP8 all-gather: 8 bits/parameter vs BF16’s 16 bits/parameter</li>
<li>On NVLink: Already saturating bandwidth, 2x reduction is valuable</li>
<li>On PCIe: Would be bandwidth-starved, 2x reduction is critical</li>
<li><strong>Our benchmark shows true FP8 potential with high-bandwidth interconnect</strong></li>
</ul></li>
<li><p><strong>Impact on Measured Performance</strong></p>
<p><strong>Our Results (with NVLink):</strong></p>
<ul>
<li>4-GPU scaling efficiency: 88-95% for aggregated throughput</li>
<li>TFLOPs: ~420 TFLOPs on 4 GPUs (near-linear from 1 GPU)</li>
<li>Communication overhead: Minimal impact on compute utilization</li>
</ul>
<p><strong>Estimated Results (with PCIe 5.0):</strong></p>
<ul>
<li>4-GPU scaling efficiency: ~50-70% (communication bottleneck)</li>
<li>TFLOPs: ~300-350 TFLOPs on 4 GPUs (significant degradation)</li>
<li>Communication overhead: 20-30% of training time wasted waiting</li>
<li><strong>Lower throughput, lower MFU, worse multi-GPU scaling</strong></li>
</ul></li>
<li><p><strong>Why This Instance Configuration is Ideal for FP8 Benchmarking</strong></p>
<p>The SXM6 form factor with NVLink enables:</p>
<ul>
<li>✅ <strong>Maximum bandwidth</strong> for parameter synchronization</li>
<li>✅ <strong>Low latency</strong> for gradient averaging (critical for FP8 stability)</li>
<li>✅ <strong>True performance potential</strong> of FP8 with FSDP2</li>
<li>✅ <strong>Realistic production environment</strong> (most large-scale training uses NVLink)</li>
</ul>
<p>With PCIe, we would see:</p>
<ul>
<li>❌ Communication bottleneck hiding FP8 compute gains</li>
<li>❌ Lower overall throughput masking precision effects</li>
<li>❌ Poor multi-GPU scaling obscuring true FP8 behavior</li>
</ul></li>
</ol>
<p><strong>Real-World Implication:</strong></p>
<p>Our benchmark results represent <strong>best-case FP8 performance</strong> with optimal hardware. If deploying on PCIe-based systems:</p>
<ul>
<li>Expect <strong>20-40% lower multi-GPU throughput</strong> than reported here</li>
<li>FP8’s communication bandwidth advantage becomes <strong>more critical</strong></li>
<li>May need <strong>larger local batch sizes</strong> to amortize communication cost</li>
<li>Consider <strong>gradient accumulation</strong> to reduce synchronization frequency</li>
</ul>
<p><strong>Lambda Cloud Instance Specifications:</strong></p>
<ul>
<li>Instance type: GPU Cloud with 4× B200 SXM6</li>
<li>Network: NVLink Gen 5.0 (900 GB/s per GPU)</li>
<li>Host-to-GPU: PCIe Gen 5.0 (only for CPU-GPU transfers, not GPU-GPU)</li>
<li>Availability: Lambda Labs on-demand instances</li>
</ul>
</section>
<section id="nvidia-b200-blackwell-architecture" class="level3">
<h3 class="anchored" data-anchor-id="nvidia-b200-blackwell-architecture">NVIDIA B200 (Blackwell) Architecture</h3>
<p>The B200 represents NVIDIA’s latest generation of data center GPUs:</p>
<p><strong>Key features:</strong></p>
<ul>
<li>2nd generation Transformer Engine with FP8 support</li>
<li>Significantly higher FP8 throughput (9000 TFLOPs)</li>
<li>Larger memory capacity (180GB vs 80GB on H100)</li>
<li>Improved NVLink for multi-GPU scaling</li>
</ul>
<p><strong>Why B200 matters for FP8:</strong> The Blackwell architecture has hardware-optimized FP8 tensor cores, making it the ideal platform for evaluating FP8 training performance.</p>
</section>
<section id="software-stack" class="level3">
<h3 class="anchored" data-anchor-id="software-stack">Software Stack</h3>
<ul>
<li><strong>PyTorch</strong>: 2.0+</li>
<li><strong>torchao</strong>: 0.1.0+ (FP8 support)</li>
<li><strong>HuggingFace Transformers</strong>: 4.30.0+</li>
<li><strong>HuggingFace Accelerate</strong>: 0.20.0+ (FSDP2 support)</li>
<li><strong>CUDA</strong>: 12.1</li>
</ul>
</section>
<section id="benchmark-configuration" class="level3">
<h3 class="anchored" data-anchor-id="benchmark-configuration">Benchmark Configuration</h3>
<p><strong>Training Configuration:</strong></p>
<ul>
<li><strong>Batch size</strong>: 1 per GPU (intentionally small to isolate effects)</li>
<li><strong>Sequence lengths</strong>: 2048, 4096, 8192 tokens</li>
<li><strong>GPU counts</strong>: 1, 2, 4 GPUs</li>
<li><strong>Precision</strong>: FP8 vs BF16</li>
<li><strong>Optimization</strong>: AdamW with fused implementation</li>
<li><strong>Learning rate</strong>: 1e-5</li>
<li><strong>Training steps</strong>: 50-1000 (depending on model/configuration)</li>
</ul>
<p><strong>Models Tested:</strong></p>
<ol type="1">
<li><strong>Llama 3.2 1B</strong> - Small efficient model</li>
<li><strong>Llama 3.2 3B</strong> - Medium-sized model</li>
<li><strong>Llama 3.1 8B</strong> - Large model</li>
<li><strong>Qwen3 4B</strong> - Alternative architecture</li>
<li><strong>Qwen3 14B</strong> - Very large model (4 GPUs only)</li>
</ol>
</section>
<section id="dataset" class="level3">
<h3 class="anchored" data-anchor-id="dataset">Dataset</h3>
<p><strong>TinyStories</strong>: A dataset of simple short stories</p>
<ul>
<li>Used for pre-training benchmarks</li>
<li>Tokenized and packed into fixed-length sequences</li>
<li>First 10% of dataset used (~10,000 sequences)</li>
</ul>
</section>
<section id="experimental-design" class="level3">
<h3 class="anchored" data-anchor-id="experimental-design">Experimental Design</h3>
<p><strong>Goal:</strong> Compare FP8 vs BF16 across:</p>
<ol type="1">
<li><strong>Performance metrics</strong>: TFLOPs, tokens/s, MFU</li>
<li><strong>Training quality</strong>: Loss convergence</li>
<li><strong>Scalability</strong>: 1, 2, 4 GPU configurations</li>
<li><strong>Model sizes</strong>: 1B to 14B parameters</li>
<li><strong>Sequence lengths</strong>: 2048 to 8192 tokens</li>
</ol>
<p><strong>Controlled variables:</strong></p>
<ul>
<li>Same random seed (42) for reproducibility</li>
<li>Same model architectures and hyperparameters</li>
<li>Same dataset and data preprocessing</li>
<li>Same optimizer and learning rate</li>
</ul>
<p><strong>Measured variables:</strong></p>
<ul>
<li>Computational throughput (TFLOPs)</li>
<li>Token processing throughput (tokens/s)</li>
<li>Hardware utilization (MFU %)</li>
<li>Training loss progression</li>
<li>GPU memory usage</li>
</ul>
</section>
<section id="why-batch-size-1" class="level3">
<h3 class="anchored" data-anchor-id="why-batch-size-1">Why Batch Size = 1?</h3>
<p>We intentionally used batch_size=1 per GPU to:</p>
<ul>
<li>✅ <strong>Isolate sequence length effects</strong>: Focus on how sequence length impacts performance without batch size confounding</li>
<li>✅ <strong>Reveal precision sensitivity</strong>: Smaller batches expose FP8’s precision limitations (as we’ll see in results)</li>
<li>✅ <strong>Test worst-case scenario</strong>: If FP8 works well at batch_size=1, it will excel at larger batches</li>
</ul>
<p>❌ <strong>Not representative of production</strong>: Real training typically uses batch_size=4-8 per GPU for better efficiency</p>
<p>This design choice led to one of our most interesting findings: the dramatic difference between FP8 and BF16 on single GPU vs multi-GPU setups.</p>
</section>
<section id="important-note-pre-training-benchmark-vs-production-pre-training" class="level3">
<h3 class="anchored" data-anchor-id="important-note-pre-training-benchmark-vs-production-pre-training">Important Note: Pre-Training Benchmark vs Production Pre-Training</h3>
<p>This benchmark implements a <strong>pre-training setup</strong> (training from scratch with random initialization) rather than fine-tuning or inference. However, it’s crucial to understand that <strong>this is a benchmark for measuring performance, not actual production pre-training</strong>.</p>
<section id="evidence-this-is-pre-training-not-fine-tuning" class="level4">
<h4 class="anchored" data-anchor-id="evidence-this-is-pre-training-not-fine-tuning">Evidence This is Pre-Training (Not Fine-Tuning)</h4>
<p>Looking at our code (fp8_benchmark.py, lines 124-127):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_config(</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    AutoConfig.from_pretrained(args.model_name, use_cache<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    torch_dtype<span class="op">=</span>torch.bfloat16,</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Key observation:</strong> We use <code>from_config()</code> instead of <code>from_pretrained()</code>, meaning:</p>
<ul>
<li>✅ Model starts with <strong>random initialization</strong> (not pretrained weights)</li>
<li>✅ Trains from scratch on text corpus (TinyStories dataset)</li>
<li>✅ Uses cross-entropy loss for next-token prediction</li>
<li>✅ This is the definition of <strong>pre-training</strong></li>
</ul>
<p>If this were fine-tuning, we would see:</p>
<ul>
<li>❌ <code>from_pretrained()</code> to load pretrained weights</li>
<li>❌ Task-specific dataset (not general text)</li>
<li>❌ Potentially different loss function or training objective</li>
</ul>
</section>
<section id="evidence-this-is-a-benchmark-not-production-pre-training" class="level4">
<h4 class="anchored" data-anchor-id="evidence-this-is-a-benchmark-not-production-pre-training">Evidence This is a Benchmark (Not Production Pre-Training)</h4>
<p>However, several characteristics distinguish this from actual production pre-training:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 28%">
<col style="width: 43%">
</colgroup>
<thead>
<tr class="header">
<th>Characteristic</th>
<th>This Benchmark</th>
<th>Production Pre-Training</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Training steps</strong></td>
<td>50-1000 steps</td>
<td>Billions of steps</td>
</tr>
<tr class="even">
<td><strong>Training duration</strong></td>
<td>Minutes to hours</td>
<td>Weeks to months</td>
</tr>
<tr class="odd">
<td><strong>Model initialization</strong></td>
<td>Random weights</td>
<td>Random weights</td>
</tr>
<tr class="even">
<td><strong>Primary goal</strong></td>
<td><strong>Measure performance</strong></td>
<td><strong>Train useful model</strong></td>
</tr>
<tr class="odd">
<td><strong>Model saving</strong></td>
<td>❌ Not saved</td>
<td>✅ Checkpoints saved</td>
</tr>
<tr class="even">
<td><strong>Dataset</strong></td>
<td>TinyStories (simple)</td>
<td>Diverse web text, books</td>
</tr>
<tr class="odd">
<td><strong>Metrics tracked</strong></td>
<td>TFLOPs, tokens/s, MFU</td>
<td>Loss, perplexity, downstream task performance</td>
</tr>
<tr class="even">
<td><strong>Hardware scale</strong></td>
<td>1-4 GPUs</td>
<td>100-1000s of GPUs</td>
</tr>
<tr class="odd">
<td><strong>Total tokens</strong></td>
<td>~10M tokens</td>
<td>Trillions of tokens</td>
</tr>
<tr class="even">
<td><strong>Cost</strong></td>
<td>$10-100</td>
<td>Millions of dollars</td>
</tr>
</tbody>
</table>
</section>
<section id="primary-use-case-performance-benchmarking" class="level4">
<h4 class="anchored" data-anchor-id="primary-use-case-performance-benchmarking">Primary Use Case: Performance Benchmarking</h4>
<p>The <strong>primary purpose</strong> of this code is to:</p>
<p>✅ <strong>Measure and compare FP8 vs BF16 training performance</strong></p>
<ul>
<li>Computational throughput (TFLOPs)</li>
<li>Token processing speed (tokens/s)</li>
<li>Hardware utilization (MFU %)</li>
<li>Training loss convergence patterns</li>
<li>Memory usage</li>
</ul>
<p>✅ <strong>Quantify benefits of FP8 training</strong></p>
<ul>
<li>Speedup: ~10-15% TFLOPs improvement</li>
<li>Memory: 50% reduction for parameters/activations</li>
<li>Communication: 2x bandwidth reduction in FSDP</li>
<li>Quality: Identify when FP8 matches BF16 (multi-GPU) vs when it fails (single GPU)</li>
</ul>
<p>✅ <strong>Guide infrastructure decisions</strong></p>
<ul>
<li>Should we use FP8 for our training job?</li>
<li>What’s the minimum GPU count for FP8?</li>
<li>What batch size do we need?</li>
<li>Which sequence length is most efficient?</li>
</ul>
</section>
<section id="why-random-initialization-is-sufficient-for-benchmarking" class="level4">
<h4 class="anchored" data-anchor-id="why-random-initialization-is-sufficient-for-benchmarking">Why Random Initialization is Sufficient for Benchmarking</h4>
<p>Random initialization works for performance benchmarking because:</p>
<ol type="1">
<li><strong>Computational patterns are identical</strong>: Random weights produce the same GEMM (matrix multiplication) operations as pretrained weights</li>
<li><strong>Loss convergence is meaningful</strong>: Even random initialization shows clear convergence trends that reveal optimization dynamics</li>
<li><strong>Much faster</strong>: No need to download/load multi-GB pretrained checkpoints</li>
<li><strong>Reproducible</strong>: Fixed random seed ensures consistent results</li>
</ol>
<p><strong>What random initialization doesn’t show:</strong></p>
<ul>
<li>Final model quality on downstream tasks</li>
<li>Long-term training stability (1000s of steps)</li>
<li>Interactions with pretrained weight distributions</li>
</ul>
</section>
<section id="production-pre-training-would-require" class="level4">
<h4 class="anchored" data-anchor-id="production-pre-training-would-require">Production Pre-Training Would Require</h4>
<p>To turn this into actual production pre-training, you would need:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Much longer training</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>num_steps <span class="op">=</span> <span class="dv">100_000_000</span>  <span class="co"># Billions instead of 50</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Larger, more diverse dataset</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(<span class="st">"c4"</span>, split<span class="op">=</span><span class="st">"train"</span>)  <span class="co"># Not TinyStories</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Save checkpoints</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> step <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>    accelerator.save_state(<span class="ss">f"checkpoint-</span><span class="sc">{</span>step<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Track model quality metrics</span></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>eval_perplexity <span class="op">=</span> evaluate_on_validation_set(model)</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>accelerator.log({<span class="st">"perplexity"</span>: eval_perplexity})</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Much larger scale</span></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>num_gpus <span class="op">=</span> <span class="dv">256</span>  <span class="co"># Not just 1-4</span></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>batch_size_per_gpu <span class="op">=</span> <span class="dv">4</span>  <span class="co"># Not just 1</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="value-of-this-benchmark-approach" class="level4">
<h4 class="anchored" data-anchor-id="value-of-this-benchmark-approach">Value of This Benchmark Approach</h4>
<p>The <strong>benchmark approach</strong> (short runs with random initialization) provides <strong>invaluable insights</strong> without the time and cost of full pre-training:</p>
<p><strong>Time savings:</strong></p>
<ul>
<li>Benchmark: Hours to complete full sweep</li>
<li>Production pre-training: Weeks to months</li>
</ul>
<p><strong>Cost savings:</strong></p>
<ul>
<li>Benchmark: $50-500 in GPU time</li>
<li>Production pre-training: $1M-100M in GPU time</li>
</ul>
<p><strong>Insights gained:</strong></p>
<ul>
<li>✅ Performance characteristics of FP8 vs BF16</li>
<li>✅ Optimal batch size and sequence length</li>
<li>✅ Multi-GPU scaling efficiency</li>
<li>✅ Hardware utilization (MFU)</li>
<li>✅ Critical finding: FP8 requires multi-GPU or larger batches</li>
</ul>
<p>These insights inform <strong>actual production training decisions</strong>, allowing teams to optimize their multi-million dollar training jobs before committing resources.</p>
<hr>
</section>
</section>
</section>
<section id="experimental-results-and-analysis" class="level2">
<h2 class="anchored" data-anchor-id="experimental-results-and-analysis">Experimental Results and Analysis</h2>
<p>Our comprehensive benchmark reveals nuanced performance characteristics of FP8 training across different configurations. Let’s examine each metric with detailed plots and analysis.</p>
<section id="computational-throughput-tflops-vs-sequence-length" class="level3">
<h3 class="anchored" data-anchor-id="computational-throughput-tflops-vs-sequence-length">Computational Throughput: TFLOPs vs Sequence Length</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./assets/tflops_vs_seqlen_by_gpu.png" class="img-fluid figure-img"></p>
<figcaption>TFLOPs vs Sequence Length</figcaption>
</figure>
</div>
<section id="key-findings" class="level4">
<h4 class="anchored" data-anchor-id="key-findings">Key Findings</h4>
<ol type="1">
<li><p><strong>FP8 achieves 10-15% higher TFLOPs than BF16</strong> across all configurations</p>
<ul>
<li>Llama 3.1 8B on 1 GPU: ~430 TFLOPs (FP8) vs ~380 TFLOPs (BF16)</li>
<li>Advantage is consistent across all model sizes</li>
</ul></li>
<li><p><strong>Sequence length 4096 is the sweet spot</strong> for computational efficiency</p>
<ul>
<li>Both 2048 (too short) and 8192 (memory-bound) show reduced TFLOPs</li>
<li>The 4096 sweet spot appears across all GPU counts</li>
</ul></li>
<li><p><strong>Larger models achieve higher absolute TFLOPs</strong></p>
<ul>
<li>Llama 3.1 8B: ~400-430 TFLOPs</li>
<li>Llama 3.2 3B: ~240-280 TFLOPs</li>
<li>Llama 3.2 1B: ~170-230 TFLOPs</li>
<li>This reflects higher arithmetic intensity in larger models</li>
</ul></li>
<li><p><strong>Multi-GPU scaling increases total TFLOPs but reduces per-GPU efficiency</strong></p>
<ul>
<li>Communication overhead becomes more significant</li>
<li>Still beneficial for overall throughput</li>
</ul></li>
</ol>
</section>
<section id="technical-explanation" class="level4">
<h4 class="anchored" data-anchor-id="technical-explanation">Technical Explanation</h4>
<p><strong>Why does FP8 achieve higher TFLOPs?</strong></p>
<p>FP8 operations are fundamentally faster on B200 tensor cores:</p>
<ul>
<li>FP8 peak: 9000 TFLOPs</li>
<li>BF16 peak: 4500 TFLOPs</li>
<li>Theoretical 2x advantage</li>
</ul>
<p>However, we see only ~10-15% improvement because:</p>
<ul>
<li>Dynamic scaling overhead in FP8</li>
<li>Memory bandwidth bottlenecks (same for both precisions)</li>
<li>Non-compute operations (normalization, etc.) don’t benefit from FP8</li>
</ul>
<p><strong>Why does sequence length 4096 perform best?</strong></p>
<p>This represents an optimal balance: - <strong>2048 (too short)</strong>: Kernel launch overhead becomes proportionally significant; insufficient work to saturate tensor cores - <strong>4096 (optimal)</strong>: Attention matrices large enough for efficient tensor core utilization while memory bandwidth is still adequate - <strong>8192 (too long)</strong>: Memory bandwidth becomes the bottleneck; attention’s O(n²) memory footprint dominates</p>
<p><strong>Why do larger models achieve higher TFLOPs?</strong></p>
<p>Arithmetic intensity = FLOPs / bytes accessed:</p>
<ul>
<li><strong>Larger models</strong>: More FLOPs per byte (higher arithmetic intensity) → compute-bound → high TFLOPs</li>
<li><strong>Smaller models</strong>: Fewer FLOPs per byte (lower arithmetic intensity) → memory-bound → lower TFLOPs</li>
</ul>
<p>This is why Llama 8B achieves ~400 TFLOPs while Llama 1B achieves only ~200 TFLOPs.</p>
<hr>
</section>
</section>
<section id="token-processing-throughput" class="level3">
<h3 class="anchored" data-anchor-id="token-processing-throughput">Token Processing Throughput</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./assets/throughput_vs_seqlen_by_gpu.png" class="img-fluid figure-img"></p>
<figcaption>Throughput vs Sequence Length</figcaption>
</figure>
</div>
<section id="key-findings-1" class="level4">
<h4 class="anchored" data-anchor-id="key-findings-1">Key Findings</h4>
<ol type="1">
<li><p><strong>Throughput follows O(n²) scaling</strong> with sequence length</p>
<ul>
<li>Doubling sequence length roughly halves tokens/s</li>
<li>Reflects quadratic attention complexity</li>
</ul></li>
<li><p><strong>Smaller models process dramatically more tokens/s</strong></p>
<ul>
<li>Llama 3.2 1B: ~42,000 tokens/s (seq_len=2048, 1 GPU)</li>
<li>Llama 3.1 8B: ~11,000 tokens/s (same config)</li>
<li>8x parameters → 4x throughput reduction</li>
</ul></li>
<li><p><strong>FP8 and BF16 show comparable tokens/s</strong></p>
<ul>
<li>FP8 slight edge on 1 GPU (~5-10% improvement)</li>
<li>Difference narrows on multi-GPU and longer sequences</li>
<li>Memory bandwidth equalizes performance</li>
</ul></li>
<li><p><strong>Multi-GPU reduces per-device tokens/s</strong></p>
<ul>
<li>With batch_size=1, GPUs must synchronize</li>
<li>Communication overhead proportionally expensive</li>
<li>Artifact of experimental design, not FP8 limitation</li>
</ul></li>
</ol>
</section>
<section id="technical-explanation-1" class="level4">
<h4 class="anchored" data-anchor-id="technical-explanation-1">Technical Explanation</h4>
<p><strong>Why does throughput decrease quadratically?</strong></p>
<p>Self-attention complexity is O(n²):</p>
<pre><code>For sequence length n:

- Attention matrix: n × n
- Computation: n² × d (where d = hidden dimension)
- Memory: O(n²) for attention scores</code></pre>
<p>Empirical observation:</p>
<ul>
<li>2048 → 4096: Tokens/s halves</li>
<li>4096 → 8192: Tokens/s halves again</li>
</ul>
<p><strong>Why is FP8 advantage minimal for tokens/s?</strong></p>
<p>While FP8 achieves higher TFLOPs, tokens/s depends on:</p>
<ol type="1">
<li><strong>Compute time</strong> (where FP8 helps)</li>
<li><strong>Memory bandwidth</strong> (same for both precisions)</li>
<li><strong>Communication</strong> (FSDP overhead)</li>
<li><strong>Non-compute ops</strong> (no FP8 benefit)</li>
</ol>
<p>At longer sequences, memory bandwidth dominates, equalizing FP8 and BF16.</p>
<p><strong>Why do smaller models process more tokens?</strong></p>
<p>Tokens/s = 1 / (time per token) Time per token ∝ model size × sequence length</p>
<p>Smaller models:</p>
<ul>
<li>Fewer parameters → less computation per token</li>
<li>Lower memory footprint → better cache utilization</li>
<li>Faster forward/backward passes</li>
</ul>
<hr>
</section>
</section>
<section id="system-level-aggregated-throughput" class="level3">
<h3 class="anchored" data-anchor-id="system-level-aggregated-throughput">System-Level Aggregated Throughput</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./assets/aggregated_throughput_vs_seqlen_by_gpu.png" class="img-fluid figure-img"></p>
<figcaption>Aggregated Throughput</figcaption>
</figure>
</div>
<section id="key-findings-2" class="level4">
<h4 class="anchored" data-anchor-id="key-findings-2">Key Findings</h4>
<ol type="1">
<li><p><strong>Near-linear multi-GPU scaling</strong> despite reduced per-device efficiency</p>
<ul>
<li>4 GPUs achieve 3.5-3.8x throughput vs 1 GPU</li>
<li>88-95% scaling efficiency (excellent for FSDP2)</li>
</ul></li>
<li><p><strong>Peak system throughput: 120,000-130,000 tokens/s</strong></p>
<ul>
<li>Llama 3.2 1B on 4 GPUs at seq_len=2048</li>
<li>Demonstrates FSDP2’s strong scaling properties</li>
</ul></li>
<li><p><strong>FP8 and BF16 remain comparable</strong> in aggregate</p>
<ul>
<li>&lt;10% difference across most configurations</li>
<li>Communication and memory bandwidth limit FP8 advantage</li>
</ul></li>
<li><p><strong>Sequence length still dominates performance</strong></p>
<ul>
<li>O(n²) scaling persists in aggregate metrics</li>
<li>Even 4 GPUs struggle at seq_len=8192</li>
</ul></li>
</ol>
</section>
<section id="technical-explanation-2" class="level4">
<h4 class="anchored" data-anchor-id="technical-explanation-2">Technical Explanation</h4>
<p><strong>Why near-linear scaling?</strong></p>
<p>FSDP2’s efficiency comes from:</p>
<ul>
<li>Overlapping computation and communication</li>
<li>Efficient reduce-scatter for gradients</li>
<li>NVLink high-bandwidth interconnect on B200</li>
</ul>
<p>Scaling efficiency = (Throughput_N_GPUs / N) / Throughput_1_GPU</p>
<ul>
<li>4 GPUs: ~88% efficiency (excellent!)</li>
</ul>
<p><strong>What limits perfect linear scaling?</strong></p>
<ol type="1">
<li><strong>Communication overhead</strong>: All-gather and reduce-scatter operations</li>
<li><strong>Synchronization</strong>: Barrier points in training loop</li>
<li><strong>Batch size = 1</strong>: Cannot parallelize across samples</li>
<li><strong>Memory bandwidth contention</strong>: Shared memory channels</li>
</ol>
<p><strong>Practical implications:</strong></p>
<p>For production training:</p>
<ul>
<li>Use larger batch sizes (4-8 per GPU)</li>
<li>Expected scaling efficiency: 90-95% with optimal batch size</li>
<li>FP8’s communication bandwidth savings more impactful at larger scale</li>
</ul>
<p><strong>Critical Hardware Note: NVLink vs PCIe</strong></p>
<p>Our excellent scaling results (88-95% efficiency) are achieved with <strong>NVLink interconnect (900 GB/s per GPU)</strong>, not standard PCIe.</p>
<p><strong>If using PCIe-based systems (128 GB/s per GPU):</strong></p>
<ul>
<li>Scaling efficiency would drop to ~50-70% (communication bottleneck)</li>
<li>Communication overhead would dominate at 4 GPUs</li>
<li>Aggregated throughput would be <strong>20-40% lower</strong> than reported here</li>
<li>Would need larger batch sizes to amortize communication cost</li>
</ul>
<p><strong>Why this matters for FP8:</strong></p>
<ul>
<li>NVLink: Already high bandwidth → FP8’s 2x savings is nice-to-have</li>
<li>PCIe: Bandwidth-starved → FP8’s 2x savings becomes <strong>critical</strong></li>
<li><strong>Our results show FP8’s best-case performance</strong> with optimal interconnect</li>
<li>Real-world PCIe deployments would see even greater FP8 advantage</li>
</ul>
<p><strong>Recommendation:</strong> For multi-GPU FP8 training at scale, prioritize NVLink-enabled instances (SXM form factor) or high-bandwidth interconnects. On PCIe systems, FP8’s communication benefits become more important than compute speedup.</p>
<hr>
</section>
</section>
<section id="hardware-utilization-mfu-analysis" class="level3">
<h3 class="anchored" data-anchor-id="hardware-utilization-mfu-analysis">Hardware Utilization: MFU Analysis</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./assets/mfu_vs_seqlen_by_gpu.png" class="img-fluid figure-img"></p>
<figcaption>MFU vs Sequence Length</figcaption>
</figure>
</div>
<section id="key-findings-3" class="level4">
<h4 class="anchored" data-anchor-id="key-findings-3">Key Findings</h4>
<ol type="1">
<li><p><strong>Overall MFU is very low: 2-9%</strong></p>
<ul>
<li>Expected given batch_size=1 constraint</li>
<li>B200’s 9000 TFLOPs peak severely underutilized</li>
</ul></li>
<li><p><strong>Llama 3.1 8B achieves highest MFU: ~8-9%</strong></p>
<ul>
<li>Larger models better utilize tensor cores</li>
<li>Higher arithmetic intensity</li>
</ul></li>
<li><p><strong>MFU peaks at sequence length 4096</strong></p>
<ul>
<li>Matches TFLOPs sweet spot</li>
<li>Best balance of compute vs memory</li>
</ul></li>
<li><p><strong>FP8 and BF16 show nearly identical MFU</strong></p>
<ul>
<li>Both ~4-8% depending on model</li>
<li>FP8’s higher peak TFLOPs offset by higher achieved TFLOPs</li>
</ul></li>
<li><p><strong>Multi-GPU marginally improves MFU</strong></p>
<ul>
<li>Communication overhead counteracts benefits</li>
<li>Larger models see more improvement</li>
</ul></li>
</ol>
</section>
<section id="technical-explanation-3" class="level4">
<h4 class="anchored" data-anchor-id="technical-explanation-3">Technical Explanation</h4>
<p><strong>Why is MFU so low?</strong></p>
<p>MFU = (Achieved TFLOPs / Peak TFLOPs) × 100%</p>
<p>For BF16 on Llama 3.1 8B: - Achieved: ~380 TFLOPs - Peak: 4500 TFLOPs - MFU: 380 / 4500 = 8.4%</p>
<p><strong>Root cause: batch_size = 1</strong></p>
<p>Modern GPUs are designed for massive parallelism: - B200 can process 100,000+ tokens in parallel - batch_size=1 × seq_len=8192 = only 8,192 tokens - ~99% of GPU capacity idle!</p>
<p><strong>Additional factors:</strong></p>
<ol type="1">
<li><strong>Non-compute operations</strong>: Data loading, normalization (no FLOPs)</li>
<li><strong>Memory bandwidth</strong>: GPUs wait for data</li>
<li><strong>Kernel launch overhead</strong>: Frequent small kernels</li>
<li><strong>FSDP communication</strong>: All-gather/reduce-scatter idle compute</li>
</ol>
<p><strong>Why is FP8 MFU comparable to BF16?</strong></p>
<p>Surprising result: FP8 sometimes shows <em>lower</em> MFU than BF16!</p>
<p>Example:</p>
<ul>
<li>BF16: 350 TFLOPs / 4500 peak = 7.8% MFU</li>
<li>FP8: 430 TFLOPs / 9000 peak = 4.8% MFU</li>
</ul>
<p>Reason:</p>
<ul>
<li>FP8 overhead (scaling, casting) reduces efficiency</li>
<li>Memory operations unchanged</li>
<li>Higher peak doesn’t translate to proportionally higher utilization</li>
</ul>
<p><strong>How to achieve 30-60% MFU (production-level):</strong></p>
<ol type="1">
<li><strong>Increase batch size to 8-16</strong>: Most impactful change</li>
<li><strong>Use gradient accumulation</strong>: Simulate larger batches</li>
<li><strong>Optimize sequence length</strong>: Stay in 2048-4096 range</li>
<li><strong>Use larger models</strong>: 8B+ parameters for better arithmetic intensity</li>
<li><strong>Enable torch.compile</strong>: Kernel fusion reduces overhead</li>
</ol>
<p><strong>Context:</strong></p>
<p>Production LLM training (GPT-3, LLaMA):</p>
<ul>
<li>Typical MFU: 30-60%</li>
<li>Batch size: 4-8 per GPU</li>
<li>Micro-batches with gradient accumulation</li>
<li>Hundreds of GPUs with optimized communication</li>
</ul>
<p>Our 2-9% MFU is expected and acceptable for this benchmark’s goals.</p>
<hr>
</section>
</section>
<section id="training-quality-the-critical-finding" class="level3">
<h3 class="anchored" data-anchor-id="training-quality-the-critical-finding">Training Quality: The Critical Finding</h3>
<p>This section presents our most significant empirical finding: <strong>the dramatic difference in FP8 vs BF16 training quality between single-GPU and multi-GPU configurations</strong>.</p>
<section id="four-gpus-fp8-and-bf16-equivalent" class="level4">
<h4 class="anchored" data-anchor-id="four-gpus-fp8-and-bf16-equivalent">Four GPUs: FP8 and BF16 Equivalent</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./assets/loss_comparison_4gpus_seqlen8192.png" class="img-fluid figure-img"></p>
<figcaption>Loss Comparison 4 GPUs</figcaption>
</figure>
</div>
<p><strong>Key Observations:</strong></p>
<ol type="1">
<li><p><strong>FP8 and BF16 curves are virtually identical</strong></p>
<ul>
<li>All models converge from loss ~12-13 to ~3-6</li>
<li>No evidence of FP8 training instability</li>
<li>Curves overlap throughout training</li>
</ul></li>
<li><p><strong>Model-specific convergence rates:</strong></p>
<ul>
<li><strong>Llama 3.1 8B</strong>: Fastest convergence (loss ~3 by step 200)</li>
<li><strong>Llama 3.2 1B/3B</strong>: Moderate convergence (loss ~3-4 by step 200)</li>
<li><strong>Qwen3 14B</strong>: Slower initial drop but smoothest curve</li>
<li><strong>Qwen3 4B</strong>: Similar to Llama 3.2 3B</li>
</ul></li>
<li><p><strong>Smooth loss curves across all models</strong></p>
<ul>
<li>Minimal oscillation</li>
<li>Consistent downward trend</li>
<li>No precision-related instabilities</li>
</ul></li>
</ol>
<p><strong>Implication:</strong> <strong>FP8 is production-ready for multi-GPU training</strong> with no quality degradation.</p>
<hr>
</section>
<section id="two-gpus-fp8-remains-comparable" class="level4">
<h4 class="anchored" data-anchor-id="two-gpus-fp8-remains-comparable">Two GPUs: FP8 Remains Comparable</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./assets/loss_comparison_2gpus_seqlen8192.png" class="img-fluid figure-img"></p>
<figcaption>Loss Comparison 2 GPUs</figcaption>
</figure>
</div>
<p><strong>Key Observations:</strong></p>
<ol type="1">
<li><p><strong>FP8 and BF16 still highly comparable</strong></p>
<ul>
<li>Nearly overlapping loss curves</li>
<li>All models converge successfully</li>
</ul></li>
<li><p><strong>Slightly more oscillation</strong> than 4-GPU case</p>
<ul>
<li>Visible in later training steps (after step 400)</li>
<li>Affects both precisions equally</li>
<li>Not a precision issue but gradient noise</li>
</ul></li>
<li><p><strong>Convergence patterns match 4-GPU results</strong></p>
<ul>
<li>Final loss values similar</li>
<li>No systematic FP8 disadvantage</li>
</ul></li>
</ol>
<p><strong>Implication:</strong> <strong>2 GPUs is sufficient for FP8 training</strong> with batch_size=1 per GPU.</p>
<hr>
</section>
<section id="single-gpu-bf16-dramatically-outperforms-fp8" class="level4">
<h4 class="anchored" data-anchor-id="single-gpu-bf16-dramatically-outperforms-fp8">Single GPU: BF16 Dramatically Outperforms FP8</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./assets/loss_comparison_1gpu_seqlen8192.png" class="img-fluid figure-img"></p>
<figcaption>Loss Comparison 1 GPU</figcaption>
</figure>
</div>
<p><strong>Key Observations:</strong></p>
<ol type="1">
<li><p><strong>BF16 significantly outperforms FP8 on all models</strong></p>
<ul>
<li>BF16 converges to loss ~5-7</li>
<li>FP8 plateaus at loss ~11-12</li>
<li>Gap: 4.5-6.5 loss units</li>
</ul></li>
<li><p><strong>FP8 shows minimal learning progress</strong></p>
<ul>
<li>Initial drop from 12.5 → 11.5</li>
<li>Then plateaus with no further improvement</li>
<li>Fails to learn effectively</li>
</ul></li>
<li><p><strong>BF16 demonstrates smooth convergence</strong></p>
<ul>
<li>Consistent downward trend</li>
<li>Reaches good loss values</li>
<li>Normal training dynamics</li>
</ul></li>
<li><p><strong>Gap is consistent across all models</strong></p>
<ul>
<li>Not model-specific</li>
<li>Fundamental interaction between precision and batch size</li>
</ul></li>
</ol>
<p><strong>Model-Specific Results:</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>BF16 Final Loss</th>
<th>FP8 Final Loss</th>
<th>Gap</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Llama 3.1 8B</td>
<td>~5.0</td>
<td>~11.5</td>
<td>~6.5</td>
</tr>
<tr class="even">
<td>Llama 3.2 1B</td>
<td>~6.5</td>
<td>~11.0</td>
<td>~4.5</td>
</tr>
<tr class="odd">
<td>Llama 3.2 3B</td>
<td>~5.5</td>
<td>~11.0</td>
<td>~5.5</td>
</tr>
<tr class="even">
<td>Qwen3 4B</td>
<td>~6.5</td>
<td>~11.5</td>
<td>~5.0</td>
</tr>
</tbody>
</table>
<p><strong>Implication:</strong> <strong>Never use FP8 for single-GPU training with small batches.</strong></p>
<hr>
</section>
</section>
<section id="the-precision-noise-trade-off-theoretical-analysis" class="level3">
<h3 class="anchored" data-anchor-id="the-precision-noise-trade-off-theoretical-analysis">The Precision-Noise Trade-off: Theoretical Analysis</h3>
<section id="why-does-fp8-fail-on-1-gpu-but-succeed-on-2-gpus" class="level4">
<h4 class="anchored" data-anchor-id="why-does-fp8-fail-on-1-gpu-but-succeed-on-2-gpus">Why Does FP8 Fail on 1 GPU but Succeed on 2+ GPUs?</h4>
<p>This is the most important theoretical insight from our benchmark. The answer lies in the <strong>interaction between numerical precision and gradient estimation quality</strong>.</p>
</section>
<section id="gradient-noise-dominates-at-batch-size-1" class="level4">
<h4 class="anchored" data-anchor-id="gradient-noise-dominates-at-batch-size-1">Gradient Noise Dominates at Batch Size 1</h4>
<p>Stochastic Gradient Descent (SGD) relies on gradient estimates:</p>
<pre><code>True gradient = E[∇L(θ, x)]  (expectation over all data)
Estimated gradient = ∇L(θ, x_batch)  (gradient from batch)</code></pre>
<p><strong>With batch_size=1:</strong></p>
<ul>
<li>Each gradient comes from a single sample</li>
<li>Extremely high variance (single sample cannot represent distribution)</li>
<li>“Noise” (sampling error) dominates “signal” (true gradient direction)</li>
</ul>
<p><strong>The gradient noise problem:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Single sample gradient (batch_size=1)</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>grad_sample_1 <span class="op">=</span> [<span class="fl">0.5</span>, <span class="op">-</span><span class="fl">2.3</span>, <span class="fl">0.1</span>, ...]  <span class="co"># High variance</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>grad_sample_2 <span class="op">=</span> [<span class="op">-</span><span class="fl">0.3</span>, <span class="fl">1.8</span>, <span class="op">-</span><span class="fl">0.5</span>, ...]  <span class="co"># Very different!</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>grad_sample_3 <span class="op">=</span> [<span class="fl">0.8</span>, <span class="op">-</span><span class="fl">0.5</span>, <span class="fl">0.3</span>, ...]  <span class="co"># Also very different!</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="co"># True gradient (average of many samples)</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>grad_true <span class="op">=</span> [<span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.4</span>, <span class="fl">0.1</span>, ...]  <span class="co"># Much more stable</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="fp8s-limited-precision-amplifies-the-noise" class="level4">
<h4 class="anchored" data-anchor-id="fp8s-limited-precision-amplifies-the-noise">2. FP8’s Limited Precision Amplifies the Noise</h4>
<p>FP8 quantization introduces errors:</p>
<p><strong>Precision comparison:</strong></p>
<ul>
<li>FP32: 23-bit mantissa (~7 decimal digits)</li>
<li>BF16: 7-bit mantissa (~3 decimal digits)</li>
<li>FP8: 2-3 bit mantissa (~1-2 decimal digits)</li>
</ul>
<p><strong>FP8 quantization errors:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># BF16 → FP8 conversion loses precision</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>true_gradient <span class="op">=</span> <span class="fl">0.000123456</span>  (BF16)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>fp8_gradient  <span class="op">=</span> <span class="fl">0.000123</span>     (FP8, rounded)</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>error <span class="op">=</span> <span class="fl">0.000000456</span>          (quantization error)</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Small values critical for optimization are lost!</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>small_component <span class="op">=</span> <span class="fl">0.00001</span>    (BF16)</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>fp8_component   <span class="op">=</span> <span class="fl">0.0</span>        (FP8, underflow<span class="op">!</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>When noise is high (batch_size=1):</strong></p>
<ul>
<li>FP8’s precision is insufficient to preserve gradient signal</li>
<li>Important small gradient components lost to quantization</li>
<li>Optimization cannot make progress</li>
</ul>
</section>
<section id="multi-gpu-gradient-averaging-as-noise-reduction" class="level4">
<h4 class="anchored" data-anchor-id="multi-gpu-gradient-averaging-as-noise-reduction">Multi-GPU Gradient Averaging as Noise Reduction</h4>
<p>FSDP performs gradient averaging across GPUs:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># What happens during multi-GPU backward pass</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Each GPU computes gradients independently</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>grad_gpu0 <span class="op">=</span> compute_gradients(model, batch_gpu0)  <span class="co"># Noisy</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>grad_gpu1 <span class="op">=</span> compute_gradients(model, batch_gpu1)  <span class="co"># Noisy</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>grad_gpu2 <span class="op">=</span> compute_gradients(model, batch_gpu2)  <span class="co"># Noisy</span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>grad_gpu3 <span class="op">=</span> compute_gradients(model, batch_gpu3)  <span class="co"># Noisy</span></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: All-reduce averages gradients (in FP32 accumulator)</span></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>averaged_grad <span class="op">=</span> (grad_gpu0 <span class="op">+</span> grad_gpu1 <span class="op">+</span> grad_gpu2 <span class="op">+</span> grad_gpu3) <span class="op">/</span> <span class="dv">4</span></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Each GPU receives averaged gradient</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Why averaging helps:</strong></p>
<p>Statistical principle: Variance of mean = Variance / N</p>
<ul>
<li>1 GPU: Variance = σ²</li>
<li>2 GPUs: Variance = σ² / 2 (variance reduced by √2)</li>
<li>4 GPUs: Variance = σ² / 4 (variance reduced by 2x)</li>
</ul>
<p><strong>Effect on FP8:</strong></p>
<ul>
<li>Lower gradient noise → FP8’s precision becomes sufficient</li>
<li>Outlier values averaged out</li>
<li>Signal-to-noise ratio improves</li>
</ul>
</section>
<section id="mathematical-framework-the-precision-noise-trade-off" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-framework-the-precision-noise-trade-off">Mathematical Framework: The Precision-Noise Trade-off</h4>
<p>We can formalize this as:</p>
<pre><code>Total Optimization Error = Gradient Sampling Noise + Numerical Precision Error

Single GPU (batch_size=1):
  Sampling Noise: HIGH (σ²)
  Precision Error: MEDIUM (FP8 quantization)
  Total Error: HIGH + MEDIUM = TOO HIGH for learning ❌

2 GPUs (batch_size=1 each):
  Sampling Noise: MEDIUM (σ²/2)
  Precision Error: MEDIUM (FP8 quantization)
  Total Error: MEDIUM + MEDIUM = ACCEPTABLE ✅

4 GPUs (batch_size=1 each):
  Sampling Noise: LOW (σ²/4)
  Precision Error: MEDIUM (FP8 quantization)
  Total Error: LOW + MEDIUM = GOOD ✅</code></pre>
<p><strong>Phase transition:</strong> At some point (between 1 and 2 GPUs), total error drops below the threshold needed for effective learning.</p>
</section>
<section id="why-bf16-is-more-robust-on-single-gpu" class="level4">
<h4 class="anchored" data-anchor-id="why-bf16-is-more-robust-on-single-gpu">Why BF16 is More Robust on Single GPU</h4>
<p><strong>BF16 advantages:</strong></p>
<ul>
<li><strong>8-bit exponent</strong> (same range as FP32)</li>
<li><strong>7-bit mantissa</strong> (4-8x more precision than FP8’s 2-3 bits)</li>
<li>Can represent wide dynamic range simultaneously</li>
</ul>
<p><strong>Numerical example:</strong></p>
<pre><code>Gradient component:     0.000123456
BF16 representation:    0.000123
FP8 representation:     0.000120  (or 0.0000 if underflow)

BF16 error: 0.456e-6    (tiny)
FP8 error:  3.456e-6    (significant) or total loss</code></pre>
<p><strong>BF16’s extra precision:</strong></p>
<ul>
<li>Preserves small but important gradient components</li>
<li>Handles outlier values better</li>
<li>Less sensitive to scaling issues</li>
<li>Sufficient precision even with high noise</li>
</ul>
</section>
<section id="empirical-validation" class="level4">
<h4 class="anchored" data-anchor-id="empirical-validation">Empirical Validation</h4>
<p>Our results empirically validate this theory:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 23%">
<col style="width: 28%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Configuration</th>
<th>Effective Batch</th>
<th>Gradient Variance</th>
<th>FP8 Performance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1 GPU</td>
<td>1</td>
<td>Very High (σ²)</td>
<td>❌ Fails (loss ~11)</td>
</tr>
<tr class="even">
<td>2 GPUs</td>
<td>2</td>
<td>Medium (σ²/2)</td>
<td>✅ Works (loss ~3)</td>
</tr>
<tr class="odd">
<td>4 GPUs</td>
<td>4</td>
<td>Low (σ²/4)</td>
<td>✅ Works (loss ~3)</td>
</tr>
</tbody>
</table>
<p><strong>Phase transition observed:</strong></p>
<ul>
<li>1 GPU: Total error too high for FP8</li>
<li>2 GPUs: Total error acceptable for FP8</li>
<li>The transition happens between 1 and 2 GPUs</li>
</ul>
</section>
<section id="practical-recommendations" class="level4">
<h4 class="anchored" data-anchor-id="practical-recommendations">Practical Recommendations</h4>
<p><strong>For FP8 Training:</strong></p>
<p>✅ <strong>Use FP8 when:</strong></p>
<ul>
<li>Multi-GPU training (2+ GPUs with FSDP/DDP)</li>
<li>Batch size ≥ 4 per GPU</li>
<li>Gradient accumulation over multiple micro-batches</li>
<li>Training at scale (communication bandwidth matters)</li>
</ul>
<p>❌ <strong>Avoid FP8 when:</strong></p>
<ul>
<li>Single GPU with batch_size ≤ 2</li>
<li>Tasks requiring maximum numerical precision</li>
<li>Early research with minimal infrastructure</li>
</ul>
<p><strong>Minimum recommended configurations:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Option 1: Multi-GPU (minimum 2 GPUs)</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>batch_size_per_gpu <span class="op">=</span> <span class="dv">1</span>  <span class="co"># Acceptable with 2+ GPUs</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>num_gpus <span class="op">=</span> <span class="dv">2</span>  <span class="co"># Minimum for FP8</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>effective_batch_size <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Option 2: Single GPU with larger batch</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>batch_size_per_gpu <span class="op">=</span> <span class="dv">4</span>  <span class="co"># Minimum for single GPU FP8</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>num_gpus <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>effective_batch_size <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Option 3: Gradient accumulation</span></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>batch_size_per_gpu <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>accumulation_steps <span class="op">=</span> <span class="dv">4</span>  <span class="co"># Simulate effective_batch_size=4</span></span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>num_gpus <span class="op">=</span> <span class="dv">1</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>For Production Training:</strong></p>
<p>Typical settings:</p>
<ul>
<li>8-64 GPUs</li>
<li>batch_size = 1-4 per GPU</li>
<li>Effective batch size = 8-256</li>
<li>FP8 works excellently in this regime</li>
</ul>
<hr>
</section>
</section>
<section id="summary-of-experimental-findings" class="level3">
<h3 class="anchored" data-anchor-id="summary-of-experimental-findings">Summary of Experimental Findings</h3>
<section id="performance-metrics-summary" class="level4">
<h4 class="anchored" data-anchor-id="performance-metrics-summary">Performance Metrics Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 20%">
<col style="width: 37%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th>Metric</th>
<th>FP8 vs BF16</th>
<th>Optimal Sequence Length</th>
<th>Multi-GPU Scaling</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>TFLOPs</strong></td>
<td>FP8 +10-15%</td>
<td>4096</td>
<td>Good (3.5-3.8x on 4 GPUs)</td>
</tr>
<tr class="even">
<td><strong>Tokens/s</strong></td>
<td>Comparable</td>
<td>2048 (highest)</td>
<td>Sublinear (batch_size=1)</td>
</tr>
<tr class="odd">
<td><strong>MFU</strong></td>
<td>Comparable (2-9%)</td>
<td>4096</td>
<td>Marginal improvement</td>
</tr>
</tbody>
</table>
</section>
<section id="training-quality-summary" class="level4">
<h4 class="anchored" data-anchor-id="training-quality-summary">Training Quality Summary</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>GPU Count</th>
<th>FP8 vs BF16</th>
<th>Gradient Variance</th>
<th>Recommendation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>1 GPU</strong></td>
<td>BF16 ≫ FP8</td>
<td>Very High</td>
<td>Never use FP8</td>
</tr>
<tr class="even">
<td><strong>2 GPUs</strong></td>
<td>FP8 ≈ BF16</td>
<td>Medium</td>
<td>Minimum for FP8</td>
</tr>
<tr class="odd">
<td><strong>4 GPUs</strong></td>
<td>FP8 = BF16</td>
<td>Low</td>
<td>Ideal for FP8</td>
</tr>
</tbody>
</table>
</section>
<section id="key-insights" class="level4">
<h4 class="anchored" data-anchor-id="key-insights">Key Insights</h4>
<ol type="1">
<li><strong>FP8 is production-ready for multi-GPU training</strong> (2+ GPUs)</li>
<li><strong>Batch size is critical for FP8 stability</strong>, not just throughput</li>
<li><strong>Sequence length 4096 offers best TFLOPs/MFU balance</strong></li>
<li><strong>Low MFU (2-9%) is expected</strong> with batch_size=1</li>
<li><strong>Gradient averaging compensates for FP8 precision</strong> in distributed training</li>
</ol>
<hr>
</section>
</section>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>Our comprehensive benchmark of FP8 training on NVIDIA B200 GPUs reveals several critical insights that advance both the practical deployment and theoretical understanding of low-precision training for large language models. FP8 delivers measurable performance gains across all tested configurations, achieving 10-15% higher computational throughput (TFLOPs) compared to BF16, along with a 2x reduction in communication bandwidth when using <code>enable_fsdp_float8_all_gather=True</code> and 50% memory savings for parameters and activations. However, our most important finding centers on the interaction between numerical precision and gradient estimation quality: FP8 training quality is not solely determined by bit precision, but rather by the interplay between precision limitations and gradient noise. On multi-GPU configurations (2 or more GPUs), FP8 achieves training quality equivalent to BF16, with loss curves that track nearly perfectly throughout training, while single-GPU training with small batch sizes shows BF16 significantly outperforming FP8, with FP8 models plateauing at loss values 4-6 units higher. This phenomenon stems from gradient averaging in distributed training acting as essential noise reduction that compensates for FP8’s precision limitations, explaining why FP8 has become practical primarily in the era of large-scale distributed training. For practitioners, these results translate to clear deployment guidelines: FP8 should be used for multi-GPU training with FSDP2 or DDP (minimum 2 GPUs), production-scale training (8+ GPUs), memory-constrained scenarios, and communication-bound workloads, while BF16 remains preferable for single-GPU training with small batches, early research and prototyping, and tasks requiring maximum precision. Key configuration recommendations include maintaining a minimum effective batch size of 4, using sequence lengths of 2048-4096 tokens for optimal efficiency, skipping first and last layer FP8 conversion for stability, and enabling <code>enable_fsdp_float8_all_gather=True</code> for communication bandwidth savings.</p>
<p>The role of hardware interconnect emerged as a crucial consideration. Our excellent multi-GPU scaling results (88-95% efficiency on 4 GPUs) were achieved with NVLink connectivity providing 900 GB/s bandwidth per GPU. Systems using standard PCIe interconnect (128 GB/s per GPU) should expect 20-40% lower multi-GPU throughput and degraded scaling efficiency of 50-70%. On such systems, FP8’s communication bandwidth advantages become even more critical, potentially shifting the cost-benefit analysis in favor of low-precision training despite compute-bound workloads.</p>
<p>Several limitations of this study must be acknowledged. Our intentional use of batch size 1, while valuable for isolating sequence length effects and revealing precision sensitivity, does not represent production training practices where batch sizes of 4-8 per GPU are standard. The short training runs (50-1000 steps) and use of random initialization, though sufficient for performance benchmarking and convergence trend analysis, cannot speak to final model quality or long-term training stability over billions of steps. The TinyStories dataset, while convenient for benchmarking, may not expose all numerical stability issues present in diverse production datasets. Finally, our focus on models up to 14B parameters leaves open questions about how FP8 behaves at the 70B-405B parameter scales common in production systems.</p>
<p>The dramatic difference between single-GPU and multi-GPU FP8 performance reveals a deep connection between numerical precision and gradient estimation quality in stochastic optimization. This finding has implications beyond FP8, informing our understanding of how low-precision arithmetic interacts with the fundamental dynamics of deep learning. The precision-noise trade-off we documented provides empirical evidence for theoretical frameworks in stochastic optimization, demonstrating that required numerical precision scales with gradient noise levels. As large language models continue to grow and training costs escalate into millions of dollars, techniques like FP8 training will become increasingly important for making cutting-edge AI research accessible to a broader community.</p>
<hr>
</section>
<section id="future-research-directions" class="level2">
<h2 class="anchored" data-anchor-id="future-research-directions">Future Research Directions</h2>
<p>The findings from this benchmark open several promising avenues for future investigation, each addressing limitations of the current work while building on the insights gained about FP8 training dynamics. The question of optimal batch size for single-GPU FP8 training remains open, with our results showing a clear phase transition between ineffective training at batch size 1 and effective training at larger batch sizes, warranting systematic exploration to identify the precise threshold where FP8 becomes viable in resource-constrained environments. FP8 for fine-tuning represents largely unexplored territory, as pretrained weights exhibit specific learned distributions that may interact differently with FP8 quantization compared to random initialization, with critical questions including whether FP8 preserves pretrained knowledge, how it interacts with parameter-efficient methods like LoRA and QLoRA, and whether certain layers show heightened sensitivity during fine-tuning. Scaling to very large models of 70B-405B parameters represents the next frontier, where testing across multi-node training setups would reveal how FP8 interacts with other essential optimizations like Flash Attention and gradient checkpointing, with the hypothesis that FP8 advantages may become more pronounced at larger scales where communication bandwidth and memory capacity become primary bottlenecks. The choice of optimizer may significantly impact FP8 training dynamics, as alternatives to AdamW such as Lion, Adafactor, and Sophia exhibit different numerical characteristics that could interact differently with reduced precision, raising questions about whether simpler optimizers work better with FP8 and whether optimizer states themselves can be quantized. Mixed precision strategies offering finer granularity than all-or-nothing FP8 deserve investigation, with approaches like selectively maintaining critical layers in BF16 while using FP8 for large feedforward networks, or dynamically adjusting precision during training, potentially delivering better quality than full FP8 while achieving more memory savings than full BF16. Hardware comparisons across AMD’s MI300X, Google’s TPU v5, and Intel’s Gaudi2 would provide valuable context for generalizability, revealing whether our findings are NVIDIA-specific or represent universal properties of FP8 training while informing hardware selection decisions. Production deployment case studies spanning weeks or months on production datasets would validate whether FP8’s advantages persist over billions of training steps, with comprehensive cost-benefit analysis measuring training time, monetary cost, energy consumption, and downstream task performance providing the economic data necessary for informed precision choices. The convergence of these investigations would provide comprehensive understanding of FP8 training across the full spectrum of practical applications, from resource-constrained single-GPU research to massive-scale production training, proving essential as the field continues pushing toward larger models and more efficient training methods while managing computational costs and environmental impact.</p>
<hr>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<section id="documentation-and-guides" class="level3">
<h3 class="anchored" data-anchor-id="documentation-and-guides">Documentation and Guides</h3>
<ol type="1">
<li><p><a href="https://huggingface.co/docs/accelerate/v1.10.0/en/concept_guides/low_precision_training">HuggingFace Accelerate - Low Precision Training</a></p></li>
<li><p><a href="https://pytorch.org/blog/training-using-float8-fsdp2/">PyTorch Blog - Training using float8 and FSDP2</a></p></li>
</ol>
</section>
<section id="code-repositories" class="level3">
<h3 class="anchored" data-anchor-id="code-repositories">Code Repositories</h3>
<ol start="3" type="1">
<li><p><a href="https://github.com/pytorch/ao/tree/main/torchao/float8">TorchAO Float8 Module</a></p></li>
<li><p><a href="https://github.com/huggingface/accelerate/blob/main/benchmarks/fp8/torchao/ddp.py">HuggingFace Accelerate - DDP FP8 Benchmark</a></p></li>
<li><p><a href="https://github.com/huggingface/accelerate/blob/main/examples/torch_native_parallelism/fsdp2_fp8.py">HuggingFace Accelerate - FSDP2 FP8 Example</a></p></li>
</ol>
</section>
<section id="hardware-specifications" class="level3">
<h3 class="anchored" data-anchor-id="hardware-specifications">Hardware Specifications</h3>
<ol start="6" type="1">
<li><strong>NVIDIA B200 Tensor Core GPU</strong> NVIDIA Data Center GPU specifications Peak performance: 9000 TFLOPs (FP8), 4500 TFLOPs (BF16)</li>
</ol>
</section>
<section id="related-research" class="level3">
<h3 class="anchored" data-anchor-id="related-research">Related Research</h3>
<ol start="7" type="1">
<li><p><strong>Mixed Precision Training (Micikevicius et al., 2018)</strong></p></li>
<li><p><strong>FP8 Formats for Deep Learning (Micikevicius et al., 2022)</strong></p></li>
<li><p><strong>PyTorch FSDP (Zhao et al., 2023)</strong></p></li>
</ol>
</section>
<section id="our-benchmark-code" class="level3">
<h3 class="anchored" data-anchor-id="our-benchmark-code">Our Benchmark Code</h3>
<p>10.<a href="https://github.com/Scratch-to-Scale/low-precision-training-daddyofadoggy/tree/main">Low-Precision Training Benchmark Repository</a>. Complete benchmark code, results, and analysis</p>
</section>
<section id="additional-resources" class="level3">
<h3 class="anchored" data-anchor-id="additional-resources">Additional Resources</h3>
<p>11.<a href="https://blog.eleuther.ai/transformer-math/">Transformer Math 101</a></p>
<p>12.<a href="https://lambdalabs.com/service/gpu-cloud">Lambda Cloud GPU Instances</a></p>
<hr>
</section>
</section>
<section id="appendix-reproducing-this-benchmark" class="level2">
<h2 class="anchored" data-anchor-id="appendix-reproducing-this-benchmark">Appendix: Reproducing This Benchmark</h2>
<section id="environment-setup" class="level3">
<h3 class="anchored" data-anchor-id="environment-setup">Environment Setup</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb42"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create virtual environment</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> <span class="at">-m</span> venv venv</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> venv/bin/activate</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Install dependencies</span></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install torch torchvision torchaudio <span class="at">--index-url</span> https://download.pytorch.org/whl/cu121</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install transformers<span class="op">&gt;</span>=4.30.0</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install accelerate<span class="op">&gt;</span>=0.20.0</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install torchao<span class="op">&gt;</span>=0.1.0</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install datasets<span class="op">&gt;</span>=2.12.0</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install pandas matplotlib</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="running-benchmarks" class="level3">
<h3 class="anchored" data-anchor-id="running-benchmarks">Running Benchmarks</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb43"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Single model, single configuration</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="ex">accelerate</span> launch <span class="at">--num_processes</span><span class="op">=</span>4 fp8_benchmark.py <span class="dt">\</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    meta-llama/Llama-3.2-1B <span class="dt">\</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">--sequence-length</span> 8192 <span class="dt">\</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">--precision</span> fp8 <span class="dt">\</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">--num-steps</span> 1000</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Run full sweep</span></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="fu">bash</span> run_sweep.sh</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="hardware-requirements" class="level3">
<h3 class="anchored" data-anchor-id="hardware-requirements">Hardware Requirements</h3>
<ul>
<li><strong>Minimum</strong>: 1x NVIDIA H100 or B200 (80GB+)</li>
<li><strong>Recommended</strong>: 4x NVIDIA B200 (180GB) for full benchmark</li>
<li><strong>Storage</strong>: 50GB for models and datasets</li>
<li><strong>RAM</strong>: 128GB+ system RAM</li>
</ul>
<hr>
</section>
</section>
<section id="acknowledgments" class="level2">
<h2 class="anchored" data-anchor-id="acknowledgments">Acknowledgments</h2>
<p>We thank:</p>
<ul>
<li><strong>HuggingFace</strong> for the excellent Accelerate library and examples</li>
<li><strong>PyTorch</strong> team for torchao and FSDP2 implementation</li>
<li><strong>NVIDIA</strong> for B200 GPU architecture and FP8 support</li>
<li><strong>Lambda Labs</strong> for providing GPU cloud infrastructure</li>
<li><strong>Open-source community</strong> for models, datasets, and tools</li>
</ul>
<hr>
<p><strong>Repository Used in the Experiment</strong>: <a href="https://github.com/Scratch-to-Scale/low-precision-training-daddyofadoggy/tree/main">github</a></p>
<p><strong>Last Updated</strong>: December 2025</p>
<hr>
<p><em>This blog post is based on research conducted in December 26-28, 2025 using NVIDIA B200 GPUs. Results may vary on different hardware configurations and your moods</em></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/your-website-url\.example\.com");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>