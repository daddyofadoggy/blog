<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>report – My Blogs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">My Blogs</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block"></header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="distributed-data-parallelism-ddp-explained-for-beginners" class="level1">
<h1>Distributed Data Parallelism (DDP) Explained for Beginners</h1>
<section id="what-is-distributed-data-parallelism" class="level2">
<h2 class="anchored" data-anchor-id="what-is-distributed-data-parallelism">What is Distributed Data Parallelism?</h2>
<p>Distributed Data Parallelism (DDP) is a technique for training large machine learning models faster by using multiple GPUs or computers at the same time. Instead of training on one GPU, you train on multiple GPUs simultaneously, with each GPU working on different parts of your data.</p>
<p>Think of it like this: if you have 1000 images to process and 4 GPUs, each GPU processes 250 images. This makes training roughly 4x faster!</p>
<hr>
</section>
<section id="code-walkthrough" class="level2">
<h2 class="anchored" data-anchor-id="code-walkthrough">Code Walkthrough</h2>
<section id="imports-and-setup-lines-1-14" class="level3">
<h3 class="anchored" data-anchor-id="imports-and-setup-lines-1-14">1. Imports and Setup (Lines 1-14)</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.distributed <span class="im">as</span> dist</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> accelerate <span class="im">import</span> PartialState</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What’s happening:</strong> - <code>torch</code>: The main PyTorch library for deep learning - <code>torch.distributed (dist)</code>: PyTorch’s library for distributed training across multiple devices - <code>PartialState</code>: A helper from the Accelerate library that manages which GPU each process should use</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>state <span class="op">=</span> PartialState()</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> state.device</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>set_seed(<span class="dv">42</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What’s happening:</strong> - <code>PartialState()</code> automatically figures out which GPU this process should use - <code>device</code> stores the assigned GPU - <code>set_seed(42)</code> ensures reproducibility - all processes start with the same random state</p>
<hr>
</section>
<section id="the-simpledistributeddataparallelism-class-lines-16-42" class="level3">
<h3 class="anchored" data-anchor-id="the-simpledistributeddataparallelism-class-lines-16-42">2. The SimpleDistributedDataParallelism Class (Lines 16-42)</h3>
<p>This is the heart of the code! It shows how DDP works under the hood.</p>
<section id="initialization-init-lines-17-27" class="level4">
<h4 class="anchored" data-anchor-id="initialization-init-lines-17-27">Initialization (<strong>init</strong>, Lines 17-27)</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model:torch.nn.Module):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param <span class="kw">in</span> model.parameters():</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        rank0_param <span class="op">=</span> param.data.clone()</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        dist.broadcast(rank0_param, src<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> torch.equal(param.data, rank0_param):</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(...)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What’s happening:</strong></p>
<ol type="1">
<li><strong>Takes a model as input</strong> and stores it</li>
<li><strong>Broadcasts parameters from GPU 0 to all other GPUs</strong>:
<ul>
<li>In distributed training, each GPU (called a “rank”) has its own copy of the model</li>
<li>“Broadcasting” means copying data from one GPU to all others</li>
<li>This ensures all GPUs start with identical model weights</li>
</ul></li>
<li><strong>Verification check</strong>: If any GPU has different parameters, raise an error</li>
</ol>
<p><strong>Why this matters:</strong> All GPUs must start with the exact same model, or they’ll learn different things!</p>
</section>
<section id="gradient-synchronization-lines-29-33" class="level4">
<h4 class="anchored" data-anchor-id="gradient-synchronization-lines-29-33">Gradient Synchronization (Lines 29-33)</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sync_gradients(<span class="va">self</span>):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param <span class="kw">in</span> <span class="va">self</span>.model.parameters():</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> param.grad <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>            dist.all_reduce(param.grad, op<span class="op">=</span>dist.ReduceOp.SUM)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>            param.grad <span class="op">/=</span> dist.get_world_size()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What’s happening:</strong></p>
<p>This is THE KEY operation in DDP! Let me explain with an example:</p>
<p>Imagine you have 2 GPUs: - GPU 0 processes batch A and calculates gradient = [1, 2, 3] - GPU 1 processes batch B and calculates gradient = [4, 5, 6]</p>
<p>After <code>all_reduce</code> with SUM operation: - Both GPUs now have gradient = [5, 7, 9] (sum of both)</p>
<p>After dividing by world_size (2): - Both GPUs have gradient = [2.5, 3.5, 4.5] (average)</p>
<p><strong>Why averaging?</strong> This is equivalent to processing both batches on a single GPU! The gradient is the average across all data processed by all GPUs.</p>
</section>
<section id="helper-methods-lines-35-42" class="level4">
<h4 class="anchored" data-anchor-id="helper-methods-lines-35-42">Helper Methods (Lines 35-42)</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span>.model(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(<span class="va">self</span>):</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.model.train()</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> <span class="bu">eval</span>(<span class="va">self</span>):</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.model.<span class="bu">eval</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What’s happening:</strong> These methods allow our wrapper to behave like a regular PyTorch model.</p>
<hr>
</section>
</section>
<section id="data-preparation-lines-44-68" class="level3">
<h3 class="anchored" data-anchor-id="data-preparation-lines-44-68">3. Data Preparation (Lines 44-68)</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> get_dataset()[<span class="st">"train"</span>]</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>train_ds <span class="op">=</span> dataset.shuffle(seed<span class="op">=</span><span class="dv">42</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What’s happening:</strong> - Load the training dataset - Shuffle it with a fixed seed so all GPUs shuffle the same way</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> collate_func(batch):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenizer.pad(</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>        batch,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">"longest"</span>,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        pad_to_multiple_of<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        return_tensors<span class="op">=</span><span class="st">"pt"</span>,</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What’s happening:</strong> - This function prepares batches of text data - It pads sequences to the same length (needed for batch processing) - Pads to multiples of 8 (optimization for GPU efficiency)</p>
<hr>
</section>
<section id="data-sharding---the-critical-part-lines-87-98" class="level3">
<h3 class="anchored" data-anchor-id="data-sharding---the-critical-part-lines-87-98">4. Data Sharding - The Critical Part! (Lines 87-98)</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Shard data for first parallel dimension</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>ds_length <span class="op">=</span> <span class="bu">len</span>(train_ds)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>ds_length_per_rank <span class="op">=</span> ds_length <span class="op">//</span> get(<span class="st">"ws"</span>)  <span class="co"># ws = world_size</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>rank <span class="op">=</span> get(<span class="st">"rank"</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> rank <span class="op">*</span> ds_length_per_rank</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>end <span class="op">=</span> start <span class="op">+</span> ds_length_per_rank <span class="cf">if</span> rank <span class="op">!=</span> get(<span class="st">"ws"</span>) <span class="op">-</span> <span class="dv">1</span> <span class="cf">else</span> ds_length</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>train_shard <span class="op">=</span> train_ds.select(<span class="bu">list</span>(<span class="bu">range</span>(start, end)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What’s happening:</strong></p>
<p>This splits the dataset into separate chunks for each GPU!</p>
<p><strong>Example with 1000 samples and 4 GPUs:</strong> - Total samples: 1000 - Samples per GPU: 1000 / 4 = 250 - GPU 0 (rank 0): samples 0-249 - GPU 1 (rank 1): samples 250-499 - GPU 2 (rank 2): samples 500-749 - GPU 3 (rank 3): samples 750-999</p>
<p><strong>Why this matters:</strong> Each GPU only loads and processes its own portion of data, so they work on different examples simultaneously!</p>
<hr>
</section>
<section id="model-setup-lines-109-112" class="level3">
<h3 class="anchored" data-anchor-id="model-setup-lines-109-112">5. Model Setup (Lines 109-112)</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_smol_model()</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>model.to(device)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.model.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SimpleDistributedDataParallelism(model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What’s happening:</strong> 1. Create the model 2. Move it to the assigned GPU 3. Create an optimizer (SGD with learning rate 0.001) 4. Wrap the model with our DDP wrapper</p>
<hr>
</section>
<section id="profiler-setup-lines-114-136" class="level3">
<h3 class="anchored" data-anchor-id="profiler-setup-lines-114-136">6. Profiler Setup (Lines 114-136)</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> state.is_main_process:</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    profiler_context <span class="op">=</span> profile(...)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What’s happening:</strong> - Only the main process (GPU 0) runs the profiler - The profiler tracks performance metrics like memory usage and computation time - Results are saved to “ddp_trace” folder for analysis with TensorBoard</p>
<p><strong>Why only main process?</strong> To avoid multiple GPUs writing the same profiling data and causing conflicts.</p>
<section id="understanding-the-profiler-schedule" class="level4">
<h4 class="anchored" data-anchor-id="understanding-the-profiler-schedule">Understanding the Profiler Schedule</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>profiler_schedule <span class="op">=</span> schedule(</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    skip_first<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    wait<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    warmup<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    active<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    repeat<span class="op">=</span><span class="dv">1</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What’s happening:</strong></p>
<p>The profiler schedule controls WHEN the profiler collects data. It doesn’t run on every iteration because profiling adds overhead and generates large trace files. The schedule has four phases that cycle through iterations:</p>
<ol type="1">
<li><strong>skip_first=5</strong>: Skip the first 5 iterations completely (no profiling)
<ul>
<li>Why? The first few iterations are often slower due to initialization and GPU warm-up</li>
<li>Skipping them gives more accurate performance measurements</li>
</ul></li>
<li><strong>wait=1</strong>: Wait for 1 iteration without profiling
<ul>
<li>This is a “rest” phase between profiling cycles</li>
<li>Allows the system to stabilize before starting to profile again</li>
</ul></li>
<li><strong>warmup=2</strong>: Run for 2 iterations collecting basic profiling data
<ul>
<li>This is a “warm-up” phase where the profiler starts but doesn’t record everything yet</li>
<li>Helps the profiler itself initialize properly</li>
</ul></li>
<li><strong>active=5</strong>: Actively profile for 5 iterations with full data collection
<ul>
<li>This is when the profiler records detailed performance data</li>
<li>Captures CPU usage, GPU usage, memory allocations, and operation timing</li>
</ul></li>
<li><strong>repeat=1</strong>: Repeat the cycle (wait → warmup → active) 1 time
<ul>
<li>After the first cycle completes, it runs one more cycle</li>
<li>Total cycles = initial + repeat = 2 cycles</li>
</ul></li>
</ol>
<p><strong>Timeline example for 20 iterations:</strong></p>
<pre><code>Iterations 0-4:   SKIP (skip_first=5)
Iteration 5:      WAIT (wait=1)
Iterations 6-7:   WARMUP (warmup=2)
Iterations 8-12:  ACTIVE - recording data! (active=5)
Iteration 13:     WAIT (wait=1)
Iterations 14-15: WARMUP (warmup=2)
Iterations 16-20: ACTIVE - recording data! (active=5)</code></pre>
</section>
<section id="understanding-the-profile-configuration" class="level4">
<h4 class="anchored" data-anchor-id="understanding-the-profile-configuration">Understanding the Profile Configuration</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>profiler_context <span class="op">=</span> profile(</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    activities<span class="op">=</span>[ProfilerActivity.CPU, ProfilerActivity.CUDA],</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    schedule<span class="op">=</span>profiler_schedule,</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    on_trace_ready<span class="op">=</span>torch.profiler.tensorboard_trace_handler(<span class="st">"ddp_trace"</span>),</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    record_shapes<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    profile_memory<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    with_stack<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    with_flops<span class="op">=</span><span class="va">True</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What each parameter means:</strong></p>
<ul>
<li><strong>activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]</strong>
<ul>
<li>Track both CPU and GPU (CUDA) operations</li>
<li>Shows where time is spent on both devices</li>
</ul></li>
<li><strong>schedule=profiler_schedule</strong>
<ul>
<li>Use the schedule defined above to control when profiling happens</li>
</ul></li>
<li><strong>on_trace_ready=torch.profiler.tensorboard_trace_handler(“ddp_trace”)</strong>
<ul>
<li>When profiling data is ready, save it to the “ddp_trace” folder</li>
<li>Can be visualized with TensorBoard using: <code>tensorboard --logdir=ddp_trace</code></li>
</ul></li>
<li><strong>record_shapes=True</strong>
<ul>
<li>Record the shapes of tensors (e.g., [batch_size, sequence_length, hidden_size])</li>
<li>Helps identify operations working on large tensors that might be slow</li>
</ul></li>
<li><strong>profile_memory=True</strong>
<ul>
<li>Track memory allocations and deallocations</li>
<li>Shows which operations use the most GPU memory</li>
<li>Helps identify memory bottlenecks or leaks</li>
</ul></li>
<li><strong>with_stack=True</strong>
<ul>
<li>Record the Python call stack for each operation</li>
<li>Shows which line of code triggered each operation</li>
<li>Makes it easier to find performance bottlenecks in your code</li>
</ul></li>
<li><strong>with_flops=True</strong>
<ul>
<li>Estimate floating-point operations (FLOPs) for each operation</li>
<li>Helps understand computational intensity</li>
<li>Higher FLOPs = more computation work</li>
</ul></li>
</ul>
<p><strong>Why this matters:</strong> Profiling helps you understand where your training time is spent. You can identify if you’re bottlenecked by data loading, forward pass, backward pass, or gradient synchronization!</p>
</section>
<section id="visualizing-profiler-data-with-tensorboard-remote-setup" class="level4">
<h4 class="anchored" data-anchor-id="visualizing-profiler-data-with-tensorboard-remote-setup">Visualizing Profiler Data with TensorBoard (Remote Setup)</h4>
<p>Since your code is running on <strong>Lambda Labs</strong> (remote GPU server) and you’re accessing it from your <strong>MacBook via VSCode</strong>, here’s how to visualize the profiler traces:</p>
<p><strong>Step 1: Run the DDP Training Script on Lambda Labs</strong></p>
<p>First, execute your training script on the Lambda Labs instance. This will generate the profiler trace files:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># On Lambda Labs (via VSCode terminal)</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> ddp.py</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>After the script completes, you should see a <code>ddp_trace</code> folder created with trace files inside.</p>
<p><strong>Step 2: Install TensorBoard (if not already installed)</strong></p>
<p>On your Lambda Labs instance:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install tensorboard</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Step 3: Launch TensorBoard on Lambda Labs</strong></p>
<p>Start TensorBoard on the remote server:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="ex">tensorboard</span> <span class="at">--logdir</span><span class="op">=</span>ddp_trace <span class="at">--port</span><span class="op">=</span>6006</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This will output something like:</p>
<pre><code>TensorBoard 2.x.x at http://localhost:6006/</code></pre>
<p><strong>Important:</strong> Keep this terminal running! Don’t close it.</p>
<p><strong>Step 4: Port Forwarding via VSCode (Easy Method)</strong></p>
<p>VSCode makes port forwarding super easy!</p>
<p><strong>Option A: Automatic Port Forwarding (Recommended)</strong></p>
<ol type="1">
<li>VSCode should automatically detect that port 6006 is being used</li>
<li>Look for a notification in the bottom-right corner saying “Port 6006 is available”</li>
<li>Click “Open in Browser” or “Forward Port”</li>
</ol>
<p><strong>Option B: Manual Port Forwarding</strong></p>
<ol type="1">
<li>In VSCode, press <code>Cmd+Shift+P</code> (on Mac) to open the Command Palette</li>
<li>Type “Forward a Port” and select it</li>
<li>Enter port number: <code>6006</code></li>
<li>Press Enter</li>
</ol>
<p>You should see the forwarded port appear in the “PORTS” panel at the bottom of VSCode.</p>
<p><strong>Step 5: Open TensorBoard in Your MacBook Browser</strong></p>
<p>Once the port is forwarded, open your web browser on your MacBook and go to:</p>
<pre><code>http://localhost:6006</code></pre>
<p>You should see the TensorBoard interface!</p>
<p><strong>Step 6: Navigate to the Profiler Tab</strong></p>
<p>In TensorBoard: 1. Click on the <strong>“PYTORCH_PROFILER”</strong> or <strong>“PROFILE”</strong> tab at the top 2. You’ll see a dropdown to select which trace file to view 3. Select the trace file you want to analyze</p>
<p><strong>What You’ll See in TensorBoard:</strong></p>
<p>The profiler visualization shows several views:</p>
<ol type="1">
<li><strong>Overview Page:</strong>
<ul>
<li>Performance summary</li>
<li>GPU utilization over time</li>
<li>Step time breakdown (how long each training iteration took)</li>
</ul></li>
<li><strong>Operator View:</strong>
<ul>
<li>Shows which PyTorch operations took the most time</li>
<li>See operations like <code>matmul</code>, <code>conv2d</code>, <code>all_reduce</code>, etc.</li>
<li>Sorted by execution time</li>
</ul></li>
<li><strong>Kernel View:</strong>
<ul>
<li>Low-level GPU kernel performance</li>
<li>Shows actual CUDA kernels that ran on the GPU</li>
</ul></li>
<li><strong>Trace View:</strong>
<ul>
<li>Timeline visualization</li>
<li>Shows when each operation executed</li>
<li>You can zoom in to see individual operations</li>
<li><strong>Look for the <code>sync_grads</code> section</strong> - this shows the time spent on gradient synchronization!</li>
</ul></li>
<li><strong>Memory View:</strong>
<ul>
<li>Memory allocation over time</li>
<li>Helps identify memory leaks or spikes</li>
</ul></li>
</ol>
<p><strong>Tips for Analysis:</strong></p>
<ul>
<li><strong>Look for the “sync_grads” operations</strong> in the trace view - this is your DDP gradient synchronization time</li>
<li><strong>Compare “forward”, “backward”, and “sync_grads” times</strong> - ideally, sync time should be small compared to computation</li>
<li><strong>Check GPU utilization</strong> - you want this close to 100% during training</li>
<li><strong>Identify bottlenecks</strong> - if data loading takes longer than forward/backward, you need faster data loading</li>
</ul>
<p><strong>Alternative: Using SSH Tunnel (Manual Method)</strong></p>
<p>If VSCode port forwarding doesn’t work, you can use SSH tunneling:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># On your MacBook terminal (not VSCode)</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ssh</span> <span class="at">-L</span> 6006:localhost:6006 username@lambda-labs-ip-address</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Then access <code>http://localhost:6006</code> in your browser.</p>
<p><strong>Troubleshooting:</strong></p>
<ul>
<li><strong>Port already in use?</strong> Change the port: <code>tensorboard --logdir=ddp_trace --port=6007</code></li>
<li><strong>Can’t see traces?</strong> Make sure the <code>ddp_trace</code> folder exists and contains <code>.pt.trace.json</code> files</li>
<li><strong>Port forwarding not working?</strong> Try restarting VSCode or manually set up SSH tunnel</li>
<li><strong>No data in TensorBoard?</strong> The profiler only collects data during “active” iterations (8-12 and 16-20 in this code)</li>
</ul>
<hr>
</section>
</section>
<section id="the-training-loop-lines-138-161" class="level3">
<h3 class="anchored" data-anchor-id="the-training-loop-lines-138-161">7. The Training Loop (Lines 138-161)</h3>
<p>This is where everything comes together!</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i, batch) <span class="kw">in</span> <span class="bu">enumerate</span>(train_dataloader):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&gt;</span> <span class="dv">20</span>:</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What’s happening:</strong> Loop through batches, stopping after 20 iterations (for demonstration).</p>
<section id="step-1-move-data-to-gpu-lines-143-144" class="level4">
<h4 class="anchored" data-anchor-id="step-1-move-data-to-gpu-lines-143-144">Step 1: Move Data to GPU (Lines 143-144)</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> record_function(<span class="st">"data_movement"</span>):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    batch <span class="op">=</span> {k: v.to(device) <span class="cf">for</span> k, v <span class="kw">in</span> batch.items()}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What’s happening:</strong> Transfer the batch from CPU memory to GPU memory.</p>
</section>
<section id="step-2-forward-pass-lines-146-147" class="level4">
<h4 class="anchored" data-anchor-id="step-2-forward-pass-lines-146-147">Step 2: Forward Pass (Lines 146-147)</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> record_function(<span class="st">"forward"</span>):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> model(<span class="op">**</span>batch)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What’s happening:</strong> - Run the model on the input data - Each GPU processes its own batch independently - Calculate predictions and loss</p>
</section>
<section id="step-3-backward-pass-lines-148-149" class="level4">
<h4 class="anchored" data-anchor-id="step-3-backward-pass-lines-148-149">Step 3: Backward Pass (Lines 148-149)</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> record_function(<span class="st">"backward"</span>):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    output.loss.backward()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What’s happening:</strong> - Calculate gradients using backpropagation - Each GPU calculates gradients based on its own batch - At this point, gradients are still different on each GPU!</p>
</section>
<section id="step-4-synchronize-gradients-lines-151-152" class="level4">
<h4 class="anchored" data-anchor-id="step-4-synchronize-gradients-lines-151-152">Step 4: Synchronize Gradients (Lines 151-152)</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> record_function(<span class="st">"sync_grads"</span>):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    model.sync_gradients()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What’s happening:</strong> - <strong>THIS IS THE MAGIC!</strong> - All GPUs communicate and average their gradients - After this step, all GPUs have identical gradients - This makes it as if we processed all batches on a single GPU</p>
</section>
<section id="step-5-update-model-lines-154-158" class="level4">
<h4 class="anchored" data-anchor-id="step-5-update-model-lines-154-158">Step 5: Update Model (Lines 154-158)</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> record_function(<span class="st">"opt_step"</span>):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What’s happening:</strong> 1. Update model parameters using the averaged gradients 2. Reset gradients to zero for the next iteration 3. Since all GPUs have the same gradients, they all update identically 4. Models stay synchronized!</p>
<hr>
</section>
</section>
<section id="cleanup-lines-160-163" class="level3">
<h3 class="anchored" data-anchor-id="cleanup-lines-160-163">8. Cleanup (Lines 160-163)</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> profiler_context:</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    profiler_context.<span class="fu">__exit__</span>(<span class="va">None</span>, <span class="va">None</span>, <span class="va">None</span>)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>dist.destroy_process_group()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What’s happening:</strong> - Close the profiler - Destroy the process group (disconnect GPUs from each other)</p>
<hr>
</section>
</section>
<section id="the-big-picture-how-ddp-works" class="level2">
<h2 class="anchored" data-anchor-id="the-big-picture-how-ddp-works">The Big Picture: How DDP Works</h2>
<section id="the-ddp-workflow" class="level3">
<h3 class="anchored" data-anchor-id="the-ddp-workflow">The DDP Workflow</h3>
<ol type="1">
<li><strong>Initialization:</strong> All GPUs start with identical model copies</li>
<li><strong>Data Sharding:</strong> Each GPU gets a different subset of the training data</li>
<li><strong>Independent Forward/Backward:</strong> Each GPU processes its own data independently</li>
<li><strong>Gradient Synchronization:</strong> GPUs communicate and average their gradients</li>
<li><strong>Synchronized Update:</strong> All GPUs update their models identically</li>
<li><strong>Repeat:</strong> Back to step 3 for the next batch</li>
</ol>
</section>
<section id="why-ddp-is-powerful" class="level3">
<h3 class="anchored" data-anchor-id="why-ddp-is-powerful">Why DDP is Powerful</h3>
<p><strong>Speed:</strong> With N GPUs, you process N times more data per iteration!</p>
<p><strong>Example:</strong> - Single GPU: Process 8 samples per iteration - 4 GPUs with DDP: Process 32 samples per iteration (8 per GPU) - This is like having a batch size of 32, but the memory usage per GPU is only for batch size 8!</p>
<p><strong>Equivalence to Single GPU:</strong> DDP is mathematically equivalent to training on a single GPU with a larger batch size, because: - You process more samples total (N times more) - Gradients are averaged across all samples - Model updates are based on the averaged gradient</p>
</section>
<section id="key-concepts-recap" class="level3">
<h3 class="anchored" data-anchor-id="key-concepts-recap">Key Concepts Recap</h3>
<ul>
<li><strong>Rank:</strong> The ID of each GPU (0, 1, 2, …)</li>
<li><strong>World Size:</strong> Total number of GPUs</li>
<li><strong>Broadcast:</strong> Copy data from one GPU to all others</li>
<li><strong>All-Reduce:</strong> Combine data from all GPUs (sum, average, etc.)</li>
<li><strong>Data Sharding:</strong> Split dataset so each GPU gets different samples</li>
<li><strong>Gradient Synchronization:</strong> Average gradients across all GPUs</li>
</ul>
<hr>
</section>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>This code demonstrates a simplified version of PyTorch’s Distributed Data Parallelism. The key insight is:</p>
<blockquote class="blockquote">
<p>Each GPU works on different data independently, but they synchronize their gradients after backpropagation, ensuring all GPUs learn the same model together.</p>
</blockquote>
<p>By splitting the work across multiple GPUs, you can train models much faster without changing the final result!</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/your-website-url\.example\.com");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>