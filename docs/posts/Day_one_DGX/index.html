<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Dipankar Baisya">
<meta name="dcterms.date" content="2025-11-25">

<title>Day One with DGX Spark: From Setup to Running Local LLMs – My Blogs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">My Blogs</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Day One with DGX Spark: From Setup to Running Local LLMs</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">GPU-computing</div>
                <div class="quarto-category">nvidia-dgx</div>
                <div class="quarto-category">LLM</div>
                <div class="quarto-category">deployment</div>
                <div class="quarto-category">ollama</div>
                <div class="quarto-category">open-webui</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Dipankar Baisya </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 25, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="introduction-meet-the-dgx-spark" class="level2">
<h2 class="anchored" data-anchor-id="introduction-meet-the-dgx-spark">Introduction: Meet the DGX Spark</h2>
<p>The NVIDIA DGX Spark, launched in October 2025, represents a significant leap in making enterprise-grade AI infrastructure accessible to individual developers and researchers. This compact AI supercomputer packs an impressive 1 petaFLOP of AI performance into a desktop form factor, powered by the NVIDIA GB10 Grace Blackwell Superchip. At its heart, the system features a 20-core Arm processor (10 Cortex-X925 performance cores and 10 Cortex-A725 efficiency cores) paired with 128GB of unified memory that’s seamlessly shared between the CPU and GPU. This unified memory architecture is particularly powerful for AI workloads, allowing the system to run inference on models with up to 200 billion parameters and fine-tune models up to 70 billion parameters locally. With 4TB of solid-state storage and dual QSFP Ethernet ports providing 200 Gb/s of aggregate bandwidth, the DGX Spark at $3,999 brings what was once exclusive to data centers right to your desk.</p>
</section>
<section id="a-personal-note-from-gtx-1080-ti-to-grace-blackwell" class="level2">
<h2 class="anchored" data-anchor-id="a-personal-note-from-gtx-1080-ti-to-grace-blackwell">A Personal Note: From GTX 1080 Ti to Grace Blackwell</h2>
<p>My journey with GPUs began back in October 2017, when I was setting up my lab’s computer with an NVIDIA GTX 1080 Ti. It was time I was persuing a PhD (1st year). That experience introduced me to the world of GPU-accelerated computing and sparked my interest in leveraging specialized hardware for computational tasks. Fast forward eight years, and the DGX Spark represents my second foray into GPU ownership—though calling it just a “GPU” would be a significant understatement. While the GTX 1080 Ti was a powerful graphics card in its time, the DGX Spark is an entirely different beast: a complete AI supercomputer that integrates CPU, GPU, and unified memory into a cohesive system designed specificallyfor AI workloads. The evolution from a single GPU card to this integrated Grace Blackwell architecture reflects not just technological progress, but also the democratization of AI infrastructure that was once accessible only to large research institutions and tech giants</p>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/DGX.jpg" height="400" class="figure-img"></p>
<figcaption>NVIDIA DGX Spark - A complete AI supercomputer featuring the Grace Blackwell architecture, representing eight years of technological evolution</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/GTX1080.jpg" height="400" class="figure-img"></p>
<figcaption>NVIDIA GTX 1080 Ti - My first GPU from October 2017, a powerful graphics card that introduced me to GPU-accelerated computing</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="evolution-of-gpu-computing-a-side-by-side-comparison" class="level2">
<h2 class="anchored" data-anchor-id="evolution-of-gpu-computing-a-side-by-side-comparison">Evolution of GPU Computing: A Side-by-Side Comparison</h2>
<p>The eight-year gap between these two systems tells a remarkable story of technological advancement, particularly in the shift from graphics-focused GPUs to AI-specialized computing platforms.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 30%">
<col style="width: 27%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Specification</strong></th>
<th><strong>GTX 1080 Ti (2017)</strong></th>
<th><strong>DGX Spark (2025)</strong></th>
<th><strong>Evolution</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Architecture</strong></td>
<td>Pascal (16nm)</td>
<td>Grace Blackwell GB10 (3nm)</td>
<td>Integrated CPU-GPU superchip design</td>
</tr>
<tr class="even">
<td><strong>CPU Cores</strong></td>
<td>Host system dependent</td>
<td>20 Arm cores (10 X925 + 10 A725)</td>
<td>Integrated high-performance CPU</td>
</tr>
<tr class="odd">
<td><strong>CUDA Cores</strong></td>
<td>3,584</td>
<td>6,144</td>
<td><strong>71% increase</strong> in CUDA cores</td>
</tr>
<tr class="even">
<td><strong>Tensor Cores</strong></td>
<td>None</td>
<td>192 (5th generation)</td>
<td>AI-optimized matrix operations</td>
</tr>
<tr class="odd">
<td><strong>Memory Capacity</strong></td>
<td>11 GB GDDR5X</td>
<td>128 GB LPDDR5x Unified</td>
<td><strong>11.6× increase</strong> in capacity</td>
</tr>
<tr class="even">
<td><strong>Memory Architecture</strong></td>
<td>Dedicated GPU memory</td>
<td>CPU-GPU unified coherent memory</td>
<td>Seamless sharing eliminates data transfer overhead</td>
</tr>
<tr class="odd">
<td><strong>Memory Bandwidth</strong></td>
<td>484 GB/s</td>
<td>273 GB/s unified</td>
<td>Coherent memory access across CPU-GPU</td>
</tr>
<tr class="even">
<td><strong>FP32 Performance</strong></td>
<td>11.5 TFLOPS</td>
<td>31 TFLOPS</td>
<td><strong>2.7× increase</strong> in traditional compute</td>
</tr>
<tr class="odd">
<td><strong>AI Performance</strong></td>
<td>~11.5 TFLOPS (FP32 only)</td>
<td>1,000 TOPS (FP4)<br>1 PETAFLOP</td>
<td><strong>~87× increase</strong> with AI-optimized precision</td>
</tr>
<tr class="even">
<td><strong>Precision Support</strong></td>
<td>FP32, FP16</td>
<td>FP32, FP16, FP8, NVFP4, MXFP8</td>
<td>Multi-precision for optimal AI inference</td>
</tr>
<tr class="odd">
<td><strong>Model Capacity</strong></td>
<td>Limited by 11GB</td>
<td>200B parameters (inference)<br>70B parameters (fine-tuning)</td>
<td>Native support for frontier models</td>
</tr>
<tr class="even">
<td><strong>Power Consumption</strong></td>
<td>250W</td>
<td>140W TDP (240W with accessories)</td>
<td>Dramatically more efficient</td>
</tr>
<tr class="odd">
<td><strong>Primary Use Case</strong></td>
<td>Gaming &amp; Graphics</td>
<td>AI Development &amp; Inference</td>
<td>Purpose-built for AI/ML workflows</td>
</tr>
<tr class="even">
<td><strong>Price at Launch</strong></td>
<td>~$699</td>
<td>$3,999</td>
<td>Premium for integrated AI platform</td>
</tr>
<tr class="odd">
<td><strong>Form Factor</strong></td>
<td>PCIe Graphics Card</td>
<td>Complete Desktop System</td>
<td>Self-contained AI workstation</td>
</tr>
</tbody>
</table>
<section id="key-architectural-breakthroughs" class="level3">
<h3 class="anchored" data-anchor-id="key-architectural-breakthroughs">Key Architectural Breakthroughs</h3>
<p><strong>Unified Memory Revolution</strong>: The most significant advancement is the shift from discrete GPU memory to unified coherent memory. The GTX 1080 Ti required explicit data transfers between system RAM and GPU memory, creating bottlenecks. The DGX Spark’s 128GB unified memory is seamlessly accessible to both CPU and GPU, eliminating these transfers and enabling efficient processing of models that were impossible on traditional GPUs.</p>
<p><strong>Fifth-Generation Tensor Cores</strong>: Perhaps the most transformative feature is the inclusion of 192 fifth-generation Tensor Cores—technology completely absent from the GTX 1080 Ti. Tensor Cores are specialized processing units designed specifically for the matrix multiplication operations that dominate neural network training and inference. Each Tensor Core can perform multiple operations per clock cycle on matrices, dramatically accelerating AI workloads compared to traditional CUDA cores.</p>
<p>What makes the fifth-generation Tensor Cores in the DGX Spark particularly powerful is their tight integration with 256KB of Tensor Memory (TMEM) per Streaming Multiprocessor (SM). This keeps frequently accessed data close to the compute units, minimizing memory latency and maximizing throughput. The Blackwell architecture features four Tensor Cores per SM, optimized specifically for transformer-based models that have become the foundation of modern AI.</p>
<p><strong>Transformer Engine and Multi-Precision Support</strong>: The DGX Spark includes NVIDIA’s second-generation Transformer Engine, a game-changing feature for LLM inference and fine-tuning. While the GTX 1080 Ti was limited to FP32 and FP16 precision, the DGX Spark supports a range of precision formats optimized for different AI tasks:</p>
<ul>
<li><strong>FP32 (32-bit)</strong>: Traditional floating-point for general computing (31 TFLOPS)</li>
<li><strong>FP16 (16-bit)</strong>: Half-precision for training and inference</li>
<li><strong>FP8 (8-bit)</strong>: Introduced with H100, using E4M3 and E5M2 variants for efficient AI operations</li>
<li><strong>MXFP8</strong>: Blackwell’s microscaling FP8 format with block-level scaling factors for improved accuracy</li>
<li><strong>NVFP4 (4-bit)</strong>: Blackwell’s proprietary 4-bit floating-point format using two-level scaling, achieving near-FP8 accuracy while reducing memory footprint by 1.8× and enabling 1 PETAFLOP of AI performance</li>
</ul>
<p>The Transformer Engine dynamically selects the optimal precision for each layer during inference, balancing accuracy and performance. For LLM inference, FP4 precision delivers massive throughput gains—enabling the 87× performance advantage over FP32-only systems—while maintaining acceptable accuracy for most use cases. This is why the DGX Spark can handle 200B parameter models that would be impossible on the GTX 1080 Ti’s 11GB of memory.</p>
<p><strong>Integration vs.&nbsp;Component</strong>: The GTX 1080 Ti was a component requiring a host system, while the DGX Spark is a complete, integrated platform with CPU, GPU, storage, and networking designed to work in harmony for AI workloads. The NVLink-C2C chip-to-chip interconnect provides high-bandwidth, low-latency communication between the Grace CPU and Blackwell GPU, enabling the unified memory architecture that eliminates traditional PCIe bottlenecks.</p>
</section>
</section>
<section id="setting-up-dgx-spark-after-unpacking" class="level2">
<h2 class="anchored" data-anchor-id="setting-up-dgx-spark-after-unpacking">Setting up DGX Spark after Unpacking</h2>
<p>After unboxing DGX Spark, first thing stands out to me its portability. Its compact and simple to set up and then paired with a Mac book (or any other computer). Before setting up, we need to see the ports of DGX Spark. Lets have a look at the ports of DGX Spark in the diagram below:</p>
<div style="text-align: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/DGX_ports.jpg" height="400" class="figure-img"></p>
<figcaption>NVIDIA DGX Spark - Ports and Connectivity</figcaption>
</figure>
</div>
</div>
<p>First step is to connect the one end of the power adapter to the Power port and other end to power supply (socket). Click the the On/Off button and that’s it. One thing I need to mention that there is no led light indicating if the device is turned on or off. The only way to find out if its on is by detecting the Hotspot in the Wifi . The info of the Hotspot can be found on the cover of Quick Start Guide.</p>
<div style="text-align: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/Quick_start.jpg" height="400" class="figure-img"></p>
<figcaption>NVIDIA DGX Spark - Quick Start Guide</figcaption>
</figure>
</div>
</div>
<p>Once connected with the HotSpot the set up is inititated and just need to follow the the instruction . At some point it will identify the orginal Wifi you are connected in . Then it may initiate a few updates of the firmware and reboots . No human interventions needed .</p>
<div style="text-align: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/set_up.jpg" height="300" class="figure-img"></p>
<figcaption>NVIDIA DGX Spark - Set up completed</figcaption>
</figure>
</div>
</div>
<p>After the complition of the set up its going to direct you to the <a href="https://build.nvidia.com/spark">spark page</a></p>
</section>
<section id="connecting-to-dgx-spark-on-your-local-network" class="level2">
<h2 class="anchored" data-anchor-id="connecting-to-dgx-spark-on-your-local-network">Connecting to DGX Spark on Your Local Network</h2>
<p>Before diving into running LLMs, it’s essential to establish reliable network access to your DGX Spark. While the initial setup happens over the device’s WiFi hotspot, for day-to-day work you’ll want to connect to your DGX Spark over your local network via SSH. This gives you command-line access and the ability to tunnel ports for accessing web applications.</p>
<section id="prerequisites" class="level3">
<h3 class="anchored" data-anchor-id="prerequisites">Prerequisites</h3>
<p>First, verify that you have an SSH client installed. Most modern operating systems (Linux, macOS, Windows 10+) include SSH by default:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ssh</span> <span class="at">-V</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>You’ll need the following information: - <strong>Username</strong>: Your DGX Spark account name (created during initial setup) - <strong>Password</strong>: Your account password - <strong>Hostname</strong>: Your device’s mDNS hostname (typically <code>spark-xxxx.local</code>) - <strong>IP Address</strong>: As a backup if mDNS doesn’t work on your network</p>
</section>
<section id="finding-your-dgx-spark-on-the-network" class="level3">
<h3 class="anchored" data-anchor-id="finding-your-dgx-spark-on-the-network">Finding Your DGX Spark on the Network</h3>
<p>The DGX Spark uses mDNS (multicast DNS) for easy discovery on local networks. Your hostname format is typically <code>spark-xxxx.local</code> where <code>xxxx</code> is a unique identifier.</p>
<p>Test if mDNS resolution works on your network:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ping</span> spark-abcd.local</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Replace <code>spark-abcd</code> with your actual hostname. If you see ping responses with IP addresses and latency measurements, mDNS is working correctly.</p>
<p>If you get “Cannot resolve hostname” or “Unknown host” errors, your network doesn’t support mDNS (common in corporate networks). In this case, you’ll need to find your DGX Spark’s IP address through your router’s admin panel or by connecting a display directly to the device.</p>
</section>
<section id="establishing-ssh-connection" class="level3">
<h3 class="anchored" data-anchor-id="establishing-ssh-connection">Establishing SSH Connection</h3>
<p>Once you have your hostname or IP address, connect via SSH:</p>
<p><strong>Using mDNS hostname:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ssh</span> <span class="op">&lt;</span>username<span class="op">&gt;</span>@<span class="op">&lt;</span>spark-hostname<span class="op">&gt;</span>.local</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Using IP address (if mDNS fails):</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ssh</span> <span class="op">&lt;</span>username<span class="op">&gt;</span>@<span class="op">&lt;</span>ip_address<span class="op">&gt;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>For example:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ssh</span> dipankar@spark-a1b2.local</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>On your first connection, you’ll see a host fingerprint warning. Type <code>yes</code> to accept and add the host to your known hosts file, then enter your password.</p>
</section>
<section id="verifying-your-connection" class="level3">
<h3 class="anchored" data-anchor-id="verifying-your-connection">Verifying Your Connection</h3>
<p>Once connected, verify you’re on the correct device:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">hostname</span>        <span class="co"># Should show your DGX Spark hostname</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">uname</span> <span class="at">-a</span>        <span class="co"># Shows system information</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="ex">nvidia-smi</span>      <span class="co"># Check GPU status (if available)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Type <code>exit</code> to close the SSH session.</p>
</section>
<section id="ssh-port-forwarding-for-web-applications" class="level3">
<h3 class="anchored" data-anchor-id="ssh-port-forwarding-for-web-applications">SSH Port Forwarding for Web Applications</h3>
<p>One of the most powerful features of SSH is port forwarding, which allows you to access web applications running on your DGX Spark as if they were running on your local machine. This is crucial for accessing services like Open WebUI, Jupyter notebooks, or monitoring dashboards.</p>
<p>For example, to access a web service running on port 8080 on your DGX Spark:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ssh</span> <span class="at">-L</span> 8080:localhost:8080 <span class="op">&lt;</span>username<span class="op">&gt;</span>@<span class="op">&lt;</span>spark-hostname<span class="op">&gt;</span>.local</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Now, opening <code>http://localhost:8080</code> in your browser will connect to the service running on your DGX Spark. The <code>-L</code> flag creates a local port forward, tunneling traffic from your local port 8080 to the DGX Spark’s port 8080.</p>
<p>You can forward multiple ports in a single SSH session:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ssh</span> <span class="at">-L</span> 8080:localhost:8080 <span class="at">-L</span> 11000:localhost:11000 <span class="op">&lt;</span>username<span class="op">&gt;</span>@<span class="op">&lt;</span>spark-hostname<span class="op">&gt;</span>.local</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This is particularly useful when running Open WebUI (typically on port 8080) alongside other monitoring or development tools.</p>
</section>
<section id="troubleshooting-network-access" class="level3">
<h3 class="anchored" data-anchor-id="troubleshooting-network-access">Troubleshooting Network Access</h3>
<p><strong>mDNS not working?</strong> - Check if your router supports mDNS/Bonjour - Try connecting from a different network segment - Use the IP address directly instead - On corporate networks, consult with IT about mDNS availability</p>
<p><strong>Can’t find the IP address?</strong> - Check your router’s DHCP client list - Connect a monitor and keyboard directly to the DGX Spark - Use network scanning tools like <code>nmap</code> or <code>arp-scan</code></p>
<p><strong>Connection refused or timeout?</strong> - Verify the DGX Spark is powered on and connected to WiFi - Check firewall settings on both machines - Ensure you’re on the same network subnet</p>
<p>With network access established, you’re ready to start deploying AI workloads on your DGX Spark.</p>
</section>
</section>
<section id="running-local-llms-setting-up-open-webui-and-ollama" class="level2">
<h2 class="anchored" data-anchor-id="running-local-llms-setting-up-open-webui-and-ollama">Running Local LLMs: Setting Up Open WebUI and Ollama</h2>
<p>With the DGX Spark hardware setup complete, it’s time to put this AI supercomputer to work. The real power of the DGX Spark lies in its ability to run large language models locally, giving you full control over your AI infrastructure without relying on external APIs. In this section, I’ll walk through setting up <a href="https://build.nvidia.com/spark/open-webui/instructions">Open WebUI</a> with Ollama using Docker, and then demonstrate how to interact with these models programmatically using Python.</p>
<section id="why-open-webui-and-ollama" class="level3">
<h3 class="anchored" data-anchor-id="why-open-webui-and-ollama">Why Open WebUI and Ollama?</h3>
<p><strong>Ollama</strong> provides a simple, efficient way to run large language models locally. It handles model management, optimization, and inference, making it easy to work with models ranging from small 7B parameter models to the 200B parameter models that the DGX Spark can handle.</p>
<p><strong>Open WebUI</strong> offers a clean, ChatGPT-like interface for interacting with Ollama models. More importantly, it exposes an OpenAI-compatible API, which means you can use familiar tools and libraries to interact with your local models as if they were OpenAI’s GPT models.</p>
</section>
<section id="step-1-setting-up-docker-access" class="level3">
<h3 class="anchored" data-anchor-id="step-1-setting-up-docker-access">Step 1: Setting Up Docker Access</h3>
<p>Before we begin, we need to ensure Docker is properly configured. First, check if you have Docker access:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> ps</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>If you encounter permission errors, add your user to the docker group:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> usermod <span class="at">-aG</span> docker <span class="va">$USER</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="ex">newgrp</span> docker</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="step-2-deploying-open-webui-with-ollama" class="level3">
<h3 class="anchored" data-anchor-id="step-2-deploying-open-webui-with-ollama">Step 2: Deploying Open WebUI with Ollama</h3>
<p>The beauty of this setup is its simplicity. Open WebUI provides a container image with Ollama integrated, eliminating the need for separate installations.</p>
<p>Pull the container image:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> pull ghcr.io/open-webui/open-webui:ollama</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Launch the container with GPU support and persistent storage:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> run <span class="at">-d</span> <span class="at">-p</span> 8080:8080 <span class="at">--gpus</span><span class="op">=</span>all <span class="dt">\</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">-v</span> open-webui:/app/backend/data <span class="dt">\</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">-v</span> open-webui-ollama:/root/.ollama <span class="dt">\</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">--name</span> open-webui ghcr.io/open-webui/open-webui:ollama</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The <code>--gpus=all</code> flag is crucial—it gives the container access to the DGX Spark’s powerful Grace Blackwell GPU. The two volume mounts ensure that your application data and downloaded models persist across container restarts.</p>
<p>Once running, navigate to <code>http://&lt;spark-ip&gt;:8080</code> in your browser. You’ll be greeted with a setup screen where you can create your administrator account.</p>
</section>
<section id="step-3-downloading-your-first-model" class="level3">
<h3 class="anchored" data-anchor-id="step-3-downloading-your-first-model">Step 3: Downloading Your First Model</h3>
<p>Through the Open WebUI interface, you can access Ollama’s extensive model library. For my first test, I downloaded the <code>gpt-oss:20b</code> model—a 20 billion parameter open-source model that showcases the DGX Spark’s capability to handle frontier-scale models locally.</p>
<p>The download process happens directly on your DGX Spark, leveraging the 4TB of storage. Depending on the model size and your network speed, this can take several minutes, but the unified 128GB memory architecture means these large models can run entirely in RAM without swapping to disk.</p>
</section>
<section id="step-4-interacting-with-models-via-python" class="level3">
<h3 class="anchored" data-anchor-id="step-4-interacting-with-models-via-python">Step 4: Interacting with Models via Python</h3>
<p>While the web interface is great for interactive chat, the real power comes from programmatic access. I created a Python client to interact with the Ollama models running on the DGX Spark through Open WebUI’s OpenAI-compatible API.</p>
<p>Here’s the setup process I followed (you can find the complete code in my <a href="https://github.com/daddyofadoggy/DGX_ollama">DGX_ollama repository</a>):</p>
<p><strong>1. Clone and set up the environment:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> clone https://github.com/daddyofadoggy/DGX_ollama.git</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> DGX_ollama</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> <span class="at">-m</span> venv myvenv</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> myvenv/bin/activate</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install <span class="at">-r</span> requirements.txt</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>2. Configure authentication:</strong></p>
<p>Create a <code>.env</code> file with your API key (generated from the Open WebUI settings):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="va">OLLAMA_KEY</span><span class="op">=</span>your_api_key_here</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>3. The Python client:</strong></p>
<p>The implementation is remarkably simple thanks to the OpenAI-compatible API:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> openai <span class="im">import</span> OpenAI</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dotenv <span class="im">import</span> load_dotenv</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>load_dotenv()</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>client <span class="op">=</span> OpenAI(</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    base_url<span class="op">=</span><span class="st">"http://10.0.0.194:8080/ollama/v1"</span>,</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    api_key<span class="op">=</span>os.getenv(<span class="st">"OLLAMA_KEY"</span>)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> client.chat.completions.create(</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span><span class="st">"gpt-oss:20b"</span>,</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    messages<span class="op">=</span>[</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: <span class="st">"Explain the benefits of unified memory architecture for LLM inference"</span>}</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response.choices[<span class="dv">0</span>].message.content)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This code looks identical to how you’d interact with OpenAI’s API, but it’s hitting your local DGX Spark instead. The model runs entirely on your hardware, leveraging those 192 Tensor Cores and the unified memory architecture we discussed earlier.</p>
</section>
<section id="what-makes-this-setup-powerful" class="level3">
<h3 class="anchored" data-anchor-id="what-makes-this-setup-powerful">What Makes This Setup Powerful</h3>
<p>The combination of DGX Spark’s hardware capabilities and this software stack creates a remarkably powerful local AI development environment:</p>
<ol type="1">
<li><p><strong>Privacy and Control</strong>: Your data never leaves your machine. For sensitive applications or proprietary data, this is invaluable.</p></li>
<li><p><strong>No API Costs</strong>: Once you’ve invested in the DGX Spark, there are no per-token charges. Run as many inferences as you want.</p></li>
<li><p><strong>Customization</strong>: You can fine-tune models on your own data, experiment with different quantization levels, and optimize for your specific use cases.</p></li>
<li><p><strong>Low Latency</strong>: No network round trips to external APIs. With the unified memory architecture, inference happens at local GPU speeds.</p></li>
<li><p><strong>OpenAI-Compatible Interface</strong>: Existing code using OpenAI’s SDK works with minimal modifications—just change the base URL and API key.</p></li>
</ol>
<p>In my initial experiments, the DGX Spark handled the 20B parameter model effortlessly, with inference times that rival cloud-based solutions. The Transformer Engine’s dynamic precision selection and those fifth-generation Tensor Cores make a noticeable difference in real-world performance.</p>
</section>
<section id="cleaning-up-after-experiments" class="level3">
<h3 class="anchored" data-anchor-id="cleaning-up-after-experiments">Cleaning Up After Experiments</h3>
<p>When you’re done experimenting or want to free up resources, it’s important to properly clean up your Docker containers and volumes. Here’s how to do it systematically:</p>
<p><strong>1. Stop the running container:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> stop open-webui</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>2. Remove the container:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> rm open-webui</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>3. (Optional) Remove the volumes:</strong></p>
<p>If you want to completely start fresh and remove all data including downloaded models, you can delete the volumes. <strong>Warning</strong>: This will delete all your settings, chat history, and downloaded models.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> volume rm open-webui</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> volume rm open-webui-ollama</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>4. (Optional) Remove the Docker image:</strong></p>
<p>To free up disk space, you can also remove the Docker image itself:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> rmi ghcr.io/open-webui/open-webui:ollama</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>5. Verify cleanup:</strong></p>
<p>Check that everything has been removed:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb20"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check for running containers</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> ps <span class="at">-a</span> <span class="kw">|</span> <span class="fu">grep</span> open-webui</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Check for volumes</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> volume ls <span class="kw">|</span> <span class="fu">grep</span> open-webui</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Check for images</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> images <span class="kw">|</span> <span class="fu">grep</span> open-webui</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Pro Tip</strong>: If you want to preserve your models but clean up the application data, you can selectively remove only the <code>open-webui</code> volume while keeping <code>open-webui-ollama</code>. This way, you won’t need to re-download large models if you decide to set up Open WebUI again later.</p>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<section id="official-nvidia-documentation" class="level3">
<h3 class="anchored" data-anchor-id="official-nvidia-documentation">Official NVIDIA Documentation</h3>
<ol type="1">
<li><p><strong>NVIDIA DGX Spark Product Page</strong> <a href="https://www.nvidia.com/en-us/products/workstations/dgx-spark/">https://www.nvidia.com/en-us/products/workstations/dgx-spark/</a></p></li>
<li><p><strong>NVIDIA DGX Spark Arrives for World’s AI Developers | NVIDIA Newsroom</strong> <a href="https://nvidianews.nvidia.com/news/nvidia-dgx-spark-arrives-for-worlds-ai-developers">https://nvidianews.nvidia.com/news/nvidia-dgx-spark-arrives-for-worlds-ai-developers</a></p></li>
<li><p><strong>Hardware Overview — DGX Spark User Guide</strong> <a href="https://docs.nvidia.com/dgx/dgx-spark/hardware.html">https://docs.nvidia.com/dgx/dgx-spark/hardware.html</a></p></li>
<li><p><strong>NVIDIA Blackwell Architecture</strong> <a href="https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/">https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/</a></p></li>
<li><p><strong>Inside NVIDIA Blackwell Ultra | NVIDIA Technical Blog</strong> <a href="https://developer.nvidia.com/blog/inside-nvidia-blackwell-ultra-the-chip-powering-the-ai-factory-era/">https://developer.nvidia.com/blog/inside-nvidia-blackwell-ultra-the-chip-powering-the-ai-factory-era/</a></p></li>
<li><p><strong>Using FP8 and FP4 with Transformer Engine — NVIDIA Documentation</strong> <a href="https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html">https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html</a></p></li>
</ol>
</section>
<section id="hardware-specifications" class="level3">
<h3 class="anchored" data-anchor-id="hardware-specifications">Hardware Specifications</h3>
<ol start="7" type="1">
<li><p><strong>NVIDIA DGX Spark features 6144 CUDA cores | VideoCardz</strong> <a href="https://videocardz.com/newz/nvidia-dgx-spark-features-6144-cuda-cores-just-as-many-as-rtx-5070">https://videocardz.com/newz/nvidia-dgx-spark-features-6144-cuda-cores-just-as-many-as-rtx-5070</a></p></li>
<li><p><strong>NVIDIA Dissects GB10 Superchip | WCCFtech</strong> <a href="https://wccftech.com/nvidia-gb10-superchip-soc-3nm-20-arm-v9-2-cpu-cores-nvfp4-blackwell-gpu-lpddr5x-9400-memory-140w-tdp/">https://wccftech.com/nvidia-gb10-superchip-soc-3nm-20-arm-v9-2-cpu-cores-nvfp4-blackwell-gpu-lpddr5x-9400-memory-140w-tdp/</a></p></li>
<li><p><strong>NVIDIA GeForce GTX 1080 Ti Specs | TechPowerUp</strong> <a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1080-ti.c2877">https://www.techpowerup.com/gpu-specs/geforce-gtx-1080-ti.c2877</a></p></li>
<li><p><strong>Official nVidia GTX 1080 Ti Specs | GamersNexus</strong> <a href="https://gamersnexus.net/news-pc/2820-official-nvidia-gtx-1080-ti-specs-announced">https://gamersnexus.net/news-pc/2820-official-nvidia-gtx-1080-ti-specs-announced</a></p></li>
</ol>
</section>
<section id="setup-guides-and-tools" class="level3">
<h3 class="anchored" data-anchor-id="setup-guides-and-tools">Setup Guides and Tools</h3>
<ol start="11" type="1">
<li><p><strong>Open WebUI with Ollama Setup Guide for DGX Spark</strong> <a href="https://build.nvidia.com/spark/open-webui/instructions">https://build.nvidia.com/spark/open-webui/instructions</a></p></li>
<li><p><strong>Connect to Your DGX Spark via SSH</strong> <a href="https://build.nvidia.com/spark/connect-to-your-spark/manual-ssh">https://build.nvidia.com/spark/connect-to-your-spark/manual-ssh</a></p></li>
<li><p><strong>DGX Ollama Python Client | GitHub Repository</strong> <a href="https://github.com/daddyofadoggy/DGX_ollama">https://github.com/daddyofadoggy/DGX_ollama</a></p></li>
<li><p><strong>Setting Up Open WebUI on DGX Spark | YouTube Tutorial</strong> <a href="https://www.youtube.com/watch?v=yOgNv4HrYZ4">https://www.youtube.com/watch?v=yOgNv4HrYZ4</a></p></li>
</ol>
</section>
<section id="reviews-and-analysis" class="level3">
<h3 class="anchored" data-anchor-id="reviews-and-analysis">Reviews and Analysis</h3>
<ol start="15" type="1">
<li><p><strong>NVIDIA DGX Spark In-Depth Review | LMSYS Org</strong> <a href="https://lmsys.org/blog/2025-10-13-nvidia-dgx-spark/">https://lmsys.org/blog/2025-10-13-nvidia-dgx-spark/</a></p></li>
<li><p><strong>NVIDIA DGX Spark Review | IntuitionLabs</strong> <a href="https://intuitionlabs.ai/articles/nvidia-dgx-spark-review">https://intuitionlabs.ai/articles/nvidia-dgx-spark-review</a></p></li>
</ol>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/your-website-url\.example\.com");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>