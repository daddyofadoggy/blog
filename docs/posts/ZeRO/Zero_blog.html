<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>zero_blog ‚Äì My Blogs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">My Blogs</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block"></header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction-the-memory-wall-problem" class="level2">
<h2 class="anchored" data-anchor-id="introduction-the-memory-wall-problem">1. Introduction: The Memory Wall Problem</h2>
<p>Modern deep learning has witnessed an explosive growth in model sizes, driven by a simple observation: <strong>larger models deliver better performance</strong>. In Natural Language Processing alone, we‚Äôve seen a remarkable progression from BERT-Large with 0.3 billion parameters to GPT-2 (1.5B), T5 (11B), gpt-oss (117B) etc. Each increase in model size has brought significant accuracy gains, pushing the boundaries of what‚Äôs possible in language understanding and generation.</p>
<p>But there‚Äôs a problem‚Äîa critical bottleneck that threatens to halt this progress: <strong>memory</strong>.</p>
<section id="the-paradox-of-model-training" class="level3">
<h3 class="anchored" data-anchor-id="the-paradox-of-model-training">The Paradox of Model Training</h3>
<p>Consider this striking example from the ZeRO paper: A GPT-2 model with 1.5 billion parameters requires only <strong>3GB of memory</strong> to store its weights in 16-bit precision. Yet, this same model <strong>cannot be trained on a single 32GB V100 GPU</strong> using standard frameworks like PyTorch or TensorFlow [ZeRO Paper, p.7].</p>
<p>Where does all the memory go? If the model parameters only need 3GB, why can‚Äôt we use the remaining 29GB for training?</p>
</section>
<section id="the-hidden-memory-costs" class="level3">
<h3 class="anchored" data-anchor-id="the-hidden-memory-costs">The Hidden Memory Costs</h3>
<p>The answer lies in understanding the complete memory footprint of deep learning training. When you train a model, you need much more than just the parameters:</p>
<p><strong>Model States</strong> consume the majority of memory:</p>
<ul>
<li><strong>Optimizer states</strong>: Adam optimizer maintains momentum and variance for each parameter</li>
<li><strong>Gradients</strong>: Required for backpropagation</li>
<li><strong>Parameters</strong>: The model weights themselves</li>
</ul>
<p>For mixed-precision training with Adam optimizer, the memory requirement becomes <strong>16Œ® bytes</strong> for a model with Œ® parameters [ZeRO Paper, p.7-8]:</p>
<ul>
<li>2Œ® bytes for fp16 parameters</li>
<li>2Œ® bytes for fp16 gradients</li>
<li>4Œ® bytes for fp32 parameter copy</li>
<li>4Œ® bytes for fp32 momentum</li>
<li>4Œ® bytes for fp32 variance</li>
</ul>
<p><strong>Total: 16Œ® bytes</strong> just for model states</p>
<p>For our 1.5B parameter GPT-2 example, this translates to at least <strong>24GB</strong> of memory‚Äîalready approaching the 32GB limit before considering any other factors [ZeRO Paper, p.8].</p>
<p><strong>Residual States</strong> add further pressure:</p>
<ul>
<li><strong>Activations</strong>: Can require 60GB for GPT-2 with sequence length 1K and batch size 32 [ZeRO Paper, p.8]</li>
<li><strong>Temporary buffers</strong>: Used for operations like gradient all-reduce</li>
<li><strong>Memory fragmentation</strong>: Unusable memory gaps due to fragmented allocation</li>
</ul>
</section>
<section id="why-current-solutions-fall-short" class="level3">
<h3 class="anchored" data-anchor-id="why-current-solutions-fall-short">Why Current Solutions Fall Short</h3>
<p>The community has developed several approaches to tackle this memory challenge, but each comes with fundamental limitations:</p>
<p><strong>Data Parallelism (DP)</strong> is the simplest approach:</p>
<ul>
<li>‚úÖ <strong>Good</strong>: Excellent compute/communication efficiency</li>
<li>‚ùå <strong>Bad</strong>: Complete memory redundancy‚Äîevery GPU stores identical copies of all model states</li>
<li>üî¥ <strong>Result</strong>: Runs out of memory for models &gt; 1.4B parameters on 32GB GPUs [ZeRO Paper, p.1]</li>
</ul>
<p><strong>Model Parallelism (MP)</strong> splits the model across GPUs:</p>
<ul>
<li>‚úÖ <strong>Good</strong>: Reduces memory per GPU by partitioning the model</li>
<li>‚ùå <strong>Bad</strong>: Requires frequent communication between layers, especially across nodes</li>
<li>‚ùå <strong>Bad</strong>: Reduced computational granularity hurts efficiency</li>
<li>üî¥ <strong>Result</strong>: Efficiency degrades rapidly beyond single nodes. A 40B parameter model achieves only ~5 TFlops per GPU across two DGX-2 nodes‚Äîless than 5% of hardware peak [ZeRO Paper, p.2]</li>
</ul>
<p><strong>Pipeline Parallelism (PP)</strong> splits models horizontally:</p>
<ul>
<li>‚ùå <strong>Bad</strong>: Requires batch size proportional to pipeline stages to hide bubbles</li>
<li>‚ùå <strong>Bad</strong>: Large batch sizes harm convergence</li>
<li>‚ùå <strong>Bad</strong>: Difficult to implement features like tied weights [ZeRO Paper, p.6]</li>
</ul>
<p>The fundamental problem? <strong>All existing approaches make trade-offs between memory efficiency, computational efficiency, and usability</strong>‚Äîbut for large model training, we need all three.</p>
</section>
<section id="enter-zero-zero-redundancy-optimizer" class="level3">
<h3 class="anchored" data-anchor-id="enter-zero-zero-redundancy-optimizer">Enter ZeRO: Zero Redundancy Optimizer</h3>
<p>This is where ZeRO (Zero Redundancy Optimizer) comes in. Developed by Microsoft Research, ZeRO takes a fundamentally different approach by asking a simple but powerful question:</p>
<blockquote class="blockquote">
<p><strong>Why do we replicate model states across all GPUs when we don‚Äôt need all of them all the time?</strong></p>
</blockquote>
<p>ZeRO eliminates memory redundancies across data-parallel processes while retaining the computational granularity and communication efficiency of data parallelism [ZeRO Paper, p.2]. It achieves this through three progressive optimization stages:</p>
<ol type="1">
<li><strong>ZeRO-1 (P_os)</strong>: Partitions optimizer states ‚Üí 4√ó memory reduction</li>
<li><strong>ZeRO-2 (P_os+g)</strong>: Adds gradient partitioning ‚Üí 8√ó memory reduction</li>
<li><strong>ZeRO-3 (P_os+g+p)</strong>: Adds parameter partitioning ‚Üí <strong>Memory reduction scales linearly with number of GPUs</strong></li>
</ol>
<p>According to the paper‚Äôs analysis, ZeRO can train models with <strong>over 1 trillion parameters</strong> using today‚Äôs hardware [ZeRO Paper, p.2-3]. The implementation demonstrated in the paper (ZeRO-100B) successfully trained models up to 170B parameters‚Äîover <strong>8√ó larger</strong> than state-of-the-art at the time‚Äîwhile achieving <strong>10√ó faster</strong> training speeds [ZeRO Paper, p.4].</p>
</section>
<section id="what-youll-learn-in-this-blog" class="level3">
<h3 class="anchored" data-anchor-id="what-youll-learn-in-this-blog">What You‚Äôll Learn in This Blog</h3>
<p>In this comprehensive guide, we‚Äôll take you on a journey from theory to practice:</p>
<ul>
<li><strong>Understand the fundamentals</strong>: Deep dive into where memory goes and why ZeRO‚Äôs approach works</li>
<li><strong>See the math</strong>: Mathematical analysis of memory savings and communication costs</li>
<li><strong>Read the code</strong>: Line-by-line walkthrough of implementing all three ZeRO stages</li>
<li><strong>Analyze real results</strong>: Detailed profiling data from training a 2.3B parameter model</li>
<li><strong>Learn when to use what</strong>: Practical decision framework for choosing ZeRO stages</li>
</ul>
<p>Most importantly, we‚Äôll show you how to <strong>reproduce these results yourself</strong> with the complete implementation available in our repository.</p>
<p>The memory wall doesn‚Äôt have to stop progress in large model training. ZeRO shows us how to break through it‚Äîlet‚Äôs see how it works.</p>
<hr>
</section>
</section>
<section id="background-where-does-memory-go-in-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="background-where-does-memory-go-in-deep-learning">2. Background: Where Does Memory Go in Deep Learning?</h2>
<p>Before we dive into how ZeRO optimizes memory, we need to understand exactly where memory goes during deep learning training. The ZeRO paper categorizes memory consumption into two main parts: <strong>Model States</strong> and <strong>Residual States</strong> [ZeRO Paper, p.7]. Let‚Äôs dissect each component with both theoretical analysis and practical measurements from our experiments.</p>
<section id="model-states-the-primary-memory-consumer" class="level3">
<h3 class="anchored" data-anchor-id="model-states-the-primary-memory-consumer">2.1 Model States: The Primary Memory Consumer</h3>
<p>Model states include everything needed to maintain and update the model during training. For large models, this is typically where most of your memory goes.</p>
<section id="mixed-precision-training-primer" class="level4">
<h4 class="anchored" data-anchor-id="mixed-precision-training-primer">2.1.1 <strong>Mixed-Precision Training Primer</strong></h4>
<p>Modern deep learning training uses <strong>mixed-precision</strong> to leverage specialized hardware like NVIDIA‚Äôs Tensor Cores [ZeRO Paper, p.7]. The strategy is elegant:</p>
<ul>
<li><strong>fp16 (16-bit)</strong> for forward and backward passes ‚Üí Fast computation, less memory</li>
<li><strong>fp32 (32-bit)</strong> for optimizer states and updates ‚Üí Numerical stability</li>
</ul>
<p>This hybrid approach gives us the best of both worlds: speed of fp16 with the stability of fp32.</p>
</section>
<section id="memory-breakdown-with-adam-optimizer" class="level4">
<h4 class="anchored" data-anchor-id="memory-breakdown-with-adam-optimizer">2.1.2 <strong>Memory Breakdown with Adam Optimizer</strong></h4>
<p>Let‚Äôs use Adam optimizer as our example‚Äîit‚Äôs the most popular choice for training large language models. For a model with <strong>Œ® parameters</strong>, here‚Äôs the complete memory picture [ZeRO Paper, p.7-8]:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Component</th>
<th>Precision</th>
<th>Memory (bytes)</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Parameters</td>
<td>fp16</td>
<td>2Œ®</td>
<td>Model weights for forward/backward</td>
</tr>
<tr class="even">
<td>Gradients</td>
<td>fp16</td>
<td>2Œ®</td>
<td>Computed during backward pass</td>
</tr>
<tr class="odd">
<td>Parameters (copy)</td>
<td>fp32</td>
<td>4Œ®</td>
<td>Master copy for stable updates</td>
</tr>
<tr class="even">
<td>Momentum</td>
<td>fp32</td>
<td>4Œ®</td>
<td>First moment estimate (Adam)</td>
</tr>
<tr class="odd">
<td>Variance</td>
<td>fp32</td>
<td>4Œ®</td>
<td>Second moment estimate (Adam)</td>
</tr>
<tr class="even">
<td><strong>TOTAL</strong></td>
<td>-</td>
<td><strong>16Œ®</strong></td>
<td>-</td>
</tr>
</tbody>
</table>
<p><strong>Memory multiplier K = 12</strong> (optimizer states alone)</p>
<p><strong>Why fp32 for optimizer states?</strong> The updates computed by Adam are often very small. In fp16, these tiny values can underflow to zero, causing training to stagnate. The fp32 master copy ensures these small but crucial updates are preserved [ZeRO Paper, p.7]. In this experiment, we have used a 2.3B parameter model to explain ZeRO . However, we have also discussed about bigger size model.</p>
</section>
<section id="concrete-example-our-2.3b-parameter-model" class="level4">
<h4 class="anchored" data-anchor-id="concrete-example-our-2.3b-parameter-model">2.1.3 <strong>Concrete Example: Our 2.3B Parameter Model</strong></h4>
<p>Let‚Äôs calculate the memory requirements for our experimental model with 2,289,050,000 parameters:</p>
<pre><code>Œ® = 2.289 billion parameters

Parameters (fp16):    2 √ó 2.289B = 4.578 GB ‚Üí 2,289.05 MB √ó 2
Gradients (fp16):     2 √ó 2.289B = 4.578 GB ‚Üí 2,289.05 MB √ó 2
Optimizer States:     12 √ó 2.289B = 27.468 GB ‚Üí 2,289.05 MB √ó 12
-----------------------------------------------------------
Model States Total:   16 √ó 2.289B = 36.624 GB</code></pre>
<p>This matches our experimental observations! From the output logs, after the warmup step:</p>
<pre><code>GPU 0 - Initial state:
  Model parameters: 2289.05 MB
  Gradients: 2289.05 MB
  Optimizer states: 2289.05 MB
  Total allocated: 6944.14 MB</code></pre>
<p>Wait‚Äîthe optimizer states show only 2,289 MB, should‚Äôt it be 2 X 2,289 MB where one copy for momemtum and one for varience (assuming fp16 precesion). However, its ZeRO stage 1 that splits the optimizer stage in 2 GPUs in our experiment. More on this in Section 3.</p>
</section>
</section>
<section id="residual-states-the-secondary-memory-consumers" class="level3">
<h3 class="anchored" data-anchor-id="residual-states-the-secondary-memory-consumers">2.2 Residual States: The Secondary Memory Consumers</h3>
<p>Beyond model states, several other factors consume significant memory during training [ZeRO Paper, p.8].</p>
<section id="activations-the-hidden-giant" class="level4">
<h4 class="anchored" data-anchor-id="activations-the-hidden-giant">2.2.1 <strong>Activations: The Hidden Giant</strong></h4>
<p>Activations are intermediate outputs from each layer, stored during the forward pass and needed again during backpropagation to compute gradients. For transformer models, activation memory scales as:</p>
<pre><code>Activation Memory ‚àù num_layers √ó hidden_dim √ó sequence_length √ó batch_size</code></pre>
<p>[ZeRO Paper, p.8, footnote 3]</p>
<p><strong>Example from the paper:</strong> A 1.5B parameter GPT-2 model with:</p>
<ul>
<li>Sequence length: 1,024</li>
<li>Batch size: 32</li>
<li>Requires: <strong>~60 GB</strong> of activation memory [ZeRO Paper, p.8]</li>
</ul>
<p>This is <strong>2√ó the entire model states memory!</strong></p>
<p><strong>Activation Checkpointing to the Rescue:</strong></p>
<p>Instead of storing all activations, we can use <strong>gradient checkpointing</strong> [ZeRO Paper, p.3]:</p>
<ul>
<li>Store only selected checkpoint activations (typically one per transformer layer)</li>
<li>Recompute the others during backward pass</li>
<li>Memory reduction: ~‚àöN where N is the number of layers</li>
<li>Cost: 33% extra computation [ZeRO Paper, p.3]</li>
</ul>
<p>For our GPT-2 example, this reduces activation memory from 60GB to <strong>~8GB</strong> [ZeRO Paper, p.8].</p>
<p><strong>Our Experimental Setup:</strong></p>
<p>Looking at our zero1.py implementation:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># From zero1.py, lines 92-96</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(batch_size, <span class="dv">10000</span>, device<span class="op">=</span>device)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.randn(batch_size, <span class="dv">10000</span>, device<span class="op">=</span>device)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>With a 6-layer linear network of dimension 10,000, let‚Äôs calculate activation memory per layer:</p>
<p><strong>Activation size per layer:</strong></p>
<pre><code>batch_size √ó hidden_dim √ó bytes_per_element
= 16 √ó 10,000 √ó 2 bytes (fp16)
= 320,000 bytes
= 0.32 MB per activation</code></pre>
<p><strong>For 6 layers with checkpointing:</strong></p>
<pre><code>6 layers √ó 0.32 MB = 1.92 MB (checkpointed activations)</code></pre>
<p>This is tiny compared to model states! Our simple fully-connected architecture has minimal activation overhead. In contrast, transformers have much larger activations due to attention mechanisms storing query-key-value matrices for every token pair, which is why the GPT-2 example above requires 60GB before checkpointing.</p>
</section>
<section id="temporary-buffers-communication-overhead" class="level4">
<h4 class="anchored" data-anchor-id="temporary-buffers-communication-overhead">2.2.2 <strong>Temporary Buffers: Communication Overhead</strong></h4>
<p>During distributed training, operations like gradient all-reduce create temporary buffers to improve communication efficiency. The ZeRO paper notes [ZeRO Paper, p.8]:</p>
<blockquote class="blockquote">
<p>‚ÄúOperations such as gradient all-reduce, or gradient norm computation tend to fuse all the gradients into a single flattened buffer before applying the operation in an effort to improve throughput.‚Äù</p>
</blockquote>
<p>For our 2.3B parameter model: - fp32 buffer for all gradients: 2.289B √ó 4 bytes = <strong>9.156 GB</strong></p>
<p>These buffers are temporary but their peak usage contributes to memory pressure.</p>
</section>
<section id="memory-fragmentation-the-silent-killer" class="level4">
<h4 class="anchored" data-anchor-id="memory-fragmentation-the-silent-killer">2.2.3 <strong>Memory Fragmentation: The Silent Killer</strong></h4>
<p>Memory fragmentation occurs due to the interleaving of short-lived and long-lived tensors [ZeRO Paper, p.12-13]:</p>
<p><strong>During Forward Pass:</strong></p>
<ul>
<li>‚úÖ Long-lived: Activation checkpoints (kept for backward)</li>
<li>‚ùå Short-lived: Non-checkpoint activations (discarded immediately)</li>
</ul>
<p><strong>During Backward Pass:</strong></p>
<ul>
<li>‚úÖ Long-lived: Parameter gradients (kept for optimizer step)</li>
<li>‚ùå Short-lived: Activation gradients (discarded after use)</li>
</ul>
<p>This interleaving creates memory ‚Äúholes‚Äù that can‚Äôt be used for large allocations. The ZeRO paper observes [ZeRO Paper, p.8]:</p>
<blockquote class="blockquote">
<p>‚ÄúWe observe significant memory fragmentation when training very large models, resulting in out of memory issue with over 30% of memory still available in some extreme cases.‚Äù</p>
</blockquote>
<p><strong>ZeRO-R Solution:</strong> Pre-allocate contiguous buffers and copy tensors into them on-the-fly to prevent fragmentation [ZeRO Paper, p.13].</p>
</section>
</section>
<section id="total-memory-picture" class="level3">
<h3 class="anchored" data-anchor-id="total-memory-picture">2.3 Total Memory Picture</h3>
<p>Let‚Äôs put it all together for a realistic training scenario:</p>
<p><strong>Model:</strong> GPT-2 1.5B parameters <strong>Batch Size:</strong> 32 <strong>Sequence Length:</strong> 1,024 <strong>Activation Checkpointing:</strong> Enabled</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Component</th>
<th>Memory (GB)</th>
<th>Percentage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Model Parameters (fp16)</td>
<td>3.0</td>
<td>9.4%</td>
</tr>
<tr class="even">
<td>Gradients (fp16)</td>
<td>3.0</td>
<td>9.4%</td>
</tr>
<tr class="odd">
<td>Optimizer States (fp32)</td>
<td>18.0</td>
<td>56.2%</td>
</tr>
<tr class="even">
<td>Activation Checkpoints</td>
<td>8.0</td>
<td>25.0%</td>
</tr>
<tr class="odd">
<td><strong>TOTAL</strong></td>
<td><strong>32.0</strong></td>
<td><strong>100%</strong></td>
</tr>
</tbody>
</table>
<p>This barely fits on a single 32GB V100 GPU‚Äîand that‚Äôs with no room for temporary buffers or any memory fragmentation!</p>
</section>
<section id="our-experimental-setup-a-reproducible-testbed" class="level3">
<h3 class="anchored" data-anchor-id="our-experimental-setup-a-reproducible-testbed">2.4 Our Experimental Setup: A Reproducible Testbed</h3>
<p>For the experiments in this blog, we designed a setup that clearly demonstrates ZeRO‚Äôs impact while remaining reproducible:</p>
<p><strong>Model Architecture:</strong> 6-layer fully connected network</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>nn.Sequential(</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">10_000</span>, <span class="dv">10_000</span>),  <span class="co"># 100M parameters</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">10_000</span>, <span class="dv">10_000</span>),  <span class="co"># 100M parameters</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... (6 layers total)</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Total Parameters:</strong> 2.289 billion (2,289,050,000) <strong>Hardware:</strong> 2√ó NVIDIA GPUs <strong>Batch Size:</strong> 16 <strong>Optimizer:</strong> Adam (lr=0.001)</p>
<p><strong>Why this setup?</strong> 1. <strong>Large enough</strong> to show meaningful memory pressure (~36GB model states) 2. <strong>Simple architecture</strong> makes profiling analysis clear 3. <strong>Reproducible</strong> on commodity multi-GPU systems 4. <strong>Fast iterations</strong> for experimentation</p>
</section>
<section id="the-redundancy-problem-in-data-parallelism" class="level3">
<h3 class="anchored" data-anchor-id="the-redundancy-problem-in-data-parallelism">2.5 The Redundancy Problem in Data Parallelism</h3>
<p>Here‚Äôs the critical insight that motivates ZeRO: In standard data parallelism, <strong>every GPU maintains a complete copy of all model states</strong> [ZeRO Paper, p.2].</p>
<p>With 2 GPUs training our 2.3B parameter model using <strong>standard data parallelism</strong>, each GPU stores:</p>
<pre><code>Per GPU Memory Breakdown:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Parameters (fp16):        2Œ® = 2 √ó 2.289B √ó 2 bytes = 4.578 GB
Gradients (fp16):         2Œ® = 2 √ó 2.289B √ó 2 bytes = 4.578 GB
Optimizer States (fp32):
  - fp32 parameters:      4Œ® = 4 √ó 2.289B √ó 4 bytes = 9.156 GB
  - Momentum:             4Œ® = 4 √ó 2.289B √ó 4 bytes = 9.156 GB
  - Variance:             4Œ® = 4 √ó 2.289B √ó 4 bytes = 9.156 GB
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Total per GPU:           16Œ® = 36.624 GB</code></pre>
<p><strong>With 2 GPUs (Standard Data Parallelism):</strong></p>
<pre><code>GPU 0: 36.6 GB (complete copy of everything)
GPU 1: 36.6 GB (complete copy of everything)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Total Cluster Memory:     73.2 GB
Unique Information:       36.6 GB
Wasted (Redundancy):      36.6 GB (50%)
</code></pre>
<p>This massive redundancy is the core problem ZeRO solves. Instead of replicating all model states, ZeRO partitions them across GPUs while maintaining computational efficiency.</p>
<p>Now that we understand where memory goes and why we run out, we‚Äôre ready to see how ZeRO addresses each component systematically.</p>
<hr>
</section>
</section>
<section id="zero-foundations-three-stages-of-optimization" class="level2">
<h2 class="anchored" data-anchor-id="zero-foundations-three-stages-of-optimization">3. ZeRO Foundations: Three Stages of Optimization</h2>
<p>Now that we understand the memory problem, let‚Äôs see how ZeRO solves it. ZeRO‚Äôs approach is elegantly simple: <strong>partition model states across data-parallel processes instead of replicating them</strong> [ZeRO Paper, p.2]. But it does this progressively through three optimization stages, each building on the previous one.</p>
<section id="mathematical-framework-memory-savings" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-framework-memory-savings">3.1 Mathematical Framework: Memory Savings</h3>
<p>Before diving into implementation details, let‚Äôs understand the theoretical memory savings. The ZeRO paper provides clear formulas for each stage [ZeRO Paper, p.3, Figure 1]:</p>
<p><strong>Notation:</strong></p>
<ul>
<li>Œ® = Number of model parameters</li>
<li>K = Memory multiplier for optimizer states (K=12 for mixed-precision Adam)</li>
<li>Nd = Data parallelism degree (number of GPUs)</li>
</ul>
<p><strong>Memory Consumption Per GPU:</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Stage</th>
<th>Memory Formula</th>
<th>Reduction Factor</th>
<th>Example (Œ®=7.5B, Nd=64)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Baseline DP</strong></td>
<td>(2+2+K)Œ® = 16Œ®</td>
<td>1√ó</td>
<td>120 GB</td>
</tr>
<tr class="even">
<td><strong>ZeRO-1 (P_os)</strong></td>
<td>4Œ® + KŒ®/Nd</td>
<td>4√ó (as Nd‚Üí‚àû)</td>
<td>31.4 GB</td>
</tr>
<tr class="odd">
<td><strong>ZeRO-2 (P_os+g)</strong></td>
<td>2Œ® + (K+2)Œ®/Nd</td>
<td>8√ó (as Nd‚Üí‚àû)</td>
<td>16.6 GB</td>
</tr>
<tr class="even">
<td><strong>ZeRO-3 (P_os+g+p)</strong></td>
<td>(2+2+K)Œ®/Nd</td>
<td>Nd√ó</td>
<td>1.9 GB</td>
</tr>
</tbody>
</table>
<p>[ZeRO Paper, p.3, Figure 1]</p>
</section>
<section id="visual-understanding-memory-consumption-across-stages" class="level3">
<h3 class="anchored" data-anchor-id="visual-understanding-memory-consumption-across-stages">3.2 Visual Understanding: Memory Consumption Across Stages</h3>
<p>The figure from the ZeRO paper (Figure 1, p.3) beautifully illustrates how each stage progressively reduces memory:</p>
<img src="assets/zero_mem.png" alt="ZeRO-Memory" width="720">
<p align="center">
<em>Comparing the per-device memory consumption of model states, with three stages of ZeRO-DP optimizations. Ref: ZeRO paper</em>
</p>
<p>Each stage removes redundancy from one component while keeping the computation pattern efficient.</p>
<hr>
</section>
<section id="zero-1-optimizer-state-partitioning-p_os" class="level3">
<h3 class="anchored" data-anchor-id="zero-1-optimizer-state-partitioning-p_os">3.3 ZeRO-1: Optimizer State Partitioning (P_os)</h3>
<p><strong>Core Idea:</strong> Each GPU only stores and updates optimizer states for a subset of parameters [ZeRO Paper, p.10].</p>
<section id="how-it-works" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works">3.3.1 <strong>How It Works</strong></h4>
<ol type="1">
<li><strong>Partition Assignment:</strong> Divide all parameters into Nd equal partitions</li>
<li><strong>Local Ownership:</strong> GPU i only maintains optimizer states for partition i</li>
<li><strong>Training Step:</strong>
<ul>
<li>All-reduce gradients (same as baseline DP)</li>
<li>Each GPU updates only its partition</li>
<li>Broadcast updated parameters from each GPU to all others</li>
</ul></li>
</ol>
<p><strong>Memory Savings:</strong> 4Œ® + KŒ®/Nd ‚âà 4Œ® bytes (when Nd is large) - Optimizer states reduced from 12Œ® to 12Œ®/Nd - Parameters and gradients still replicated</p>
</section>
<section id="communication-pattern" class="level4">
<h4 class="anchored" data-anchor-id="communication-pattern">3.3.2 <strong>Communication Pattern</strong></h4>
<pre><code>Step 1: All-Reduce Gradients (same as baseline)
  GPU 0: [g0, g1, g2, ...] ‚Üí all-reduce ‚Üí [·∏°0, ·∏°1, ·∏°2, ...]
  GPU 1: [g0, g1, g2, ...] ‚Üí all-reduce ‚Üí [·∏°0, ·∏°1, ·∏°2, ...]

Step 2: Local Optimizer Update
  GPU 0: Updates params [p0, p1]     (owns partition 0)
  GPU 1: Updates params [p2, p3]     (owns partition 1)

Step 3: Broadcast Parameters
  GPU 0 ‚Üí broadcast [p0, p1] ‚Üí GPU 1
  GPU 1 ‚Üí broadcast [p2, p3] ‚Üí GPU 0</code></pre>
<p><strong>Communication Volume:</strong> 2Œ® (same as baseline DP) [ZeRO Paper, p.13-14]</p>
</section>
<section id="our-experimental-results-zero-1" class="level4">
<h4 class="anchored" data-anchor-id="our-experimental-results-zero-1">3.3.3 <strong>Our Experimental Results: ZeRO-1</strong></h4>
<p>Let‚Äôs see how this plays out with our 2.3B parameter model on 2 GPUs:</p>
<p><strong>From output_log.txt:</strong></p>
<pre><code>=== Regular Adam (Baseline) ===
Step 0 memory:
Before backward: 6947.19 MB
Gradient memory after backward: 2289.05 MB
Peak memory this step: 11528.60 MB

Final peak memory: 11528.60 MB

=== ZeRO-1 (Sharded Optimizer States) ===
GPU 0 - Initial state:
  Model parameters: 2289.05 MB
  Gradients: 2289.05 MB
  Optimizer states: 2289.05 MB  ‚Üê Half of baseline (sharded!)
  Total allocated: 6944.14 MB
  Max allocated: 8090.25 MB

Step 0 memory:
Before backward: 5801.07 MB  ‚Üê 1,146 MB less than baseline!
Gradient memory after backward: 2289.05 MB
Peak memory this step: 8090.25 MB

Final peak memory: 8090.25 MB

Memory Usage Summary:
Peak memory with regular Adam: 11528.60 MB
Peak memory with ZeRO-1: 8090.25 MB
Memory reduction: 3438.35 MB (29.82%)</code></pre>
<p><strong>Analysis:</strong></p>
<p>‚úÖ <strong>Memory Reduction Achieved:</strong> 29.82% (3.44 GB saved) ‚úÖ <strong>Optimizer States Sharded:</strong> 2,289 MB per GPU (half the expected 4,578 MB) ‚úÖ <strong>Communication Overhead:</strong> 0.0% (excellent!)</p>
<p><strong>Why only 29.82% and not 37.5%?</strong> The ZeRO paper predicts ~37.5% reduction with 8 GPUs. With only 2 GPUs:</p>
<pre><code>Theoretical: (4Œ® + KŒ®/2) / 16Œ® = (4 + 6)/16 = 62.5% of baseline ‚Üí 37.5% reduction
Observed: 29.82% reduction</code></pre>
<p>The difference comes from activation memory and other overheads not included in the theoretical model states calculation.</p>
<hr>
</section>
</section>
<section id="zero-2-gradient-partitioning-p_osg" class="level3">
<h3 class="anchored" data-anchor-id="zero-2-gradient-partitioning-p_osg">3.4 ZeRO-2: Gradient Partitioning (P_os+g)</h3>
<p><strong>Core Idea:</strong> Each GPU only stores gradients for parameters it owns, discarding the rest [ZeRO Paper, p.10].</p>
<section id="how-it-works-1" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-1">3.4.1 <strong>How It Works</strong></h4>
<p>Building on ZeRO-1, we add gradient sharding:</p>
<ol type="1">
<li><strong>Gradient Hooks:</strong> Register backward hooks on all parameters
<ul>
<li>Local parameters: Keep gradient</li>
<li>Non-local parameters: Discard gradient (return None)</li>
</ul></li>
<li><strong>Reduce-Scatter:</strong> Instead of all-reduce, use reduce-scatter
<ul>
<li>Reduces communication into chunks</li>
<li>Each GPU receives only the gradient chunk it needs</li>
</ul></li>
<li><strong>Memory Release:</strong> Non-local gradients never stored ‚Üí 1/Nd memory</li>
</ol>
<p><strong>Memory Savings:</strong> 2Œ® + (K+2)Œ®/Nd ‚âà 2Œ® bytes (when Nd is large) - Optimizer states: 12Œ®/Nd (same as ZeRO-1) - Gradients: 2Œ®/Nd (NEW!) - Parameters: 2Œ® (still replicated)</p>
</section>
<section id="the-reduce-scatter-operation" class="level4">
<h4 class="anchored" data-anchor-id="the-reduce-scatter-operation">3.4.2 <strong>The Reduce-Scatter Operation</strong></h4>
<pre><code>All-Reduce (baseline):
  Each GPU sends: full gradient (Œ® elements)
  Each GPU receives: full gradient (Œ® elements)
  Volume: 2Œ® per GPU

Reduce-Scatter (ZeRO-2):
  Each GPU sends: full gradient (Œ® elements)
  Each GPU receives: 1/Nd chunk (Œ®/Nd elements)
  Volume: Œ® per GPU</code></pre>
<p><strong>Why Reduce-Scatter?</strong> It combines reduction and distribution in one operation, saving both time and memory [ZeRO Paper, p.10].</p>
</section>
<section id="implementation-detail-gradient-hooks" class="level4">
<h4 class="anchored" data-anchor-id="implementation-detail-gradient-hooks">3.4.3 <strong>Implementation Detail: Gradient Hooks</strong></h4>
<p>From our zero2.py (lines 73-84):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> register_gradient_hooks(<span class="va">self</span>):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> param <span class="kw">in</span> <span class="va">self</span>.local_params:</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Keep gradients for parameters we own</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>            hook <span class="op">=</span> <span class="kw">lambda</span> grad: grad</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Discard gradients for non-local parameters</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>            hook <span class="op">=</span> <span class="kw">lambda</span> grad: <span class="va">None</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        handle <span class="op">=</span> param.register_hook(hook)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.grad_hooks[param] <span class="op">=</span> handle</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This elegant mechanism ensures gradients are automatically discarded during backward pass, preventing unnecessary memory allocation.</p>
</section>
<section id="our-experimental-results-zero-2" class="level4">
<h4 class="anchored" data-anchor-id="our-experimental-results-zero-2">3.4.4 <strong>Our Experimental Results: ZeRO-2</strong></h4>
<p><strong>From output_log.txt:</strong></p>
<pre><code>=== Regular Adam (Baseline) ===
Peak memory: 11528.60 MB

=== ZeRO-2 (Sharded Optimizer + Gradients) ===
GPU 0 - Initial state:
  Model parameters: 2289.05 MB
  Gradients: 1144.52 MB  ‚Üê HALF of baseline (sharded!)
  Optimizer states: 2289.05 MB  ‚Üê Half (same as ZeRO-1)
  Total allocated: 5797.49 MB
  Max allocated: 6943.23 MB

Step 0 memory:
Before backward: 4654.43 MB  ‚Üê Even lower than ZeRO-1!
Gradient memory after backward: 2289.05 MB
Peak memory this step: 8470.02 MB

Final peak memory: 8470.02 MB

Timing and Communication Stats:
Average step time: 0.029s
Average communication time: 0.014s  ‚Üê Non-zero now
Average compute time: 0.015s
Communication overhead: 48.6%  ‚Üê Trade-off for memory

Memory Usage Summary:
Peak memory with regular Adam: 11528.60 MB
Peak memory with sharded Adam: 8470.02 MB
Memory reduction: 3058.58 MB (26.53%)</code></pre>
<p><strong>Analysis:</strong></p>
<p>‚úÖ <strong>Memory Reduction Achieved:</strong> 26.53% (3.06 GB saved) ‚úÖ <strong>Gradient Sharding Working:</strong> 1,144 MB per GPU (half the expected 2,289 MB) ‚úÖ <strong>Optimizer States Sharded:</strong> 2,289 MB per GPU (same as ZeRO-1) ‚ö†Ô∏è <strong>Communication Overhead:</strong> 48.6% (significant trade-off)</p>
<p><strong>Why 26.53% and not more?</strong> Let‚Äôs compare theoretical vs observed with 2 GPUs (Nd=2):</p>
<pre><code>Theoretical Calculation (Œ® = 2.289B, Nd = 2):
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Memory Formula: 2Œ® + (K+2)Œ®/Nd = 2Œ® + 14Œ®/2 = 2Œ® + 7Œ® = 9Œ®

Expected: 9 √ó 2.289B √ó 1 byte = 20.6 GB
Baseline: 16 √ó 2.289B √ó 1 byte = 36.6 GB
Theoretical reduction: (36.6 - 20.6) / 36.6 = 43.7%

Observed: 26.53% reduction
Difference: 43.7% - 26.53% = 17.2% gap</code></pre>
<p><strong>Why the 17.2% gap?</strong> Similar to ZeRO-1, but with additional factors:</p>
<ol type="1">
<li><strong>Activation memory</strong> (~1.92 MB, negligible but present)</li>
<li><strong>Temporary buffers</strong> for reduce-scatter (more significant than ZeRO-1!)</li>
<li><strong>Reduce-scatter buffers</strong> create larger temporary allocations</li>
<li><strong>Peak measurement captures worst case</strong> during gradient communication</li>
</ol>
<p><strong>The Peak Memory Story:</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Stage</th>
<th>Before Backward</th>
<th>Peak Memory</th>
<th>Theoretical</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Baseline</strong></td>
<td>6,947 MB</td>
<td>11,529 MB</td>
<td>36,600 MB</td>
<td>Model states only</td>
</tr>
<tr class="even">
<td><strong>ZeRO-1</strong></td>
<td>5,801 MB</td>
<td>8,090 MB</td>
<td>27,450 MB</td>
<td>4Œ® + KŒ®/2</td>
</tr>
<tr class="odd">
<td><strong>ZeRO-2</strong></td>
<td>4,654 MB ‚úÖ</td>
<td>8,470 MB ‚ö†Ô∏è</td>
<td>20,601 MB</td>
<td>2Œ® + 14Œ®/2</td>
</tr>
</tbody>
</table>
<p><strong>Key Observations:</strong> - ‚úÖ <strong>Before Backward is Better</strong>: 4,654 MB vs 5,801 MB (ZeRO-1) - ‚ö†Ô∏è <strong>Peak Memory is Worse</strong>: 8,470 MB vs 8,090 MB (ZeRO-1) - Due to reduce-scatter buffers</p>
<p><strong>Explanation:</strong></p>
<ul>
<li><strong>Initial state is better</strong> (5,797 MB vs 6,944 MB for ZeRO-1)</li>
<li><strong>Peak during backward is worse</strong> (8,470 MB vs 8,090 MB)</li>
<li>The reduce-scatter operation creates <strong>large temporary buffers</strong> during gradient communication</li>
<li>These buffers must hold full gradients before distribution, causing memory spikes</li>
<li>The theoretical model only counts persistent state, not temporary communication buffers</li>
</ul>
<p><strong>Why the communication overhead?</strong></p>
<ul>
<li>Reduce-scatter requires coordination across all GPUs</li>
<li>With only 2 GPUs and small batch size, communication time (0.014s) rivals compute (0.015s)</li>
<li>The 48.6% overhead would decrease significantly with more GPUs and larger batches</li>
</ul>
</section>
<section id="when-zero-2-shines" class="level4">
<h4 class="anchored" data-anchor-id="when-zero-2-shines">3.4.5 <strong>When ZeRO-2 Shines</strong></h4>
<p>ZeRO-2 becomes more beneficial as: 1. <strong>Number of GPUs increases</strong> (Nd &gt; 8): Gradient memory savings scale with Nd 2. <strong>Model size grows</strong> relative to batch size 3. <strong>Intra-node communication</strong> is available (reduce-scatter benefits from high bandwidth)</p>
<p>For our 2-GPU setup, the communication overhead dominates, but with 8+ GPUs, the memory savings would be more pronounced.</p>
<hr>
</section>
</section>
<section id="zero-3-parameter-partitioning-p_osgp" class="level3">
<h3 class="anchored" data-anchor-id="zero-3-parameter-partitioning-p_osgp">3.5 ZeRO-3: Parameter Partitioning (P_os+g+p)</h3>
<p><strong>Core Idea:</strong> Partition parameters themselves and materialize them on-demand during forward/backward passes [ZeRO Paper, p.11].</p>
<section id="how-it-works-2" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-2">3.5.1 <strong>How It Works</strong></h4>
<p>This is the most aggressive optimization:</p>
<ol type="1">
<li><p><strong>Parameter Sharding:</strong> Each GPU stores only 1/Nd of the model parameters</p></li>
<li><p><strong>On-Demand Materialization:</strong></p>
<ul>
<li>Before forward pass of layer i: All-gather parameters for layer i</li>
<li>Compute forward pass</li>
<li>Release parameters (keep only local shard)</li>
<li>Repeat for backward pass</li>
</ul></li>
<li><p><strong>Lifecycle Management:</strong> Parameters exist in full form only during their layer‚Äôs computation</p></li>
</ol>
<p><strong>Memory Savings:</strong> (2+2+K)Œ®/Nd = 16Œ®/Nd bytes</p>
<ul>
<li>Everything divided by Nd!</li>
<li>With 64 GPUs: 64√ó memory reduction</li>
</ul>
</section>
<section id="parameter-lifecycle" class="level4">
<h4 class="anchored" data-anchor-id="parameter-lifecycle">3.5.2 <strong>Parameter Lifecycle</strong></h4>
<pre><code>Before Layer Computation:
  GPU 0: [p0_shard]           GPU 1: [p1_shard]
         ‚Üì all-gather                ‚Üì all-gather
  GPU 0: [p0_full, p1_full]   GPU 1: [p0_full, p1_full]

During Computation:
  Both GPUs: Compute with full parameters

After Layer Computation:
  GPU 0: [p0_full, p1_full]   GPU 1: [p0_full, p1_full]
         ‚Üì release                   ‚Üì release
  GPU 0: [p0_shard]           GPU 1: [p1_shard]</code></pre>
</section>
<section id="implementation-zero3parammanager" class="level4">
<h4 class="anchored" data-anchor-id="implementation-zero3parammanager">3.5.3 <strong>Implementation: Zero3ParamManager</strong></h4>
<p>From our zero3.py (lines 23-51):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Zero3ParamManager:</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, param, shard_idx, world_size, shard_dim<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.param <span class="op">=</span> param</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.shard_idx <span class="op">=</span> shard_idx</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.world_size <span class="op">=</span> world_size</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.shard_dim <span class="op">=</span> shard_dim</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.full_data <span class="op">=</span> <span class="va">None</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> materialize(<span class="va">self</span>):</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Gather full parameter from all shards"""</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>        local_shard <span class="op">=</span> <span class="va">self</span>.param.data.contiguous()</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        global_shards <span class="op">=</span> [torch.empty_like(local_shard)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>                         <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.world_size)]</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>        dist.all_gather(global_shards, local_shard)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.full_data <span class="op">=</span> torch.cat(global_shards, dim<span class="op">=</span><span class="va">self</span>.shard_dim)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.param.data <span class="op">=</span> <span class="va">self</span>.full_data</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> release(<span class="va">self</span>):</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Keep only local shard"""</span></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>        shards <span class="op">=</span> <span class="va">self</span>.param.data.chunk(<span class="va">self</span>.world_size, dim<span class="op">=</span><span class="va">self</span>.shard_dim)</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>        local_shard <span class="op">=</span> shards[<span class="va">self</span>.shard_idx].contiguous()</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.param.data <span class="op">=</span> local_shard</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.full_data <span class="op">=</span> <span class="va">None</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The parameter manager controls the materialize/release cycle automatically through forward/backward hooks.</p>
</section>
<section id="hook-registration" class="level4">
<h4 class="anchored" data-anchor-id="hook-registration">3.5.4 <strong>Hook Registration</strong></h4>
<p>From zero3.py (lines 54-75):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> register_zero3_hooks(model, param_managers):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> pre_hook(module, inputs):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Materialize parameters before computation"""</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _, param <span class="kw">in</span> module.named_parameters(recurse<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> param <span class="kw">in</span> param_managers:</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>                param_managers[param].materialize()</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> post_hook(module, inputs, outputs):</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Release parameters after computation"""</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _, param <span class="kw">in</span> module.named_parameters(recurse<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> param <span class="kw">in</span> param_managers:</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>                param_managers[param].release()</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Register on all modules</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> m <span class="kw">in</span> model.modules():</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        m.register_forward_pre_hook(pre_hook)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        m.register_forward_hook(post_hook)</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>        m.register_full_backward_pre_hook(pre_hook)</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>        m.register_full_backward_hook(post_hook)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Elegance:</strong> PyTorch‚Äôs hook system handles the complexity automatically. Parameters are gathered right before needed and released immediately after.</p>
</section>
<section id="our-experimental-results-zero-3" class="level4">
<h4 class="anchored" data-anchor-id="our-experimental-results-zero-3">3.5.5 <strong>Our Experimental Results: ZeRO-3</strong></h4>
<p><strong>From output_log.txt:</strong></p>
<pre><code>=== Regular Adam (Baseline) ===
Peak memory: 11528.60 MB

=== ZeRO-3 (Sharded Everything!) ===
GPU 0 - Initial state:
  Model parameters: 1144.52 MB  ‚Üê HALF! (sharded)
  Gradients: 0.00 MB  ‚Üê Not yet computed
  Optimizer states: 0.00 MB  ‚Üê Empty initially
  Total allocated: 2359.67 MB  ‚Üê Dramatically lower!
  Max allocated: 5033.95 MB

Step 0 memory:
Before backward: 2362.73 MB  ‚Üê Lowest of all!
Gradient memory after backward: 1335.28 MB
Peak memory this step: 5033.95 MB  ‚Üê Best peak memory!

Final peak memory: 5033.95 MB

Timing and Communication Stats:
Average step time: 0.005s
Average communication time: 0.005s  ‚Üê Almost all comm!
Average compute time: 0.000s
Communication overhead: 97.0%  ‚Üê Extreme trade-off

Memory Usage Summary:
Peak memory with regular Adam: 11528.60 MB
Peak memory with ZeRO-3: 5033.95 MB
Memory reduction: 6494.65 MB (56.34%!!!)</code></pre>
<p><strong>Analysis:</strong></p>
<p>‚úÖ <strong>Memory Reduction Achieved:</strong> 56.34% (6.49 GB saved!!!) ‚úÖ <strong>Parameters Sharded:</strong> 1,144 MB per GPU (half the expected 2,289 MB) ‚úÖ <strong>Optimizer States Sharded:</strong> 0 MB initially (will be created as shards) ‚úÖ <strong>Gradients Sharded:</strong> Remain sharded throughout ‚ö†Ô∏è <strong>Communication Overhead:</strong> 97.0% (extreme trade-off)</p>
<p><strong>Why 56.34%?</strong> Let‚Äôs compare theoretical vs observed with 2 GPUs (Nd=2):</p>
<pre><code>Theoretical Calculation (Œ® = 2.289B, Nd = 2):
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Memory Formula: (2+2+K)Œ®/Nd = 16Œ®/2 = 8Œ®

Expected: 8 √ó 2.289B √ó 1 byte = 18.3 GB
Baseline: 16 √ó 2.289B √ó 1 byte = 36.6 GB
Theoretical reduction: (36.6 - 18.3) / 36.6 = 50.0%

Observed: 56.34% reduction (even better!)
Difference: 56.34% - 50.0% = +6.34% bonus!</code></pre>
<p><strong>Why do we get BETTER than theoretical?</strong> This is the ZeRO-3 magic:</p>
<ol type="1">
<li><strong>Theoretical assumes all parameters in memory at once</strong>: The formula 8Œ® assumes all sharded states are held simultaneously</li>
<li><strong>Reality: Parameters exist only temporarily</strong>: ZeRO-3 materializes parameters one layer at a time</li>
<li><strong>Peak happens during single layer computation</strong>: Not all 8Œ® is needed at peak</li>
<li><strong>On-demand materialization wins</strong>: Only ~1-2 layers worth of parameters exist in full form at any moment</li>
</ol>
<p><strong>Detailed breakdown:</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Memory State</th>
<th>Memory Usage</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>At rest (between steps)</strong></td>
<td>2.36 GB</td>
<td>Only shards stored</td>
</tr>
<tr class="even">
<td><strong>During layer computation</strong></td>
<td>5.03 GB</td>
<td>One layer materialized</td>
</tr>
<tr class="odd">
<td><strong>Theoretical (all shards)</strong></td>
<td>18.3 GB</td>
<td>If we held everything</td>
</tr>
<tr class="even">
<td><strong>Actual peak</strong></td>
<td>5.03 GB</td>
<td><strong>3.6√ó better than theoretical!</strong></td>
</tr>
</tbody>
</table>
<p><strong>Why 56.34% - The Best Memory Savings?</strong></p>
<p><strong>The Complete Memory Story:</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Stage</th>
<th>Initial State</th>
<th>Peak Memory</th>
<th>Memory Reduction</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Baseline</strong></td>
<td>~7,000 MB</td>
<td>11,529 MB</td>
<td>-</td>
</tr>
<tr class="even">
<td><strong>ZeRO-1</strong></td>
<td>6,944 MB</td>
<td>8,090 MB</td>
<td>29.82%</td>
</tr>
<tr class="odd">
<td><strong>ZeRO-2</strong></td>
<td>5,797 MB</td>
<td>8,470 MB</td>
<td>26.53%</td>
</tr>
<tr class="even">
<td><strong>ZeRO-3</strong></td>
<td>2,360 MB ‚≠ê</td>
<td>5,034 MB ‚≠ê</td>
<td><strong>56.34%</strong></td>
</tr>
</tbody>
</table>
<p><strong>‚≠ê ZeRO-3 achieves dramatic improvement across both metrics!</strong></p>
<p><strong>What makes ZeRO-3 special?</strong> - <strong>Everything is sharded:</strong> Parameters, gradients, AND optimizer states divided by Nd - <strong>Initial state minimal:</strong> Only 2.36 GB (vs 6.94 GB baseline) - <strong>Peak during layer computation:</strong> 5.03 GB when parameters are temporarily materialized - <strong>No permanent full copies:</strong> Parameters gathered only when needed, then released</p>
<p><strong>Why 97% communication overhead?</strong></p>
<ul>
<li><strong>Per-layer all-gather:</strong> Each of 6 layers requires all-gather before forward/backward</li>
<li><strong>Small model + 2 GPUs:</strong> Communication dominates compute time</li>
<li>Step time: 0.005s (communication: 0.005s, compute: ~0.000s)</li>
<li>With our setup, we‚Äôre almost entirely <strong>communication-bound</strong></li>
</ul>
<p><strong>This overhead is expected and acceptable:</strong></p>
<ul>
<li>For models too large to fit in memory, <strong>97% overhead is better than 0% success rate</strong></li>
<li>With 100B+ parameter models and 64+ GPUs, compute time increases dramatically</li>
<li>The paper shows [ZeRO Paper, p.17] that with large models, efficiency reaches 30+ TFlops/GPU</li>
</ul>
<hr>
</section>
</section>
</section>
<section id="profiler-deep-dive-understanding-zero-through-execution-traces" class="level2">
<h2 class="anchored" data-anchor-id="profiler-deep-dive-understanding-zero-through-execution-traces">4. Profiler Deep Dive: Understanding ZeRO Through Execution Traces</h2>
<p>Having understood the theory and experimental results of each ZeRO stage, let‚Äôs now dive deep into the profiler traces to understand <strong>how</strong> these optimizations manifest at the execution level. We‚Äôll use TensorBoard‚Äôs PyTorch Profiler to examine operator-level behavior, kernel execution patterns, and memory timelines.</p>
<section id="zero-1-profiler-analysis-baseline-vs-optimizer-sharding" class="level3">
<h3 class="anchored" data-anchor-id="zero-1-profiler-analysis-baseline-vs-optimizer-sharding">4.1 ZeRO-1 Profiler Analysis: Baseline vs Optimizer Sharding</h3>
<section id="overview" class="level4">
<h4 class="anchored" data-anchor-id="overview">4.1.1 <strong>Overview</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/overview_zero1.png" class="img-fluid figure-img"></p>
<figcaption>ZeRO-1 Overview</figcaption>
</figure>
</div>
<p align="center">
<em>ZeRO-1 Overview</em>
</p>
<p>The overview shows:</p>
<ul>
<li><strong>GPU utilization</strong>: 95.91% that is similiar to regular adam (while optimizer not sharded)</li>
<li><strong>Kernel execution time</strong>: Similar between baseline and ZeRO-1</li>
<li><strong>Communication overhead</strong>: Minimal (0.0% added over baseline)</li>
</ul>
</section>
<section id="operator-breakdown" class="level4">
<h4 class="anchored" data-anchor-id="operator-breakdown">4.1.2 <strong>Operator Breakdown</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/operator_regular_adam.png" class="img-fluid figure-img"></p>
<figcaption>Regular Adam Operators</figcaption>
</figure>
</div>
<p align="center">
<em>Regular Adam Operators</em>
</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/operator_zero1_ops.png" class="img-fluid figure-img"></p>
<figcaption>ZeRO-1 Operators</figcaption>
</figure>
</div>
<p align="center">
<em>ZeRO-1 Operators</em>
</p>
<p>Key differences:</p>
<ul>
<li><strong>All-reduce operations</strong>: In ZeRO-1, we can see All-reduce (gradient averaging) while not in the baseline</li>
<li><strong>Broadcast operations</strong>: Appear in ZeRO-1 (parameter synchronization after update)</li>
<li><strong>Optimizer step</strong>: Faster in ZeRO-1 (fewer states to update)</li>
</ul>
</section>
<section id="kernel-execution" class="level4">
<h4 class="anchored" data-anchor-id="kernel-execution">4.1.3 <strong>Kernel Execution</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/kernel_regular_adam.png" class="img-fluid figure-img"></p>
<figcaption>Regular Adam Kernels Profiler</figcaption>
</figure>
</div>
<p align="center">
<em>Regular Adam Kernels</em>
</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/kernel_zero1.png" class="img-fluid figure-img"></p>
<figcaption>ZeRO-1 Kernels Profiler</figcaption>
</figure>
</div>
<p align="center">
<em>ZeRO-1 Kernels</em>
</p>
<ul>
<li><strong>Compute kernels</strong>: Nearly identical execution patterns</li>
<li><strong>Memory kernels</strong>: ZeRO-1 shows lower memory allocations</li>
<li><strong>Communication kernels</strong>: Similar bandwidth utilization except for All-reduce and broadcast</li>
</ul>
</section>
<section id="peak-memory-timeline" class="level4">
<h4 class="anchored" data-anchor-id="peak-memory-timeline">4.1.4 <strong>Peak Memory Timeline</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/peak_mem_regular_adam.png" class="img-fluid figure-img"></p>
<figcaption>Regular Adam Peak Memory Profiler</figcaption>
</figure>
</div>
<p align="center">
<em>Regular Adam Peak Memory: 11528.6 MB - Memory spikes during optimizer.step(), large plateau during training</em>
</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/peak_memory_zero1.png" class="img-fluid figure-img"></p>
<figcaption>ZeRO-1 Peak Memory Profiler</figcaption>
</figure>
</div>
<p align="center">
<em>ZeRO-1 Peak Memory: 8090.3 MB - Flatter memory profile, sharded states prevent spikes, lower baseline throughout training</em>
</p>
</section>
<section id="memory-operator-view" class="level4">
<h4 class="anchored" data-anchor-id="memory-operator-view">4.1.5 <strong>Memory Operator View</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/operator_mem_adam.png" class="img-fluid figure-img"></p>
<figcaption>Regular Adam Memory Operators Profiler</figcaption>
</figure>
</div>
<p align="center">
<em>Regular Adam Memory Operators</em>
</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/operator_mem_zero1.png" class="img-fluid figure-img"></p>
<figcaption>ZeRO-1 Memory Operators Profiler</figcaption>
</figure>
</div>
<p align="center">
<em>ZeRO-1 Memory Operators</em>
</p>
<ul>
<li><strong>Memory Peak</strong>: 11,528.6 MB (baseline) vs 8,090.3 MB (ZeRO-1)</li>
<li><strong>Optimizer state allocations</strong>: Much smaller in ZeRO-1 (sharded)</li>
<li><strong>Gradient allocations</strong>: Same in both (not yet sharded)</li>
<li><strong>Parameter allocations</strong>: Same in both (replicated)</li>
</ul>
<hr>
</section>
</section>
<section id="zero-2-profiler-analysis-gradient-sharding-impact" class="level3">
<h3 class="anchored" data-anchor-id="zero-2-profiler-analysis-gradient-sharding-impact">4.2 ZeRO-2 Profiler Analysis: Gradient Sharding Impact</h3>
<section id="overview-1" class="level4">
<h4 class="anchored" data-anchor-id="overview-1">4.2.1 <strong>Overview</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/overview_zero2.png" class="img-fluid figure-img"></p>
<figcaption>ZeRO-2 Overview Profiler</figcaption>
</figure>
</div>
<p align="center">
<em>ZeRO-2 Overview</em>
</p>
<p>The overview profiler reals a less GPU Utilization of ZeRO-2, compared to ZeRO-1 : 95.05 vs 95.53. The reasons are as follows</p>
<ul>
<li><strong>Reduce-scatter operations</strong> dominate communication patterns</li>
<li><strong>Interleaved compute and communication</strong> more visible than ZeRO-1</li>
<li><strong>More overhead</strong> clearly visible in execution timeline</li>
</ul>
</section>
<section id="operator-breakdown-1" class="level4">
<h4 class="anchored" data-anchor-id="operator-breakdown-1">4.2.2 <strong>Operator Breakdown</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/operator_zero2.png" class="img-fluid figure-img"></p>
<figcaption>ZeRO-2 Operators Profiler</figcaption>
</figure>
</div>
<p align="center">
<em>ZeRO-2 Operators</em>
</p>
<ul>
<li><strong>Reduce-scatter operations</strong> visible in the trace (new communication pattern)</li>
<li>More frequent communication events compared to ZeRO-1</li>
<li>Gradient communication happens per-layer during backward pass</li>
</ul>
<p><strong>Why more overhead than ZeRO-1?</strong></p>
<ul>
<li>Reduce-scatter requires coordination across all GPUs</li>
<li>Multiple synchronization points during backward pass</li>
<li>Small model + small batch size means latency dominates</li>
</ul>
</section>
<section id="kernel-execution-1" class="level4">
<h4 class="anchored" data-anchor-id="kernel-execution-1">4.2.3 <strong>Kernel Execution</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/kernel_zero2.png" class="img-fluid figure-img"></p>
<figcaption>ZeRO-2 Kernels Profiler</figcaption>
</figure>
</div>
<p align="center">
<em>ZeRO-2 Kernels</em>
</p>
<ul>
<li><strong>Kernel execution time</strong>: Similar to baseline</li>
<li><strong>Communication kernels interleaved</strong> with compute kernels</li>
<li>Shows more overhead: communication and compute are roughly equal</li>
</ul>
</section>
<section id="peak-memory-timeline-1" class="level4">
<h4 class="anchored" data-anchor-id="peak-memory-timeline-1">4.2.4 <strong>Peak Memory Timeline</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/peak_mem_zero2.png" class="img-fluid figure-img"></p>
<figcaption>ZeRO-2 Peak Memory Profiler</figcaption>
</figure>
</div>
<p align="center">
<em>ZeRO-2 Peak Memory: 8470.02 MB - Memory pattern shows spikes during reduce-scatter operations, baseline lower than ZeRO-1 but spikes higher</em>
</p>
<p><strong>Memory pattern analysis:</strong></p>
<ul>
<li><strong>Lower baseline</strong> (5.8 GB) than ZeRO-1 (6.9 GB) due to gradient sharding</li>
<li><strong>Temporary spikes</strong> during reduce-scatter buffer allocations</li>
<li><strong>Trade-off</strong>: Lower average memory, higher peak during communication</li>
</ul>
</section>
<section id="memory-operator-view-1" class="level4">
<h4 class="anchored" data-anchor-id="memory-operator-view-1">4.2.5 <strong>Memory Operator View</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/operator_mem_zero2.png" class="img-fluid figure-img"></p>
<figcaption>ZeRO-2 Memory Operators Profiler</figcaption>
</figure>
</div>
<p align="center">
<em>ZeRO-2 Memory Operators</em>
</p>
<ul>
<li>Shows <strong>temporary buffer allocations</strong> during reduce-scatter</li>
<li>These temporary buffers explain the higher peak vs ZeRO-1</li>
<li>Gradient memory stays low between communications</li>
</ul>
<hr>
</section>
</section>
<section id="zero-3-profiler-analysis-full-sharding-under-the-hood" class="level3">
<h3 class="anchored" data-anchor-id="zero-3-profiler-analysis-full-sharding-under-the-hood">4.3 ZeRO-3 Profiler Analysis: Full Sharding Under the Hood</h3>
<section id="overview-2" class="level4">
<h4 class="anchored" data-anchor-id="overview-2">4.3.1 <strong>Overview</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/overview_zero3.png" class="img-fluid figure-img"></p>
<figcaption>ZeRO-3 Overview Profiler</figcaption>
</figure>
</div>
<p align="center">
<em>ZeRO-3 Overview</em>
</p>
<p>The profiler traces reveal a drastric drop in GPU Utilization (81.41%). The reasons are as follows</p>
<ul>
<li><strong>Communication completely dominates</strong> the timeline</li>
<li><strong>Highly structured pattern</strong> of gather ‚Üí compute ‚Üí release</li>
<li><strong>Minimal compute islands</strong> in a ‚Äúsea of communication‚Äù</li>
</ul>
</section>
<section id="operator-breakdown-2" class="level4">
<h4 class="anchored" data-anchor-id="operator-breakdown-2">4.3.2 <strong>Operator Breakdown</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/operator_zero3.png" class="img-fluid figure-img"></p>
<figcaption>ZeRO-3 Operators Profiler</figcaption>
</figure>
</div>
<p align="center">
<em>ZeRO-3 Operators - 12 all-gather operations (6 forward + 6 backward)</em>
</p>
<ul>
<li><strong>Repeated all-gather operations</strong> dominate the trace</li>
<li><strong>6 forward layers + 6 backward layers</strong> = 12 all-gather operations per step</li>
<li>Compute operations are brief intervals between communications</li>
<li>Communication pattern is <strong>highly structured and predictable</strong></li>
</ul>
<p><strong>Why 12 all-gathers?</strong></p>
<pre><code>Forward Pass:  Layer1_gather ‚Üí compute ‚Üí Layer2_gather ‚Üí compute ‚Üí ...
Backward Pass: Layer6_gather ‚Üí compute ‚Üí Layer5_gather ‚Üí compute ‚Üí ...
Total: 6 + 6 = 12 gather operations</code></pre>
</section>
<section id="kernel-execution-2" class="level4">
<h4 class="anchored" data-anchor-id="kernel-execution-2">4.3.3 <strong>Kernel Execution</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/kernel_zero3.png" class="img-fluid figure-img"></p>
<figcaption>ZeRO-3 Kernels Profiler</figcaption>
</figure>
</div>
<p align="center">
<em>ZeRO-3 Kernels - Dense communication kernels fill most of the timeline (97% overhead)</em>
</p>
<ul>
<li><strong>Kernel execution time</strong>: Minimal compute kernels</li>
<li><strong>Dense communication kernels</strong> fill most of the timeline</li>
<li>Shows a heavy overhead: communication completely dominates</li>
</ul>
<p><strong>Small model problem:</strong></p>
<ul>
<li>Our 2.3B param model with 6 layers has very short compute time per layer</li>
<li>With 100B+ param models, compute time per layer increases dramatically</li>
<li>The paper shows [ZeRO Paper, p.17] that overhead drops to 10-20% for large models</li>
</ul>
</section>
<section id="peak-memory-timeline-2" class="level4">
<h4 class="anchored" data-anchor-id="peak-memory-timeline-2">4.3.4 <strong>Peak Memory Timeline</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/peak_mem_zero3.png" class="img-fluid figure-img"></p>
<figcaption>ZeRO-3 Peak Memory Profiler</figcaption>
</figure>
</div>
<p align="center">
<em>ZeRO-3 Peak Memory: 5033.95 MB - Sawtooth pattern shows periodic spikes during layer computation (baseline 2.36 GB, spikes to 5.03 GB)</em>
</p>
<p><strong>The sawtooth pattern shows:</strong></p>
<ol type="1">
<li><strong>Low baseline</strong>: Parameters stored as shards (2.36 GB)</li>
<li><strong>Spike up</strong>: All-gather before layer computation (~5 GB)</li>
<li><strong>Spike down</strong>: Release parameters after layer (back to 2.36 GB)</li>
<li><strong>Repeat</strong>: For each layer in forward and backward pass</li>
</ol>
<p>This is the <strong>signature of ZeRO-3‚Äôs on-demand materialization</strong>!</p>
</section>
<section id="memory-operator-view-2" class="level4">
<h4 class="anchored" data-anchor-id="memory-operator-view-2">4.3.5 <strong>Memory Operator View</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/operator_mem_zero3.png" class="img-fluid figure-img"></p>
<figcaption>ZeRO-3 Memory Operators Profiler</figcaption>
</figure>
</div>
<p align="center">
<em>ZeRO-3 Memory Operators - Dramatically lower baseline with clean gather ‚Üí compute ‚Üí release lifecycle</em>
</p>
<ul>
<li><strong>Dramatically lower memory baseline</strong> compared to all other methods</li>
<li><strong>All-gather operations</strong> show as memory allocation spikes</li>
<li><strong>Release operations</strong> show as immediate memory deallocation</li>
<li>Very clean lifecycle: <strong>gather ‚Üí compute ‚Üí release</strong></li>
</ul>
<p><strong>Why better than theory?</strong></p>
<pre><code>Theoretical: 16Œ®/Nd = 8Œ® (with Nd=2) = 18.3 GB
Observed: 5.03 GB peak
Bonus: 13.3 GB better!

Reason: Only ONE layer's parameters materialized at a time
Not all 8Œ® held simultaneously!</code></pre>
</section>
</section>
<section id="comparative-profiler-insights" class="level3">
<h3 class="anchored" data-anchor-id="comparative-profiler-insights">4.4 Comparative Profiler Insights</h3>
<section id="communication-pattern-summary" class="level4">
<h4 class="anchored" data-anchor-id="communication-pattern-summary">4.4.1 <strong>Communication Pattern Summary</strong></h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Stage</th>
<th>Pattern</th>
<th>Frequency</th>
<th>Volume per Step</th>
<th>Overhead</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Baseline</strong></td>
<td>All-reduce gradients</td>
<td>Once per step</td>
<td>2Œ®</td>
<td>Reference</td>
</tr>
<tr class="even">
<td><strong>ZeRO-1</strong></td>
<td>All-reduce + Broadcast</td>
<td>Once per step</td>
<td>2Œ®</td>
<td><strong>0%</strong></td>
</tr>
<tr class="odd">
<td><strong>ZeRO-2</strong></td>
<td>Reduce-scatter</td>
<td>Per parameter</td>
<td>2Œ®</td>
<td><strong>48.6%</strong></td>
</tr>
<tr class="even">
<td><strong>ZeRO-3</strong></td>
<td>All-gather</td>
<td>Per layer (√ó12)</td>
<td>3Œ®</td>
<td><strong>97.0%</strong></td>
</tr>
</tbody>
</table>
<hr>
</section>
</section>
</section>
<section id="comparative-analysis-choosing-the-right-zero-stage" class="level2">
<h2 class="anchored" data-anchor-id="comparative-analysis-choosing-the-right-zero-stage">5. Comparative Analysis: Choosing the Right ZeRO Stage</h2>
<p>Now that we‚Äôve explored each ZeRO stage in detail, let‚Äôs step back and compare them systematically to help you choose the right optimization for your use case.</p>
<section id="memory-savings-comparison" class="level3">
<h3 class="anchored" data-anchor-id="memory-savings-comparison">5.1 Memory Savings Comparison</h3>
<p>Let‚Äôs visualize our experimental results across all stages:</p>
<section id="our-experimental-results-2.3b-params-2-gpus" class="level4">
<h4 class="anchored" data-anchor-id="our-experimental-results-2.3b-params-2-gpus">5.1.1 <strong>Our Experimental Results (2.3B params, 2 GPUs)</strong></h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Metric</th>
<th>Baseline</th>
<th>ZeRO-1</th>
<th>ZeRO-2</th>
<th>ZeRO-3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Peak Memory (MB)</strong></td>
<td>11,529</td>
<td>8,090</td>
<td>8,470</td>
<td><strong>5,034</strong></td>
</tr>
<tr class="even">
<td><strong>Memory Reduction</strong></td>
<td>0%</td>
<td>29.82%</td>
<td>26.53%</td>
<td><strong>56.34%</strong></td>
</tr>
<tr class="odd">
<td><strong>Memory Saved (GB)</strong></td>
<td>0</td>
<td>3.44</td>
<td>3.06</td>
<td><strong>6.49</strong></td>
</tr>
<tr class="even">
<td><strong>Initial State (MB)</strong></td>
<td>~7,000</td>
<td>6,944</td>
<td>5,797</td>
<td><strong>2,360</strong></td>
</tr>
<tr class="odd">
<td><strong>Comm Overhead</strong></td>
<td>Baseline</td>
<td>0.0%</td>
<td>48.6%</td>
<td>97.0%</td>
</tr>
<tr class="even">
<td><strong>Avg Step Time (ms)</strong></td>
<td>-</td>
<td>24</td>
<td>29</td>
<td>5</td>
</tr>
<tr class="odd">
<td><strong>Theoretical Reduction</strong></td>
<td>0%</td>
<td>37.5%</td>
<td>43.7%</td>
<td>50.0%</td>
</tr>
<tr class="even">
<td><strong>Theory vs Reality</strong></td>
<td>-</td>
<td>-7.7%</td>
<td>-17.2%</td>
<td><strong>+6.3%</strong></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="communication-overhead-analysis" class="level3">
<h3 class="anchored" data-anchor-id="communication-overhead-analysis">5.2 Communication Overhead Analysis</h3>
<p>The memory savings come with varying communication costs:</p>
<section id="communication-patterns" class="level4">
<h4 class="anchored" data-anchor-id="communication-patterns">5.2.1 <strong>Communication Patterns</strong></h4>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 50%">
<col style="width: 16%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Stage</th>
<th>Communication Operations</th>
<th>Volume</th>
<th>Overhead</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Baseline DP</strong></td>
<td>All-reduce gradients</td>
<td>2Œ®</td>
<td>Reference</td>
</tr>
<tr class="even">
<td><strong>ZeRO-1</strong></td>
<td>All-reduce gradients + Broadcast params</td>
<td>2Œ®</td>
<td><strong>0%</strong></td>
</tr>
<tr class="odd">
<td><strong>ZeRO-2</strong></td>
<td>Reduce-scatter grads + Broadcast params</td>
<td>Œ® + Œ® = 2Œ®</td>
<td><strong>48.6%</strong></td>
</tr>
<tr class="even">
<td><strong>ZeRO-3</strong></td>
<td>Reduce-scatter + All-gather (per layer)</td>
<td>3Œ®</td>
<td><strong>97.0%</strong></td>
</tr>
</tbody>
</table>
<p>[ZeRO Paper, p.13-14, Section 7]</p>
<p><strong>Why does ZeRO-1 have 0% overhead despite broadcasting?</strong> - Baseline all-reduce = reduce-scatter + all-gather = 2Œ® volume - ZeRO-1 uses reduce-scatter (Œ®) + broadcast (Œ®) = 2Œ® volume - <strong>Same total communication, different pattern!</strong></p>
<p><strong>Why does ZeRO-2 show 48.6% overhead in our experiments?</strong> - The paper predicts same volume (2Œ®) as baseline - Our 2-GPU setup with small batch size makes communication latency dominant - Reduce-scatter has more synchronization points than simple all-reduce - With 8+ GPUs and larger batches, overhead amortizes to near-zero</p>
<p><strong>Why does ZeRO-3 have 97% overhead?</strong> - All-gather for every layer (12 operations per step in our 6-layer model) - Small model means low arithmetic intensity - With 100B+ params, compute time dominates and overhead drops to ~10-20%</p>
</section>
<section id="communication-overhead-vs-model-size" class="level4">
<h4 class="anchored" data-anchor-id="communication-overhead-vs-model-size">5.2.2 <strong>Communication Overhead vs Model Size</strong></h4>
<p>From the ZeRO paper [p.17, Figure 2], with 400 GPUs:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model Size</th>
<th>Baseline-MP</th>
<th>ZeRO-100B</th>
<th>Speedup</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1.5B</td>
<td>5 TFlops/GPU</td>
<td>30 TFlops/GPU</td>
<td>6√ó</td>
</tr>
<tr class="even">
<td>40B</td>
<td>2 TFlops/GPU</td>
<td>35 TFlops/GPU</td>
<td>17.5√ó</td>
</tr>
<tr class="odd">
<td>100B</td>
<td>OOM</td>
<td><strong>38 TFlops/GPU</strong></td>
<td>‚àû (can‚Äôt run baseline)</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="scalability-comparison" class="level3">
<h3 class="anchored" data-anchor-id="scalability-comparison">5.3 Scalability Comparison</h3>
<section id="memory-scaling-with-number-of-gpus" class="level4">
<h4 class="anchored" data-anchor-id="memory-scaling-with-number-of-gpus">5.3.1 <strong>Memory Scaling with Number of GPUs</strong></h4>
<p><strong>Theoretical memory per GPU (Œ® = 7.5B params, K=12):</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th># GPUs</th>
<th>Baseline</th>
<th>ZeRO-1</th>
<th>ZeRO-2</th>
<th>ZeRO-3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>120 GB</td>
<td>120 GB</td>
<td>120 GB</td>
<td>120 GB</td>
</tr>
<tr class="even">
<td>2</td>
<td>120 GB</td>
<td>97.5 GB</td>
<td>82.5 GB</td>
<td><strong>60 GB</strong></td>
</tr>
<tr class="odd">
<td>4</td>
<td>120 GB</td>
<td>52.5 GB</td>
<td>41.3 GB</td>
<td><strong>30 GB</strong></td>
</tr>
<tr class="even">
<td>8</td>
<td>120 GB</td>
<td>41.4 GB</td>
<td>28.8 GB</td>
<td><strong>15 GB</strong></td>
</tr>
<tr class="odd">
<td>16</td>
<td>120 GB</td>
<td>35.6 GB</td>
<td>21.6 GB</td>
<td><strong>7.5 GB</strong></td>
</tr>
<tr class="even">
<td>64</td>
<td>120 GB</td>
<td>31.4 GB</td>
<td>16.6 GB</td>
<td><strong>1.9 GB</strong></td>
</tr>
</tbody>
</table>
<p>[ZeRO Paper, p.3, Figure 1; p.11, Table 1]</p>
<p><strong>Observations:</strong></p>
<ul>
<li><strong>Baseline</strong>: No benefit from more GPUs (data parallelism replicates everything)</li>
<li><strong>ZeRO-1</strong>: Diminishing returns as Nd increases (4Œ® + KŒ®/Nd ‚Üí 4Œ®)</li>
<li><strong>ZeRO-2</strong>: Better scaling than ZeRO-1 (2Œ® + 14Œ®/Nd ‚Üí 2Œ®)</li>
<li><strong>ZeRO-3</strong>: <strong>Linear scaling!</strong> (16Œ®/Nd ‚Üí 0 as Nd ‚Üí ‚àû)</li>
</ul>
</section>
<section id="maximum-trainable-model-size" class="level4">
<h4 class="anchored" data-anchor-id="maximum-trainable-model-size">5.3.2 <strong>Maximum Trainable Model Size</strong></h4>
<p>Given 32GB V100 GPUs, what‚Äôs the maximum model size?</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th># GPUs</th>
<th>Baseline</th>
<th>ZeRO-1</th>
<th>ZeRO-2</th>
<th>ZeRO-3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1.4B</td>
<td>1.4B</td>
<td>1.4B</td>
<td>1.4B</td>
</tr>
<tr class="even">
<td>4</td>
<td>1.4B</td>
<td>2.5B</td>
<td>4B</td>
<td><strong>8B</strong></td>
</tr>
<tr class="odd">
<td>8</td>
<td>1.4B</td>
<td>4B</td>
<td>6B</td>
<td><strong>16B</strong></td>
</tr>
<tr class="even">
<td>16</td>
<td>1.4B</td>
<td><strong>6.2B</strong></td>
<td>12.5B</td>
<td><strong>32B</strong></td>
</tr>
<tr class="odd">
<td>64</td>
<td>1.4B</td>
<td>7.6B</td>
<td>14.4B</td>
<td><strong>128B</strong></td>
</tr>
<tr class="even">
<td>1024</td>
<td>1.4B</td>
<td>13B</td>
<td>19B</td>
<td><strong>2 Trillion!</strong></td>
</tr>
</tbody>
</table>
<p>[ZeRO Paper, p.13, Table 2]</p>
<p><strong>Revolutionary Impact:</strong> ZeRO-3 with 1024 GPUs can train models <strong>1,428√ó larger</strong> than baseline!</p>
</section>
<section id="why-zero-2-can-be-a-free-lunch-and-why-you-should-start-there" class="level4">
<h4 class="anchored" data-anchor-id="why-zero-2-can-be-a-free-lunch-and-why-you-should-start-there">5.3.3 <strong>Why ZeRO-2 Can Be a Free Lunch (And Why You Should Start There)</strong></h4>
<p>The conventional wisdom suggests starting with ZeRO-1 because it has ‚Äúzero overhead.‚Äù However, a deeper analysis reveals that <strong>ZeRO-2 should be your default starting point</strong> in most practical scenarios. Here‚Äôs why:</p>
<section id="the-communication-volume-paradox" class="level5">
<h5 class="anchored" data-anchor-id="the-communication-volume-paradox">The Communication Volume Paradox</h5>
<p>Looking at the communication table from Section 5.2.1:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Stage</th>
<th>Communication Volume</th>
<th>Measured Overhead</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Baseline DP</strong></td>
<td>2Œ®</td>
<td>Reference (0%)</td>
</tr>
<tr class="even">
<td><strong>ZeRO-1</strong></td>
<td>2Œ®</td>
<td>0%</td>
</tr>
<tr class="odd">
<td><strong>ZeRO-2</strong></td>
<td>2Œ®</td>
<td>48.6% (?)</td>
</tr>
</tbody>
</table>
<p><strong>The paradox:</strong> ZeRO-2 has the <strong>same communication volume</strong> as both Baseline and ZeRO-1, yet shows 48.6% overhead in our small-scale experiments. What‚Äôs happening?</p>
</section>
<section id="understanding-the-48.6-overhead" class="level5">
<h5 class="anchored" data-anchor-id="understanding-the-48.6-overhead">Understanding the 48.6% Overhead</h5>
<p>The measured overhead is <strong>not fundamental</strong> to ZeRO-2, but an artifact of our experimental setup:</p>
<p><strong>Why we see overhead in 2-GPU, small-batch experiments:</strong></p>
<ol type="1">
<li><strong>Latency dominates bandwidth</strong>: With only 2 GPUs and small batches, communication latency (synchronization overhead) dominates actual data transfer time
<ul>
<li>Communication time ‚âà latency + (volume / bandwidth)</li>
<li>Small volume ‚Üí latency term dominates</li>
<li>More synchronization points in reduce-scatter vs single all-reduce</li>
</ul></li>
<li><strong>Low arithmetic intensity</strong>: Our 2.3B parameter model with batch size 16 doesn‚Äôt perform enough compute to hide communication
<ul>
<li>Compute time: ~15ms</li>
<li>Communication time: ~14ms</li>
<li>Result: 48.6% overhead</li>
</ul></li>
<li><strong>Temporary buffer allocations</strong>: ZeRO-2‚Äôs reduce-scatter creates temporary buffers during gradient bucketing (visible in profiler), adding small memory spikes</li>
</ol>
</section>
<section id="when-zero-2-becomes-free" class="level5">
<h5 class="anchored" data-anchor-id="when-zero-2-becomes-free">When ZeRO-2 Becomes Free</h5>
<p><strong>In production settings, ZeRO-2‚Äôs overhead vanishes:</strong></p>
<p><strong>Scenario 1: 8+ GPUs (Single Node)</strong></p>
<pre><code>Setup: 8 GPUs with NVLink, batch size 64, 7.5B params
Communication:
  - More GPUs ‚Üí better overlap of compute and communication
  - NVLink bandwidth (600 GB/s) easily handles 2Œ® volume
  - Overhead: &lt; 5%</code></pre>
<p><strong>Scenario 2: Larger Batch Sizes</strong></p>
<pre><code>Our experiment: batch_size = 16
  Compute time: 15ms
  Communication: 14ms
  Overhead: 48.6%

With batch_size = 128:
  Compute time: 120ms (8√ó longer)
  Communication: 14ms (same!)
  Overhead: 11.6% (4√ó reduction!)</code></pre>
<p><strong>Scenario 3: Larger Models</strong></p>
<pre><code>Our 2.3B model: 48.6% overhead

With 13B params (GPT-3 scale):
  - 5.6√ó more parameters
  - 5.6√ó more FLOPs per layer
  - Same communication volume (still 2Œ®)
  - Overhead: ~8-10%

With 70B params (Llama-2 scale):
  - Overhead: &lt; 3%</code></pre>
</section>
<section id="the-free-lunch-argument" class="level5">
<h5 class="anchored" data-anchor-id="the-free-lunch-argument">The Free Lunch Argument</h5>
<p><strong>ZeRO-2 gives you free memory savings in realistic scenarios:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 18%">
<col style="width: 21%">
<col style="width: 21%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th>Scenario</th>
<th>Typical Setup</th>
<th>ZeRO-1 Overhead</th>
<th>ZeRO-2 Overhead</th>
<th>ZeRO-2 Extra Savings</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Single node training</strong></td>
<td>8√ó A100, NVLink, batch=32</td>
<td>0%</td>
<td>~3-5%</td>
<td>+15-20% memory</td>
</tr>
<tr class="even">
<td><strong>Multi-node cluster</strong></td>
<td>64 GPUs, InfiniBand, batch=128</td>
<td>0%</td>
<td>~1-2%</td>
<td>+10-15% memory</td>
</tr>
<tr class="odd">
<td><strong>Large model (&gt;10B)</strong></td>
<td>Any setup with batch&gt;64</td>
<td>0%</td>
<td>~2-5%</td>
<td>+15-20% memory</td>
</tr>
</tbody>
</table>
<p><strong>The punchline:</strong> In production scenarios with reasonable batch sizes and GPU counts, ZeRO-2‚Äôs overhead becomes negligible (1-5%), while providing significant additional memory savings over ZeRO-1.</p>
</section>
<section id="why-start-with-zero-2-not-zero-1" class="level5">
<h5 class="anchored" data-anchor-id="why-start-with-zero-2-not-zero-1">Why Start with ZeRO-2, Not ZeRO-1</h5>
<p><strong>Practical reasons to default to ZeRO-2:</strong></p>
<ol type="1">
<li><p><strong>Better memory scaling</strong>: ZeRO-2 scales as <code>2Œ® + 14Œ®/Nd</code> vs ZeRO-1‚Äôs <code>4Œ® + 12Œ®/Nd</code></p>
<ul>
<li>With 8 GPUs: ZeRO-2 saves 28.8 GB vs ZeRO-1‚Äôs 41.4 GB (for 7.5B params)</li>
<li><strong>32% more memory available!</strong></li>
</ul></li>
<li><p><strong>Larger trainable models</strong>: The extra memory means you can fit bigger models or larger batch sizes</p>
<ul>
<li>Bigger batches ‚Üí better GPU utilization</li>
<li>Better utilization ‚Üí higher throughput</li>
<li><strong>Can offset small communication overhead!</strong></li>
</ul></li>
<li><p><strong>Future-proof</strong>: When you scale to more GPUs or larger models, ZeRO-2 is already optimized</p>
<ul>
<li>No need to re-tune or change code</li>
<li>Smooth transition from prototyping to production</li>
</ul></li>
<li><p><strong>Modern hardware hides overhead</strong>: With NVLink (A100/H100) or InfiniBand, communication is fast enough that overhead is minimal</p></li>
</ol>
<p><strong>The experimental 48.6% overhead is misleading</strong> because:</p>
<ul>
<li>It‚Äôs measured in a worst-case scenario (2 GPUs, small batch, small model)</li>
<li>Real training uses 8+ GPUs, larger batches, and larger models</li>
<li>In those settings, ZeRO-2 overhead drops to 1-5%</li>
</ul>
</section>
</section>
<section id="the-new-recommendation" class="level4">
<h4 class="anchored" data-anchor-id="the-new-recommendation">The New Recommendation</h4>
<p><strong>Old thinking:</strong> ‚ÄúStart with ZeRO-1 (zero overhead), only use ZeRO-2 if desperate for memory‚Äù</p>
<p><strong>Better approach:</strong> ‚ÄúStart with ZeRO-2 by default, fall back to ZeRO-1 only if:‚Äù</p>
<ul>
<li>You have <strong>very limited interconnect</strong> bandwidth (e.g., old PCIe Gen3)</li>
<li>You‚Äôre doing <strong>small-scale experiments</strong> with 2-4 GPUs and can‚Äôt increase batch size</li>
<li>You have a <strong>latency-critical</strong> application where every millisecond counts</li>
</ul>
<p><strong>In all other cases, ZeRO-2 is effectively free</strong> and gives you 15-20% more memory to work with.</p>
<section id="theoretical-foundation" class="level5">
<h5 class="anchored" data-anchor-id="theoretical-foundation">Theoretical Foundation</h5>
<p>From the ZeRO paper [p.14, Section 7.3]:</p>
<blockquote class="blockquote">
<p>‚ÄúZeRO-2 has the same communication volume as baseline data parallelism (2Œ®), making it a <strong>free optimization</strong> in terms of communication cost.‚Äù</p>
</blockquote>
<p>The paper‚Äôs analysis is based on:</p>
<ul>
<li>Production-scale clusters (64+ GPUs)</li>
<li>Realistic batch sizes (1-4K global batch)</li>
<li>Large models (1.5B - 100B parameters)</li>
</ul>
<p>Our small-scale experiments (2 GPUs, batch 16, 2.3B params) are <strong>outside the paper‚Äôs intended operating regime</strong>. The 48.6% overhead disappears when you move to realistic training scenarios.</p>
</section>
<section id="practical-validation" class="level5">
<h5 class="anchored" data-anchor-id="practical-validation">Practical Validation</h5>
<p>If you doubt this, try this experiment:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb27"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Our 2-GPU baseline</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="ex">torchrun</span> <span class="at">--nproc_per_node</span><span class="op">=</span>2 zero2.py  <span class="co"># 48.6% overhead</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale to 8 GPUs with larger batch</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="ex">torchrun</span> <span class="at">--nproc_per_node</span><span class="op">=</span>8 zero2.py <span class="at">--batch_size</span><span class="op">=</span>64  <span class="co"># ~5% overhead</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Even larger batch</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="ex">torchrun</span> <span class="at">--nproc_per_node</span><span class="op">=</span>8 zero2.py <span class="at">--batch_size</span><span class="op">=</span>128  <span class="co"># ~2% overhead</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Bottom line:</strong> ZeRO-2 is the sweet spot for most practitioners. It provides substantial memory savings with negligible overhead in realistic training scenarios. Don‚Äôt let our small-scale experimental artifacts mislead you‚Äîstart with ZeRO-2!</p>
<hr>
</section>
</section>
</section>
<section id="decision-framework-which-stage-should-you-use" class="level3">
<h3 class="anchored" data-anchor-id="decision-framework-which-stage-should-you-use">5.4 Decision Framework: Which Stage Should You Use?</h3>
<p>Here‚Äôs a practical decision tree based on your constraints:</p>
<section id="based-on-model-size" class="level4">
<h4 class="anchored" data-anchor-id="based-on-model-size">5.4.1 <strong>Based on Model Size</strong></h4>
<pre><code>Model Size Decision Tree:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

&lt; 3B params
‚îî‚îÄ&gt; Use standard Data Parallelism (if fits)
    ‚îî‚îÄ&gt; Or ZeRO-2 for extra headroom (recommended!)

3B - 15B params
‚îî‚îÄ&gt; ZeRO-2 (Default recommendation)
    ‚îú‚îÄ&gt; Sweet spot: Significant memory savings with minimal overhead
    ‚îú‚îÄ&gt; Works well on single node (8 GPUs)
    ‚îî‚îÄ&gt; Fall back to ZeRO-1 only with poor interconnect

15B - 100B params
‚îî‚îÄ&gt; ZeRO-2 with 8+ GPUs
    ‚îú‚îÄ&gt; Requires high-bandwidth interconnect (NVLink/InfiniBand)
    ‚îî‚îÄ&gt; Communication overhead becomes negligible at this scale

&gt; 100B params
‚îî‚îÄ&gt; ZeRO-3 (No choice!)
    ‚îú‚îÄ&gt; Only option that fits
    ‚îî‚îÄ&gt; Combine with Model Parallelism if needed</code></pre>
</section>
<section id="based-on-hardware-configuration" class="level4">
<h4 class="anchored" data-anchor-id="based-on-hardware-configuration">5.4.2 <strong>Based on Hardware Configuration</strong></h4>
<table class="caption-top table">
<colgroup>
<col style="width: 34%">
<col style="width: 41%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th>Hardware Setup</th>
<th>Recommended Stage</th>
<th>Rationale</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Single Node (8 GPUs)</strong></td>
<td><strong>ZeRO-2</strong> (default)</td>
<td>High bandwidth within node, overhead ~3-5%</td>
</tr>
<tr class="even">
<td><strong>Multi-Node (InfiniBand)</strong></td>
<td><strong>ZeRO-2</strong> (default)</td>
<td>Good inter-node bandwidth supports ZeRO-2</td>
</tr>
<tr class="odd">
<td><strong>Multi-Node (Ethernet)</strong></td>
<td>ZeRO-1 or ZeRO-2</td>
<td>Test both; ZeRO-2 may still work with large batches</td>
</tr>
<tr class="even">
<td><strong>Large Cluster (64+ GPUs)</strong></td>
<td>ZeRO-2 or ZeRO-3</td>
<td>Scale justifies communication overhead</td>
</tr>
<tr class="odd">
<td><strong>Memory-Constrained</strong></td>
<td>ZeRO-3</td>
<td>Necessity overrides efficiency concerns</td>
</tr>
</tbody>
</table>
</section>
<section id="based-on-batch-size-constraints" class="level4">
<h4 class="anchored" data-anchor-id="based-on-batch-size-constraints">5.4.3 <strong>Based on Batch Size Constraints</strong></h4>
<table class="caption-top table">
<colgroup>
<col style="width: 32%">
<col style="width: 32%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Batch Size</th>
<th>Best Stage</th>
<th>Explanation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Large batch OK (128+)</strong></td>
<td><strong>ZeRO-2</strong></td>
<td>Default choice; overhead &lt; 2% at this scale</td>
</tr>
<tr class="even">
<td><strong>Medium batch (32-128)</strong></td>
<td><strong>ZeRO-2</strong></td>
<td>Sweet spot; overhead ~3-5%</td>
</tr>
<tr class="odd">
<td><strong>Small batch (8-32)</strong></td>
<td>ZeRO-2 or ZeRO-1</td>
<td>Test both; may see 10-20% overhead</td>
</tr>
<tr class="even">
<td><strong>Very small batch (&lt;8)</strong></td>
<td>ZeRO-1 or ZeRO-3</td>
<td>ZeRO-1 if fits, else ZeRO-3 for memory</td>
</tr>
<tr class="odd">
<td><strong>Critical batch size hit</strong></td>
<td>Combine ZeRO + MP</td>
<td>Hybrid approach</td>
</tr>
</tbody>
</table>
<p>[Note: Critical batch size is the point where larger batches hurt convergence [ZeRO Paper, p.4, footnote 1]]</p>
</section>
<section id="quick-start-recommendation" class="level4">
<h4 class="anchored" data-anchor-id="quick-start-recommendation">5.4.4 <strong>Quick Start Recommendation</strong></h4>
<p><strong>If you‚Äôre unsure, start here:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Default recommendation for most use cases</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>Stage: ZeRO<span class="op">-</span><span class="dv">2</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>GPUs: <span class="dv">8</span> (single node)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>Batch size per GPU: <span class="dv">4</span><span class="op">-</span><span class="dv">8</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>Global batch size: <span class="dv">32</span><span class="op">-</span><span class="dv">64</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>Why: This gives you <span class="op">~</span><span class="dv">26</span><span class="op">%</span> memory savings <span class="cf">with</span> <span class="op">&lt;</span><span class="dv">5</span><span class="op">%</span> overhead <span class="kw">in</span> practice.</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Only deviate from ZeRO-2 if:</strong></p>
<ul>
<li>Your model fits comfortably with ZeRO-1 AND you‚Äôre bandwidth-constrained ‚Üí Use ZeRO-1</li>
<li>Your model doesn‚Äôt fit even with ZeRO-2 ‚Üí Use ZeRO-3</li>
<li>You‚Äôre doing tiny 2-GPU experiments for debugging ‚Üí Use ZeRO-1 (our experiments fall in this category!)</li>
</ul>
<hr>
</section>
</section>
</section>
<section id="implementation-deep-dive" class="level2">
<h2 class="anchored" data-anchor-id="implementation-deep-dive">6. Implementation Deep Dive</h2>
<p>With the theory and comparative analysis complete, let‚Äôs dive into the <strong>actual implementation</strong>. This section walks through the code line-by-line, revealing how ZeRO‚Äôs elegant concepts translate into working PyTorch code.</p>
<section id="project-structure" class="level3">
<h3 class="anchored" data-anchor-id="project-structure">6.1 Project Structure</h3>
<p>Our implementation consists of three main files with supporting utilities:</p>
<pre><code>zero-daddyofadoggy/
‚îú‚îÄ‚îÄ zero1.py                    # ZeRO-1: Optimizer state sharding
‚îú‚îÄ‚îÄ zero2.py                    # ZeRO-2: + Gradient sharding
‚îú‚îÄ‚îÄ zero3.py                    # ZeRO-3: + Parameter sharding
‚îî‚îÄ‚îÄ training_utils/
    ‚îú‚îÄ‚îÄ memory.py               # Memory tracking utilities
    ‚îî‚îÄ‚îÄ utils.py                # Distributed training helpers</code></pre>
<p>Each implementation follows the same pattern:</p>
<ol type="1">
<li><strong>ShardedOptimizer</strong> class wrapping PyTorch‚Äôs Adam optimizer</li>
<li><strong>Hooks</strong> to intercept gradients and parameters during training</li>
<li><strong>Communication primitives</strong> (all-reduce, broadcast, reduce-scatter, all-gather)</li>
<li><strong>Training loop</strong> with memory profiling</li>
</ol>
<p>Let‚Äôs examine each ZeRO stage in detail.</p>
<hr>
</section>
<section id="zero-1-optimizer-state-partitioning" class="level3">
<h3 class="anchored" data-anchor-id="zero-1-optimizer-state-partitioning">6.2 ZeRO-1: Optimizer State Partitioning</h3>
<p><strong>File:</strong> <code>zero1.py:22-88</code></p>
<section id="the-shardedoptimizer-class" class="level4">
<h4 class="anchored" data-anchor-id="the-shardedoptimizer-class">6.2.1 <strong>The ShardedOptimizer Class</strong></h4>
<p>The core of ZeRO-1 is parameter sharding logic:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ShardedOptimizer:</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, optimizer: Optimizer):</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.optimizer <span class="op">=</span> optimizer</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.original_param_groups <span class="op">=</span> optimizer.param_groups</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.params <span class="op">=</span> [</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>            param <span class="cf">for</span> group <span class="kw">in</span> <span class="va">self</span>.original_param_groups</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> param <span class="kw">in</span> group[<span class="st">"params"</span>]</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>        ]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What‚Äôs happening:</strong></p>
<ul>
<li>We wrap an existing PyTorch optimizer (Adam in our case)</li>
<li>Extract all parameters from param_groups into a flat list</li>
<li>This list will be sharded across GPUs</li>
</ul>
</section>
<section id="parameter-sharding-strategy" class="level4">
<h4 class="anchored" data-anchor-id="parameter-sharding-strategy">6.2.2 <strong>Parameter Sharding Strategy</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>world_size <span class="op">=</span> get(<span class="st">'ws'</span>)  <span class="co"># Number of GPUs</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>rank <span class="op">=</span> get(<span class="st">'rank'</span>)       <span class="co"># Current GPU ID</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Evenly distribute parameters across GPUs</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>params_per_rank <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.params) <span class="op">//</span> world_size</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>remainder <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.params) <span class="op">%</span> world_size</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Handle uneven division (e.g., 100 params / 3 GPUs)</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>start_idx <span class="op">=</span> rank <span class="op">*</span> params_per_rank <span class="op">+</span> <span class="bu">min</span>(rank, remainder)</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>end_idx <span class="op">=</span> start_idx <span class="op">+</span> params_per_rank <span class="op">+</span> (<span class="dv">1</span> <span class="cf">if</span> rank <span class="op">&lt;</span> remainder <span class="cf">else</span> <span class="dv">0</span>)</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a><span class="va">self</span>.local_param_indices <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(start_idx, end_idx))</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a><span class="va">self</span>.local_params <span class="op">=</span> <span class="bu">set</span>(<span class="va">self</span>.params[i] <span class="cf">for</span> i <span class="kw">in</span> <span class="va">self</span>.local_param_indices)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Example:</strong> 100 parameters, 3 GPUs</p>
<ul>
<li>GPU 0: params 0-33 (34 params)</li>
<li>GPU 1: params 34-67 (34 params)</li>
<li>GPU 2: params 68-99 (32 params)</li>
</ul>
<p>The <code>remainder</code> logic ensures fair distribution.</p>
</section>
<section id="removing-non-local-parameters" class="level4">
<h4 class="anchored" data-anchor-id="removing-non-local-parameters">6.2.3 <strong>Removing Non-Local Parameters</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _shard_optimizer_params(<span class="va">self</span>):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Remove non-local parameters from optimizer param groups"""</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> group <span class="kw">in</span> <span class="va">self</span>.optimizer.param_groups:</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>        group[<span class="st">'params'</span>] <span class="op">=</span> [p <span class="cf">for</span> p <span class="kw">in</span> group[<span class="st">'params'</span>]</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>                          <span class="cf">if</span> p <span class="kw">in</span> <span class="va">self</span>.local_params]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Critical insight:</strong> This is where memory savings happen! By removing 2/3 of parameters from the optimizer on each GPU, we reduce optimizer state memory by ~67% (for 3 GPUs).</p>
</section>
<section id="the-training-step" class="level4">
<h4 class="anchored" data-anchor-id="the-training-step">6.2.4 <strong>The Training Step</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> step(<span class="va">self</span>, closure<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    step_start <span class="op">=</span> time.perf_counter()</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: All-reduce gradients</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> record_function(<span class="st">"all_reduce_gradients"</span>):</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> p.grad <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>                dist.all_reduce(p.grad.data, op<span class="op">=</span>dist.ReduceOp.SUM)</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>                p.grad.data <span class="op">/=</span> get(<span class="st">"ws"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Why all-reduce?</strong> Each GPU computed gradients on the full model (data parallelism). We need to average them before updating parameters.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: Update only local parameters</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> record_function(<span class="st">"optimizer_step"</span>):</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.optimizer.step(closure)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Memory savings:</strong> Only local params update, so momentum/variance states exist only for local shards.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: Broadcast updated parameters</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> record_function(<span class="st">"broadcast_parameters"</span>):</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>        params_per_rank <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.params) <span class="op">//</span> get(<span class="st">'ws'</span>)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>        remainder <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.params) <span class="op">%</span> get(<span class="st">'ws'</span>)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, p <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.params):</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Recompute owner rank for this param index</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="op">&lt;</span> (params_per_rank <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> remainder:</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>                owner_rank <span class="op">=</span> i <span class="op">//</span> (params_per_rank <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>                owner_rank <span class="op">=</span> (i <span class="op">-</span> remainder) <span class="op">//</span> params_per_rank</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>            dist.broadcast(p.data, src<span class="op">=</span>owner_rank)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>The synchronization step:</strong> Each GPU broadcasts its updated parameters to all others. This ensures all GPUs have identical model weights.</p>
</section>
<section id="profiling-integration" class="level4">
<h4 class="anchored" data-anchor-id="profiling-integration">6.2.5 <strong>Profiling Integration</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># zero1.py:188-206</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>profiler_context <span class="op">=</span> profile(</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>    activities<span class="op">=</span>[ProfilerActivity.CPU, ProfilerActivity.CUDA],</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>    schedule<span class="op">=</span>schedule(skip_first<span class="op">=</span><span class="dv">5</span>, wait<span class="op">=</span><span class="dv">1</span>, warmup<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>                     active<span class="op">=</span><span class="dv">5</span>, repeat<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>    on_trace_ready<span class="op">=</span>torch.profiler.tensorboard_trace_handler(</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">"./profiler_traces/zero1_adam"</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>    record_shapes<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>    profile_memory<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>    with_stack<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>    with_flops<span class="op">=</span><span class="va">True</span></span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This generates the TensorBoard traces we analyzed in Section 3.3.4!</p>
<hr>
</section>
</section>
<section id="zero-2-adding-gradient-sharding" class="level3">
<h3 class="anchored" data-anchor-id="zero-2-adding-gradient-sharding">6.3 ZeRO-2: Adding Gradient Sharding</h3>
<p><strong>File:</strong> <code>zero2.py:21-138</code></p>
<p>ZeRO-2 builds on ZeRO-1 by <strong>also sharding gradients</strong>. The key difference is in gradient handling.</p>
<section id="gradient-hooks" class="level4">
<h4 class="anchored" data-anchor-id="gradient-hooks">6.3.1 <strong>Gradient Hooks</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Zero2Hook:</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Discard gradients of parameters not on current device"""</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, param: torch.nn.Parameter,</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>                 is_local_param: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>):</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.param <span class="op">=</span> param</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.is_local_param <span class="op">=</span> is_local_param</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, grad):</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.is_local_param:</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">None</span>  <span class="co"># Discard non-local gradients</span></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> grad      <span class="co"># Keep local gradients</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Purpose:</strong> During backward pass, discard gradients for parameters we don‚Äôt own. This saves gradient memory!</p>
</section>
<section id="registering-hooks" class="level4">
<h4 class="anchored" data-anchor-id="registering-hooks">6.3.2 <strong>Registering Hooks</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> register_gradient_hooks(<span class="va">self</span>):</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Register hooks to shard gradients during backward"""</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> param <span class="kw">in</span> <span class="va">self</span>.local_params:</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>            hook <span class="op">=</span> <span class="kw">lambda</span> grad: grad      <span class="co"># Keep gradient</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>            hook <span class="op">=</span> <span class="kw">lambda</span> grad: <span class="va">None</span>      <span class="co"># Discard gradient</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>        handle <span class="op">=</span> param.register_hook(hook)</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.grad_hooks[param] <span class="op">=</span> handle</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Lifecycle:</strong> These hooks fire <strong>during backward pass</strong>, immediately after each parameter‚Äôs gradient is computed.</p>
</section>
<section id="reduce-scatter-for-gradients" class="level4">
<h4 class="anchored" data-anchor-id="reduce-scatter-for-gradients">6.3.3 <strong>Reduce-Scatter for Gradients</strong></h4>
<p>ZeRO-2‚Äôs step function is more complex:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> step(<span class="va">self</span>, closure<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    step_start <span class="op">=</span> time.perf_counter()</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    comm_start <span class="op">=</span> step_start</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, param <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.params):</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">=</span> param.grad</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> grad <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>        flattened_grad <span class="op">=</span> grad.data.contiguous().view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Build input: each rank contributes its gradient</span></span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>        in_tensor <span class="op">=</span> torch.cat([flattened_grad</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>                               <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(get(<span class="st">"ws"</span>))], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>        output_tensor <span class="op">=</span> torch.empty_like(flattened_grad)</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>        dist.reduce_scatter_tensor(output_tensor, in_tensor,</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>                                  op<span class="op">=</span>dist.ReduceOp.SUM)</span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Keep only gradients for local parameters</span></span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="kw">in</span> <span class="va">self</span>.local_param_indices:</span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a>            param.grad.data <span class="op">=</span> (output_tensor <span class="op">/</span> get(<span class="st">"ws"</span>)).view_as(grad.data)</span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a>            param.grad <span class="op">=</span> <span class="va">None</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What‚Äôs reduce-scatter?</strong></p>
<p>Imagine 2 GPUs, parameter P with gradient G:</p>
<ol type="1">
<li>GPU 0 has: <code>[G0_chunk0, G0_chunk1]</code></li>
<li>GPU 1 has: <code>[G1_chunk0, G1_chunk1]</code></li>
</ol>
<p>After reduce-scatter:</p>
<ol type="1">
<li>GPU 0 gets: <code>(G0_chunk0 + G1_chunk0) / 2</code></li>
<li>GPU 1 gets: <code>(G0_chunk1 + G1_chunk1) / 2</code></li>
</ol>
<p>Each GPU receives only <strong>its shard</strong> of the averaged gradient!</p>
</section>
<section id="why-48.6-communication-overhead" class="level4">
<h4 class="anchored" data-anchor-id="why-48.6-communication-overhead">6.3.4 <strong>Why 48.6% Communication Overhead?</strong></h4>
<p>From <code>zero2.py:86-133</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Reduce-scatter for EVERY parameter</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, param <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.params):</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... reduce_scatter_tensor ...</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Then broadcast updated parameters</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, p <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.params):</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>    dist.broadcast(p.data, src<span class="op">=</span>owner_rank)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>With our small model (6 layers, 2 GPUs), communication dominates. Large models have higher compute-to-communication ratio.</p>
<hr>
</section>
</section>
<section id="zero-3-full-parameter-sharding" class="level3">
<h3 class="anchored" data-anchor-id="zero-3-full-parameter-sharding">6.4 ZeRO-3: Full Parameter Sharding</h3>
<p><strong>File:</strong> <code>zero3.py:23-76</code></p>
<p>ZeRO-3 is the most complex stage, requiring <strong>parameter lifecycle management</strong>.</p>
<section id="the-zero3parammanager" class="level4">
<h4 class="anchored" data-anchor-id="the-zero3parammanager">6.4.1 <strong>The Zero3ParamManager</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Zero3ParamManager:</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Tracks a parameter shard and gathers/releases full weight"""</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, param, shard_idx, world_size, shard_dim<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.param <span class="op">=</span> param</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.shard_idx <span class="op">=</span> shard_idx</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.world_size <span class="op">=</span> world_size</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.shard_dim <span class="op">=</span> shard_dim</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.full_data <span class="op">=</span> <span class="va">None</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Each parameter has a manager that controls when it‚Äôs materialized (full) vs.&nbsp;sharded.</p>
</section>
<section id="materialize-gathering-shards" class="level4">
<h4 class="anchored" data-anchor-id="materialize-gathering-shards">6.4.2 <strong>Materialize: Gathering Shards</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> materialize(<span class="va">self</span>):</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Gather full parameter from all shards"""</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    local_shard <span class="op">=</span> <span class="va">self</span>.param.data.contiguous()</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Allocate space for all shards</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    global_shards <span class="op">=</span> [torch.empty_like(local_shard)</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>                     <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(get(<span class="st">'ws'</span>))]</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># All-gather: collect shards from all GPUs</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>    dist.all_gather(global_shards, local_shard)</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Concatenate into full parameter</span></span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.full_data <span class="op">=</span> torch.cat(global_shards, dim<span class="op">=</span><span class="va">self</span>.shard_dim)</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.param.data <span class="op">=</span> <span class="va">self</span>.full_data</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Example:</strong> Linear layer weight [10000, 10000] on 2 GPUs</p>
<ul>
<li>GPU 0 holds: rows 0-4999 (shard)</li>
<li>GPU 1 holds: rows 5000-9999 (shard)</li>
<li>After materialize: Both GPUs have full [10000, 10000] weight</li>
</ul>
</section>
<section id="release-keeping-only-local-shard" class="level4">
<h4 class="anchored" data-anchor-id="release-keeping-only-local-shard">6.4.3 <strong>Release: Keeping Only Local Shard</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> release(<span class="va">self</span>):</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Keep only local shard"""</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Split full parameter into shards</span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>    shards <span class="op">=</span> <span class="va">self</span>.param.data.chunk(get(<span class="st">'ws'</span>), dim<span class="op">=</span><span class="va">self</span>.shard_dim)</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Keep only our shard</span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>    local_shard <span class="op">=</span> shards[get(<span class="st">'rank'</span>)].contiguous()</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.param.data <span class="op">=</span> local_shard</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Handle gradients too</span></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="va">self</span>.param.grad <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> <span class="op">\</span></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>       <span class="va">self</span>.param.grad.shape <span class="op">!=</span> local_shard.shape:</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>        grad_shards <span class="op">=</span> <span class="va">self</span>.param.grad.data.chunk(get(<span class="st">'ws'</span>),</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>                                                 dim<span class="op">=</span><span class="va">self</span>.shard_dim)</span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>        local_grad <span class="op">=</span> grad_shards[get(<span class="st">'rank'</span>)].contiguous()</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.param.grad.data <span class="op">=</span> local_grad</span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.full_data <span class="op">=</span> <span class="va">None</span>  <span class="co"># Free memory!</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Memory magic:</strong> <code>self.full_data = None</code> triggers garbage collection, freeing the full parameter immediately.</p>
</section>
<section id="forward-and-backward-hooks" class="level4">
<h4 class="anchored" data-anchor-id="forward-and-backward-hooks">6.4.4 <strong>Forward and Backward Hooks</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> register_zero3_hooks(model, param_managers):</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Attach hooks to modules for automatic gather/release"""</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> pre_hook(module, inputs):</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Before forward: materialize parameters</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _, param <span class="kw">in</span> module.named_parameters(recurse<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>            manager <span class="op">=</span> param_managers.get(param)</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> manager <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>                manager.materialize()</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> post_hook(module, inputs, outputs):</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># After forward: release parameters</span></span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _, param <span class="kw">in</span> module.named_parameters(recurse<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>            manager <span class="op">=</span> param_managers.get(param)</span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> manager <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>                manager.release()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Lifecycle visualization:</strong></p>
<pre><code>Forward Pass:
    Layer 1 pre_hook  ‚Üí materialize ‚Üí compute ‚Üí post_hook ‚Üí release
    Layer 2 pre_hook  ‚Üí materialize ‚Üí compute ‚Üí post_hook ‚Üí release
    ...
    Layer 6 pre_hook  ‚Üí materialize ‚Üí compute ‚Üí post_hook ‚Üí release

Backward Pass (reverse order):
    Layer 6 pre_hook  ‚Üí materialize ‚Üí compute grads ‚Üí post_hook ‚Üí release
    ...
    Layer 1 pre_hook  ‚Üí materialize ‚Üí compute grads ‚Üí post_hook ‚Üí release</code></pre>
<p><strong>Key insight:</strong> At any moment, only <strong>one layer‚Äôs parameters</strong> are materialized! This is why ZeRO-3 achieves 56.34% reduction (better than theory).</p>
</section>
<section id="parameter-initialization-with-shards" class="level4">
<h4 class="anchored" data-anchor-id="parameter-initialization-with-shards">6.4.5 <strong>Parameter Initialization with Shards</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># zero3.py:100-108</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="va">self</span>.param_managers <span class="op">=</span> {}</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> param <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>    shard_dim <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Split parameter into shards immediately</span></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>    chunks <span class="op">=</span> param.data.chunk(get(<span class="st">'ws'</span>), dim<span class="op">=</span>shard_dim)</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>    local_shard <span class="op">=</span> chunks[get(<span class="st">'rank'</span>)].contiguous()</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Replace full parameter with shard</span></span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>    param.data <span class="op">=</span> local_shard</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create manager to handle lifecycle</span></span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.param_managers[param] <span class="op">=</span> Zero3ParamManager(</span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>        param, get(<span class="st">'rank'</span>), get(<span class="st">'ws'</span>), shard_dim</span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Critical:</strong> We <strong>immediately</strong> replace <code>param.data</code> with the shard. From this point on, parameters are sharded until materialized.</p>
</section>
<section id="gradient-all-reduce" class="level4">
<h4 class="anchored" data-anchor-id="gradient-all-reduce">6.4.6 <strong>Gradient All-Reduce</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> step(<span class="va">self</span>, closure<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>    step_start <span class="op">=</span> time.perf_counter()</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    comm_start <span class="op">=</span> step_start</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, param <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.params):</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">=</span> param.grad</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> grad <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>        manager <span class="op">=</span> <span class="va">self</span>.param_managers[param]</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>        shard_dim <span class="op">=</span> manager.shard_dim</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If gradient is full-sized, shard it</span></span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> grad.shape <span class="op">!=</span> param.data.shape:</span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>            chunks <span class="op">=</span> grad.data.chunk(get(<span class="st">'ws'</span>), dim<span class="op">=</span>shard_dim)</span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a>            grad <span class="op">=</span> chunks[get(<span class="st">'rank'</span>)].contiguous()</span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># All-reduce to average shards</span></span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a>        dist.all_reduce(grad, op<span class="op">=</span>dist.ReduceOp.SUM)</span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">/=</span> get(<span class="st">'ws'</span>)</span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Assign averaged gradient to local parameters only</span></span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="kw">in</span> <span class="va">self</span>.local_param_indices:</span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a>            param.grad <span class="op">=</span> grad</span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a>            param.grad <span class="op">=</span> <span class="va">None</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Why all-reduce instead of reduce-scatter?</strong> Since parameters are already sharded, we just need to average the gradient shards across GPUs.</p>
<hr>
</section>
</section>
<section id="memory-tracking-utilities" class="level3">
<h3 class="anchored" data-anchor-id="memory-tracking-utilities">6.5 Memory Tracking Utilities</h3>
<p><strong>File:</strong> <code>training_utils/memory.py</code></p>
<section id="calculating-memory-usage" class="level4">
<h4 class="anchored" data-anchor-id="calculating-memory-usage">6.5.1 <strong>Calculating Memory Usage</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_size_in_mb(tensor):</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Get size of tensor in MB"""</span></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> tensor <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">0</span></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tensor.element_size() <span class="op">*</span> tensor.nelement() <span class="op">/</span> <span class="dv">1024</span><span class="op">**</span><span class="dv">2</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Breakdown:</strong></p>
<ul>
<li><code>element_size()</code>: Bytes per element (2 for fp16, 4 for fp32)</li>
<li><code>nelement()</code>: Total number of elements</li>
<li>Division by 1024¬≤ converts bytes to MB</li>
</ul>
</section>
<section id="optimizer-state-memory" class="level4">
<h4 class="anchored" data-anchor-id="optimizer-state-memory">6.5.2 <strong>Optimizer State Memory</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_optimizer_memory(optimizer):</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Calculate total memory used by optimizer states"""</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>    total_memory <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Handle wrapped optimizers (ShardedOptimizer)</span></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">hasattr</span>(optimizer, <span class="st">"optimizer"</span>):</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>        optimizer <span class="op">=</span> optimizer.optimizer</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Adam stores momentum and variance for each parameter</span></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> state <span class="kw">in</span> optimizer.state.values():</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> state_tensor <span class="kw">in</span> state.values():</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> torch.is_tensor(state_tensor):</span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>                total_memory <span class="op">+=</span> get_size_in_mb(state_tensor)</span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> total_memory</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Example:</strong> For 2.3B parameters with Adam:</p>
<ul>
<li><code>optimizer.state</code> contains <code>{param_id: {'exp_avg': tensor, 'exp_avg_sq': tensor}}</code></li>
<li>Each state tensor is 2.3B √ó 4 bytes (fp32) = 9.2 GB</li>
<li>Total optimizer memory: 2 √ó 9.2 GB = 18.4 GB</li>
</ul>
</section>
<section id="complete-memory-report" class="level4">
<h4 class="anchored" data-anchor-id="complete-memory-report">6.5.3 <strong>Complete Memory Report</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> print_memory_stats(prefix: <span class="bu">str</span>, model, optimizer, rank, device):</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>    model_memory <span class="op">=</span> get_model_memory(model)</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>    grad_memory <span class="op">=</span> get_gradient_memory(model)</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>    optim_memory <span class="op">=</span> get_optimizer_memory(optimizer)</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>    total_allocated <span class="op">=</span> torch.cuda.memory_allocated(device) <span class="op">/</span> <span class="dv">1024</span><span class="op">**</span><span class="dv">2</span></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>    max_allocated <span class="op">=</span> torch.cuda.max_memory_allocated(device) <span class="op">/</span> <span class="dv">1024</span><span class="op">**</span><span class="dv">2</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">GPU </span><span class="sc">{</span>rank<span class="sc">}</span><span class="ss"> - </span><span class="sc">{</span>prefix<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Model parameters: </span><span class="sc">{</span>model_memory<span class="sc">:.2f}</span><span class="ss"> MB"</span>)</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Gradients: </span><span class="sc">{</span>grad_memory<span class="sc">:.2f}</span><span class="ss"> MB"</span>)</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Optimizer states: </span><span class="sc">{</span>optim_memory<span class="sc">:.2f}</span><span class="ss"> MB"</span>)</span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Total allocated: </span><span class="sc">{</span>total_allocated<span class="sc">:.2f}</span><span class="ss"> MB"</span>)</span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Max allocated: </span><span class="sc">{</span>max_allocated<span class="sc">:.2f}</span><span class="ss"> MB"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This generates the ‚ÄúInitial state‚Äù output we saw in <code>output_log.txt</code>:</p>
<pre><code>GPU 0 - Initial state:
  Model parameters: 2289.05 MB
  Gradients: 1144.52 MB      ‚Üê ZeRO-2 sharded!
  Optimizer states: 2289.05 MB  ‚Üê ZeRO-1 sharded!
  Total allocated: 5797.49 MB
  Max allocated: 6943.23 MB</code></pre>
<hr>
</section>
</section>
<section id="distributed-training-helpers" class="level3">
<h3 class="anchored" data-anchor-id="distributed-training-helpers">6.6 Distributed Training Helpers</h3>
<p><strong>File:</strong> <code>training_utils/utils.py:24-80</code></p>
<section id="reproducibility" class="level4">
<h4 class="anchored" data-anchor-id="reproducibility">6.6.1 <strong>Reproducibility</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> set_seed(seed: <span class="bu">int</span> <span class="op">=</span> <span class="dv">42</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Sets random seed for reproducibility"""</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> random</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> torch</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>    random.seed(seed)</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>    np.random.seed(seed)</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>    torch.manual_seed(seed)</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>    torch.cuda.manual_seed_all(seed)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Why crucial?</strong> In distributed training, all GPUs must:</p>
<ol type="1">
<li>Initialize model weights identically</li>
<li>Generate the same random data (for this demo)</li>
<li>Produce identical results (for validation)</li>
</ol>
</section>
<section id="distributed-context-helper" class="level4">
<h4 class="anchored" data-anchor-id="distributed-context-helper">6.6.2 <strong>Distributed Context Helper</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="at">@cache_mesh</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get(<span class="bu">str</span>, dm: dist.device_mesh.DeviceMesh <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Convenience function to get distributed context info.</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="co">    'ws' ‚Üí world_size (number of GPUs)</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a><span class="co">    'rank' ‚Üí current GPU ID (0 to ws-1)</span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a><span class="co">    'pg' ‚Üí process group</span></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a><span class="co">    'lrank' ‚Üí local rank within node</span></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>    pg <span class="op">=</span> dm.get_group() <span class="cf">if</span> dm <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">match</span> <span class="bu">str</span>:</span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">case</span> <span class="st">"ws"</span>:</span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> dist.get_world_size(pg)</span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">case</span> <span class="st">"pg"</span>:</span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> pg</span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">case</span> <span class="st">"rank"</span> <span class="op">|</span> <span class="st">"grank"</span>:</span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> dist.get_rank(pg)</span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">case</span> <span class="st">"lrank"</span>:</span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> dm.get_local_rank() <span class="cf">if</span> dm <span class="cf">else</span> <span class="op">\</span></span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a>                   <span class="bu">int</span>(os.environ.get(<span class="st">"LOCAL_RANK"</span>, <span class="dv">0</span>))</span>
<span id="cb54-23"><a href="#cb54-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-24"><a href="#cb54-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">case</span> _:</span>
<span id="cb54-25"><a href="#cb54-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Invalid string: </span><span class="sc">{</span><span class="bu">str</span><span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Usage throughout codebase:</strong></p>
<ul>
<li><code>get('ws')</code> instead of <code>dist.get_world_size()</code></li>
<li><code>get('rank')</code> instead of <code>dist.get_rank()</code></li>
</ul>
<p>Makes code cleaner and handles process groups automatically.</p>
<hr>
</section>
</section>
<section id="training-loop-anatomy" class="level3">
<h3 class="anchored" data-anchor-id="training-loop-anatomy">6.7 Training Loop Anatomy</h3>
<p>Let‚Äôs examine the complete training loop (using <code>zero1.py:90-180</code> as reference):</p>
<section id="setup-phase" class="level4">
<h4 class="anchored" data-anchor-id="setup-phase">6.7.1 <strong>Setup Phase</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(model, optimizer, device, is_sharded<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>          profiler_context<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>    rank <span class="op">=</span> get(<span class="st">"rank"</span>)</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate dummy data</span></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.randn(batch_size, <span class="dv">10000</span>, device<span class="op">=</span>device)</span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> torch.randn(batch_size, <span class="dv">10000</span>, device<span class="op">=</span>device)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Note:</strong> We use synthetic data for reproducibility. Real training would load from DataLoader.</p>
</section>
<section id="warmup-step" class="level4">
<h4 class="anchored" data-anchor-id="warmup-step">6.7.2 <strong>Warmup Step</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Warmup step to avoid first-step overhead</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> model(x)</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> nn.functional.mse_loss(output, y)</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>    torch.cuda.synchronize()</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reset timers after warmup</span></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> is_sharded:</span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>        optimizer.communication_time <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>        optimizer.step_time <span class="op">=</span> <span class="fl">0.0</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Why warmup?</strong> First CUDA operations trigger:</p>
<ul>
<li>Kernel compilation</li>
<li>cuBLAS/cuDNN initialization</li>
<li>Memory pool allocation</li>
</ul>
<p>Warmup ensures timing measurements reflect steady-state performance.</p>
</section>
<section id="memory-profiling-loop" class="level4">
<h4 class="anchored" data-anchor-id="memory-profiling-loop">6.7.3 <strong>Memory Profiling Loop</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>    peak_memories <span class="op">=</span> []</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>    num_steps <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_steps):</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>        torch.cuda.reset_peak_memory_stats(device)</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> record_function(<span class="st">"zero_grad"</span>):</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> record_function(<span class="st">"forward"</span>):</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> model(x)</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> nn.functional.mse_loss(output, y)</span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Print memory before backward (first step only)</span></span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> rank <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Step </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> memory:"</span>)</span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Before backward: "</span></span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a>                  <span class="ss">f"</span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>memory_allocated(device)<span class="op">/</span><span class="dv">1024</span><span class="op">**</span><span class="dv">2</span><span class="sc">:.2f}</span><span class="ss"> MB"</span>)</span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> record_function(<span class="st">"backward"</span>):</span>
<span id="cb57-21"><a href="#cb57-21" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb57-22"><a href="#cb57-22" aria-hidden="true" tabindex="-1"></a>            torch.cuda.synchronize()</span>
<span id="cb57-23"><a href="#cb57-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-24"><a href="#cb57-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Print gradient memory after backward</span></span>
<span id="cb57-25"><a href="#cb57-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> rank <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb57-26"><a href="#cb57-26" aria-hidden="true" tabindex="-1"></a>            grad_memory <span class="op">=</span> <span class="bu">sum</span>(p.grad.numel() <span class="op">*</span> p.grad.element_size() <span class="op">/</span> <span class="dv">1024</span><span class="op">**</span><span class="dv">2</span></span>
<span id="cb57-27"><a href="#cb57-27" aria-hidden="true" tabindex="-1"></a>                             <span class="cf">for</span> p <span class="kw">in</span> model.parameters()</span>
<span id="cb57-28"><a href="#cb57-28" aria-hidden="true" tabindex="-1"></a>                             <span class="cf">if</span> p.grad <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>)</span>
<span id="cb57-29"><a href="#cb57-29" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Gradient memory after backward: </span><span class="sc">{</span>grad_memory<span class="sc">:.2f}</span><span class="ss"> MB"</span>)</span>
<span id="cb57-30"><a href="#cb57-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-31"><a href="#cb57-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> record_function(<span class="st">"optimizer_step_total"</span>):</span>
<span id="cb57-32"><a href="#cb57-32" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb57-33"><a href="#cb57-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-34"><a href="#cb57-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> profiler_context:</span>
<span id="cb57-35"><a href="#cb57-35" aria-hidden="true" tabindex="-1"></a>            profiler_context.step()  <span class="co"># Advance profiler</span></span>
<span id="cb57-36"><a href="#cb57-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-37"><a href="#cb57-37" aria-hidden="true" tabindex="-1"></a>        current_peak <span class="op">=</span> torch.cuda.max_memory_allocated(device) <span class="op">/</span> <span class="dv">1024</span><span class="op">**</span><span class="dv">2</span></span>
<span id="cb57-38"><a href="#cb57-38" aria-hidden="true" tabindex="-1"></a>        peak_memories.append(current_peak)</span>
<span id="cb57-39"><a href="#cb57-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-40"><a href="#cb57-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> rank <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb57-41"><a href="#cb57-41" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Peak memory this step: </span><span class="sc">{</span>current_peak<span class="sc">:.2f}</span><span class="ss"> MB"</span>)</span>
<span id="cb57-42"><a href="#cb57-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-43"><a href="#cb57-43" aria-hidden="true" tabindex="-1"></a>        dist.barrier()  <span class="co"># Synchronize all GPUs</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Key techniques:</strong></p>
<ol type="1">
<li><code>torch.cuda.reset_peak_memory_stats()</code> clears previous peak before each step</li>
<li><code>torch.cuda.synchronize()</code> ensures CUDA operations complete before measuring</li>
<li><code>record_function()</code> creates profiler scopes visible in TensorBoard</li>
<li><code>dist.barrier()</code> prevents GPU drift (one GPU racing ahead)</li>
</ol>
</section>
<section id="results-reporting" class="level4">
<h4 class="anchored" data-anchor-id="results-reporting">6.7.4 <strong>Results Reporting</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> rank <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Final peak memory: </span><span class="sc">{</span><span class="bu">max</span>(peak_memories)<span class="sc">:.2f}</span><span class="ss"> MB"</span>)</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Timing statistics</span></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> is_sharded <span class="kw">and</span> rank <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>        avg_step_time <span class="op">=</span> optimizer.step_time <span class="op">/</span> num_steps</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>        avg_comm_time <span class="op">=</span> optimizer.communication_time <span class="op">/</span> num_steps</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Timing and Communication Stats:"</span>)</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">40</span>)</span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Average step time: </span><span class="sc">{</span>avg_step_time<span class="sc">:.3f}</span><span class="ss">s"</span>)</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Average communication time: </span><span class="sc">{</span>avg_comm_time<span class="sc">:.3f}</span><span class="ss">s"</span>)</span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Average compute time: </span><span class="sc">{</span>avg_step_time <span class="op">-</span> avg_comm_time<span class="sc">:.3f}</span><span class="ss">s"</span>)</span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Communication overhead: "</span></span>
<span id="cb58-14"><a href="#cb58-14" aria-hidden="true" tabindex="-1"></a>              <span class="ss">f"</span><span class="sc">{</span>(avg_comm_time<span class="op">/</span>avg_step_time)<span class="op">*</span><span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">%"</span>)</span>
<span id="cb58-15"><a href="#cb58-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-16"><a href="#cb58-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model, optimizer, <span class="bu">max</span>(peak_memories)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Output matching <code>output_log.txt</code>:</strong></p>
<pre><code>Average step time: 0.029s
Average communication time: 0.014s
Average compute time: 0.015s
Communication overhead: 48.6%</code></pre>
<hr>
</section>
</section>
<section id="key-implementation-patterns" class="level3">
<h3 class="anchored" data-anchor-id="key-implementation-patterns">6.8 Key Implementation Patterns</h3>
</section>
<section id="pattern-1-wrapping-native-optimizers" class="level3">
<h3 class="anchored" data-anchor-id="pattern-1-wrapping-native-optimizers">Pattern 1: Wrapping Native Optimizers</h3>
<p>All three ZeRO stages wrap PyTorch‚Äôs Adam:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>base_optimizer <span class="op">=</span> Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>sharded_optimizer <span class="op">=</span> ShardedOptimizer(base_optimizer)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Benefit:</strong> Compatible with any PyTorch optimizer! Just swap <code>Adam</code> for <code>SGD</code>, <code>AdamW</code>, etc.</p>
</section>
<section id="pattern-2-lazy-materialization-zero-3" class="level3">
<h3 class="anchored" data-anchor-id="pattern-2-lazy-materialization-zero-3">Pattern 2: Lazy Materialization (ZeRO-3)</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters start sharded</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>param.data <span class="op">=</span> local_shard</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Materialize only when needed (pre_hook)</span></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>manager.materialize()  <span class="co"># param.data ‚Üí full_data</span></span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Release immediately after use (post_hook)</span></span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>manager.release()      <span class="co"># param.data ‚Üí local_shard</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>This is the secret sauce</strong> enabling ZeRO-3‚Äôs superior memory efficiency.</p>
</section>
<section id="pattern-3-communication-timing" class="level3">
<h3 class="anchored" data-anchor-id="pattern-3-communication-timing">Pattern 3: Communication Timing</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> step(<span class="va">self</span>):</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>    step_start <span class="op">=</span> time.perf_counter()</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>    comm_start <span class="op">=</span> step_start</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... communication code ...</span></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>    torch.cuda.synchronize()</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.communication_time <span class="op">+=</span> time.perf_counter() <span class="op">-</span> comm_start</span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... compute code ...</span></span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a>    torch.cuda.synchronize()</span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.step_time <span class="op">+=</span> time.perf_counter() <span class="op">-</span> step_start</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Essential for profiling:</strong> Separating communication time from total step time reveals overhead.</p>
</section>
<section id="pattern-4-gradient-hooks-for-memory-management" class="level3">
<h3 class="anchored" data-anchor-id="pattern-4-gradient-hooks-for-memory-management">Pattern 4: Gradient Hooks for Memory Management</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Register hook during initialization</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>handle <span class="op">=</span> param.register_hook(<span class="kw">lambda</span> grad: <span class="va">None</span> <span class="cf">if</span> non_local <span class="cf">else</span> grad)</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Hook fires automatically during backward</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>loss.backward()  <span class="co"># Triggers hooks as gradients are computed</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Elegant solution:</strong> No need to manually delete gradients‚Äîhooks do it automatically!</p>
<hr>
</section>
<section id="common-pitfalls-and-solutions" class="level3">
<h3 class="anchored" data-anchor-id="common-pitfalls-and-solutions">6.9 Common Pitfalls and Solutions</h3>
</section>
<section id="pitfall-1-forgetting-torch.cuda.synchronize" class="level3">
<h3 class="anchored" data-anchor-id="pitfall-1-forgetting-torch.cuda.synchronize">Pitfall 1: Forgetting torch.cuda.synchronize()</h3>
<p><strong>Problem:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>dist.all_reduce(tensor)</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>elapsed <span class="op">=</span> time.time() <span class="op">-</span> start  <span class="co"># Wrong! CUDA operations are async</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Solution:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>dist.all_reduce(tensor)</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>torch.cuda.synchronize()  <span class="co"># Wait for completion</span></span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>elapsed <span class="op">=</span> time.time() <span class="op">-</span> start  <span class="co"># Correct timing</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="pitfall-2-hooks-with-lambda-closures" class="level3">
<h3 class="anchored" data-anchor-id="pitfall-2-hooks-with-lambda-closures">Pitfall 2: Hooks with Lambda Closures</h3>
<p><strong>Problem:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> param <span class="kw">in</span> params:</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>    hook <span class="op">=</span> <span class="kw">lambda</span> grad: process(param)  <span class="co"># Bug! All hooks use last param</span></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>    param.register_hook(hook)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Solution:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> param <span class="kw">in</span> params:</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Capture param in closure correctly</span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>    hook <span class="op">=</span> (<span class="kw">lambda</span> p: <span class="kw">lambda</span> grad: process(p))(param)</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>    param.register_hook(hook)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Our code uses this pattern in <code>zero2.py:73-84</code>.</p>
</section>
<section id="pitfall-3-materialize-without-release-zero-3" class="level3">
<h3 class="anchored" data-anchor-id="pitfall-3-materialize-without-release-zero-3">Pitfall 3: Materialize Without Release (ZeRO-3)</h3>
<p><strong>Problem:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pre_hook(module, inputs):</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>    manager.materialize()  <span class="co"># Memory leak! Never released</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Solution:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pre_hook(module, inputs):</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>    manager.materialize()</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> post_hook(module, inputs, outputs):</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>    manager.release()  <span class="co"># Always pair materialize with release</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="pitfall-4-incorrect-shard-ownership-calculation" class="level3">
<h3 class="anchored" data-anchor-id="pitfall-4-incorrect-shard-ownership-calculation">Pitfall 4: Incorrect Shard Ownership Calculation</h3>
<p><strong>Problem:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Naive sharding</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>owner_rank <span class="op">=</span> param_idx <span class="op">//</span> params_per_rank  <span class="co"># Fails with remainders!</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Solution (from <code>zero1.py:76-79</code>):</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> i <span class="op">&lt;</span> (params_per_rank <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> remainder:</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>    owner_rank <span class="op">=</span> i <span class="op">//</span> (params_per_rank <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>    owner_rank <span class="op">=</span> (i <span class="op">-</span> remainder) <span class="op">//</span> params_per_rank</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Handles uneven parameter distribution correctly.</p>
<hr>
</section>
<section id="code-comparison-across-zero-stages" class="level3">
<h3 class="anchored" data-anchor-id="code-comparison-across-zero-stages">6.10 Code Comparison Across ZeRO Stages</h3>
<p>Let‚Äôs compare the three stages side-by-side:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>ZeRO-1</th>
<th>ZeRO-2</th>
<th>ZeRO-3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Optimizer sharding</strong></td>
<td>‚úÖ Yes</td>
<td>‚úÖ Yes</td>
<td>‚úÖ Yes</td>
</tr>
<tr class="even">
<td><strong>Gradient hooks</strong></td>
<td>‚ùå No</td>
<td>‚úÖ Yes (<code>Zero2Hook</code>)</td>
<td>‚úÖ Yes (implicit)</td>
</tr>
<tr class="odd">
<td><strong>Parameter managers</strong></td>
<td>‚ùå No</td>
<td>‚ùå No</td>
<td>‚úÖ Yes (<code>Zero3ParamManager</code>)</td>
</tr>
<tr class="even">
<td><strong>Forward/backward hooks</strong></td>
<td>‚ùå No</td>
<td>‚ùå No</td>
<td>‚úÖ Yes (<code>register_zero3_hooks</code>)</td>
</tr>
<tr class="odd">
<td><strong>Gradient communication</strong></td>
<td>All-reduce (full)</td>
<td>Reduce-scatter (sharded)</td>
<td>All-reduce (sharded)</td>
</tr>
<tr class="even">
<td><strong>Parameter communication</strong></td>
<td>Broadcast (full)</td>
<td>Broadcast (full)</td>
<td>None (all-gather in hooks)</td>
</tr>
<tr class="odd">
<td><strong>Code complexity</strong></td>
<td>88 lines</td>
<td>138 lines</td>
<td>223 lines</td>
</tr>
<tr class="even">
<td><strong>Memory savings</strong></td>
<td>29.82%</td>
<td>26.53%</td>
<td>56.34%</td>
</tr>
</tbody>
</table>
<p><strong>Takeaway:</strong> Complexity increases with memory savings, but the patterns remain consistent.</p>
<hr>
</section>
<section id="extending-the-code" class="level3">
<h3 class="anchored" data-anchor-id="extending-the-code">6.11 Extending the Code</h3>
</section>
<section id="extension-1-activation-checkpointing" class="level3">
<h3 class="anchored" data-anchor-id="extension-1-activation-checkpointing">Extension 1: Activation Checkpointing</h3>
<p>Combine ZeRO with gradient checkpointing for even more memory savings:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.checkpoint <span class="im">import</span> checkpoint</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Wrap layers in checkpointing</span></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CheckpointedModel(nn.Module):</span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">10000</span>, <span class="dv">10000</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">6</span>)</span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Recompute activations during backward</span></span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> checkpoint(layer, x, use_reentrant<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb72-15"><a href="#cb72-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Expected savings:</strong> Combine ZeRO-3‚Äôs 56% with checkpointing‚Äôs ~‚àöN reduction.</p>
</section>
<section id="extension-2-mixed-precision-training" class="level3">
<h3 class="anchored" data-anchor-id="extension-2-mixed-precision-training">Extension 2: Mixed Precision Training</h3>
<p>Integrate AMP (Automatic Mixed Precision):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.cuda.amp <span class="im">import</span> autocast, GradScaler</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> GradScaler()</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> autocast():</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> model(x)</span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> criterion(output, y)</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>scaler.scale(loss).backward()</span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>scaler.step(optimizer)</span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>scaler.update()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Benefit:</strong> Reduces parameter memory from 4Œ® (fp32) to 2Œ® (fp16), doubling model size capacity.</p>
</section>
<section id="extension-3-offloading-to-cpu" class="level3">
<h3 class="anchored" data-anchor-id="extension-3-offloading-to-cpu">Extension 3: Offloading to CPU</h3>
<p>For massive models, offload optimizer states to CPU:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># After optimizer step</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> state <span class="kw">in</span> optimizer.state.values():</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> state.items():</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> torch.is_tensor(v):</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>            state[k] <span class="op">=</span> v.cpu()  <span class="co"># Offload to CPU</span></span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Before next step</span></span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> state <span class="kw">in</span> optimizer.state.values():</span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> state.items():</span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> torch.is_tensor(v):</span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a>            state[k] <span class="op">=</span> v.cuda()  <span class="co"># Bring back to GPU</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Use case:</strong> Trading speed for memory when GPU memory is exhausted.</p>
<hr>
</section>
<section id="performance-optimization-tips" class="level3">
<h3 class="anchored" data-anchor-id="performance-optimization-tips">6.12 Performance Optimization Tips</h3>
</section>
<section id="tip-1-overlap-communication-with-computation" class="level3">
<h3 class="anchored" data-anchor-id="tip-1-overlap-communication-with-computation">Tip 1: Overlap Communication with Computation</h3>
<p>Current implementation:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sequential</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>all_reduce_gradients()</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>optimizer_step()</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>broadcast_parameters()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Optimized version:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Overlap using async operations</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>handle <span class="op">=</span> dist.all_reduce(grad, async_op<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ... other computations ...</span></span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>handle.wait()  <span class="co"># Wait when result is needed</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Expected improvement:</strong> 10-30% faster for large models.</p>
</section>
<section id="tip-2-fused-optimizers" class="level3">
<h3 class="anchored" data-anchor-id="tip-2-fused-optimizers">Tip 2: Fused Optimizers</h3>
<p>Use fused Adam from NVIDIA Apex:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> apex.optimizers <span class="im">import</span> FusedAdam</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> FusedAdam(model.parameters())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Benefit:</strong> Kernel fusion reduces memory bandwidth requirements.</p>
</section>
<section id="tip-3-bucketing-gradients" class="level3">
<h3 class="anchored" data-anchor-id="tip-3-bucketing-gradients">Tip 3: Bucketing Gradients</h3>
<p>Instead of all-reducing each parameter individually, bucket them:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Group small parameters into buckets</span></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>BUCKET_SIZE_MB <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>buckets <span class="op">=</span> []</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>current_bucket <span class="op">=</span> []</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>current_size <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> param <span class="kw">in</span> params:</span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>    size <span class="op">=</span> param.numel() <span class="op">*</span> param.element_size() <span class="op">/</span> <span class="dv">1024</span><span class="op">**</span><span class="dv">2</span></span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> current_size <span class="op">+</span> size <span class="op">&gt;</span> BUCKET_SIZE_MB:</span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a>        buckets.append(current_bucket)</span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>        current_bucket <span class="op">=</span> [param]</span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>        current_size <span class="op">=</span> size</span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a>        current_bucket.append(param)</span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a>        current_size <span class="op">+=</span> size</span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-18"><a href="#cb78-18" aria-hidden="true" tabindex="-1"></a><span class="co"># All-reduce buckets instead of individual params</span></span>
<span id="cb78-19"><a href="#cb78-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> bucket <span class="kw">in</span> buckets:</span>
<span id="cb78-20"><a href="#cb78-20" aria-hidden="true" tabindex="-1"></a>    flat <span class="op">=</span> torch.cat([p.grad.flatten() <span class="cf">for</span> p <span class="kw">in</span> bucket])</span>
<span id="cb78-21"><a href="#cb78-21" aria-hidden="true" tabindex="-1"></a>    dist.all_reduce(flat)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>PyTorch DDP uses this</strong> for better communication efficiency.</p>
<hr>
</section>
<section id="debugging-distributed-training" class="level3">
<h3 class="anchored" data-anchor-id="debugging-distributed-training">6.13 Debugging Distributed Training</h3>
</section>
<section id="technique-1-enable-nccl-debug-logs" class="level3">
<h3 class="anchored" data-anchor-id="technique-1-enable-nccl-debug-logs">Technique 1: Enable NCCL Debug Logs</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb79"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">NCCL_DEBUG</span><span class="op">=</span>INFO</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">NCCL_DEBUG_SUBSYS</span><span class="op">=</span>ALL</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a><span class="ex">torchrun</span> <span class="at">--nproc_per_node</span><span class="op">=</span>2 zero1.py</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Output reveals:</strong></p>
<ul>
<li>Communication patterns</li>
<li>Bandwidth utilization</li>
<li>Hang locations</li>
</ul>
</section>
<section id="technique-2-rank-specific-logging" class="level3">
<h3 class="anchored" data-anchor-id="technique-2-rank-specific-logging">Technique 2: Rank-Specific Logging</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> debug_print(<span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>    rank <span class="op">=</span> get(<span class="st">'rank'</span>)</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"[Rank </span><span class="sc">{</span>rank<span class="sc">}</span><span class="ss">]"</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Usage</span></span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>debug_print(<span class="st">"Before all-reduce:"</span>, tensor.shape)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Helps identify:</strong> Which GPU has different behavior.</p>
</section>
<section id="technique-3-gradient-verification" class="level3">
<h3 class="anchored" data-anchor-id="technique-3-gradient-verification">Technique 3: Gradient Verification</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="co"># After all-reduce, check gradients match across GPUs</span></span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> param.grad <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>        gathered <span class="op">=</span> [torch.empty_like(param.grad)</span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a>                   <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(get(<span class="st">'ws'</span>))]</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a>        dist.all_gather(gathered, param.grad)</span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># All gradients should be identical</span></span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(gathered)):</span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> torch.allclose(gathered[<span class="dv">0</span>], gathered[i]):</span>
<span id="cb81-11"><a href="#cb81-11" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Gradient mismatch in </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss"> between "</span></span>
<span id="cb81-12"><a href="#cb81-12" aria-hidden="true" tabindex="-1"></a>                      <span class="ss">f"GPU 0 and GPU </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
</section>
<section id="summary-from-theory-to-practice" class="level3">
<h3 class="anchored" data-anchor-id="summary-from-theory-to-practice">6.14 Summary: From Theory to Practice</h3>
<p>This implementation deep dive revealed:</p>
<ol type="1">
<li><strong>ZeRO-1</strong> shards optimizer states by removing non-local parameters from optimizer param_groups</li>
<li><strong>ZeRO-2</strong> adds gradient sharding via hooks and reduce-scatter operations</li>
<li><strong>ZeRO-3</strong> achieves full sharding through parameter lifecycle management with materialize/release</li>
<li><strong>Memory utilities</strong> precisely track model, gradient, and optimizer state memory</li>
<li><strong>Training loop</strong> integrates profiling and synchronization for accurate measurements</li>
<li><strong>Common pitfalls</strong> like async CUDA operations and lambda closures have clear solutions</li>
<li><strong>Extensions</strong> like activation checkpointing and CPU offloading further reduce memory</li>
</ol>
<p>The code is production-ready and demonstrates that <strong>ZeRO‚Äôs sophisticated memory optimization maps cleanly to ~300 lines of PyTorch</strong>.</p>
<hr>
<p><strong>Key Files Reference:</strong></p>
<ul>
<li>ZeRO-1: <code>zero1.py:22-88</code> (ShardedOptimizer), <code>zero1.py:90-180</code> (train loop)</li>
<li>ZeRO-2: <code>zero2.py:21-34</code> (Zero2Hook), <code>zero2.py:86-133</code> (step with reduce-scatter)</li>
<li>ZeRO-3: <code>zero3.py:23-50</code> (Zero3ParamManager), <code>zero3.py:54-76</code> (hooks)</li>
<li>Memory: <code>training_utils/memory.py:8-50</code> (all utilities)</li>
<li>Distributed: <code>training_utils/utils.py:24-80</code> (get helper, set_seed)</li>
</ul>
<hr>
</section>
</section>
<section id="running-your-own-experiments" class="level2">
<h2 class="anchored" data-anchor-id="running-your-own-experiments">7. Running Your Own Experiments</h2>
<p>Now that we understand the theory and implementation, let‚Äôs get hands-on. This section provides everything you need to reproduce our results and conduct your own ZeRO experiments.</p>
<section id="prerequisites" class="level3">
<h3 class="anchored" data-anchor-id="prerequisites">7.1 Prerequisites</h3>
<section id="hardware-requirements" class="level4">
<h4 class="anchored" data-anchor-id="hardware-requirements">7.1.1 <strong>Hardware Requirements</strong></h4>
<p><strong>Minimum:</strong></p>
<ul>
<li>2 GPUs with 16GB+ VRAM each (e.g., NVIDIA V100, RTX 3090, A100)</li>
<li>32GB system RAM</li>
<li>50GB free disk space</li>
</ul>
<p><strong>Recommended for full experiments:</strong></p>
<ul>
<li>4-8 GPUs with 24GB+ VRAM each</li>
<li>64GB system RAM</li>
<li>High-bandwidth interconnect (NVLink or InfiniBand)</li>
</ul>
<p><strong>Cloud options:</strong></p>
<ul>
<li><strong>Lambda Labs</strong>: H100 instances (8x H100 80GB) - $8.80/hr</li>
<li><strong>AWS</strong>: p4d.24xlarge (8x A100 40GB) - ~$32/hr</li>
<li><strong>Google Cloud</strong>: a2-highgpu-8g (8x A100 40GB) - ~$30/hr</li>
<li><strong>Azure</strong>: NDv4 series (8x A100 40GB) - ~$27/hr</li>
</ul>
</section>
<section id="software-requirements" class="level4">
<h4 class="anchored" data-anchor-id="software-requirements">7.1.2 <strong>Software Requirements</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb82"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Operating System</span></span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a><span class="ex">Ubuntu</span> 20.04+ or equivalent Linux distribution</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a><span class="co"># (macOS and Windows WSL2 also work but with limitations)</span></span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a><span class="co"># CUDA Toolkit</span></span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a><span class="ex">CUDA</span> 11.8+ or 12.1+</span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Python</span></span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a><span class="ex">Python</span> 3.8+</span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-11"><a href="#cb82-11" aria-hidden="true" tabindex="-1"></a><span class="co"># PyTorch</span></span>
<span id="cb82-12"><a href="#cb82-12" aria-hidden="true" tabindex="-1"></a><span class="ex">torch</span> <span class="op">&gt;</span>= 2.0.0 <span class="er">(</span><span class="ex">with</span> CUDA support<span class="kw">)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="network-requirements" class="level4">
<h4 class="anchored" data-anchor-id="network-requirements">7.1.3 <strong>Network Requirements</strong></h4>
<p>For multi-node training (beyond this tutorial): - Low-latency interconnect (&lt;10 Œºs) - High bandwidth (&gt;100 Gbps recommended) - NCCL-compatible network topology</p>
<hr>
</section>
</section>
<section id="environment-setup" class="level3">
<h3 class="anchored" data-anchor-id="environment-setup">7.2 Environment Setup</h3>
<section id="clone-the-repository" class="level4">
<h4 class="anchored" data-anchor-id="clone-the-repository">7.2.1 <strong>Clone the Repository</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb83"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Clone from GitHub</span></span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> clone https://github.com/yourusername/zero-daddyofadoggy.git</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> zero-daddyofadoggy</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Or if you're following along, create the structure:</span></span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> <span class="at">-p</span> zero-daddyofadoggy/training_utils</span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> zero-daddyofadoggy</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="create-virtual-environment" class="level4">
<h4 class="anchored" data-anchor-id="create-virtual-environment">7.2.2 <strong>Create Virtual Environment</strong></h4>
<p><strong>Using venv:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb84"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> <span class="at">-m</span> venv venv</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> venv/bin/activate  <span class="co"># On Linux/macOS</span></span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a><span class="co"># On Windows:</span></span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a><span class="co"># venv\Scripts\activate</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Using conda (alternative):</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb85"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> create <span class="at">-n</span> zero python=3.10</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> activate zero</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="install-dependencies" class="level4">
<h4 class="anchored" data-anchor-id="install-dependencies">7.2.3 <strong>Install Dependencies</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb86"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Upgrade pip</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install <span class="at">--upgrade</span> pip</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Install PyTorch with CUDA support</span></span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a><span class="co"># For CUDA 11.8:</span></span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 <span class="dt">\</span></span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">--index-url</span> https://download.pytorch.org/whl/cu118</span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a><span class="co"># For CUDA 12.1:</span></span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 <span class="dt">\</span></span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">--index-url</span> https://download.pytorch.org/whl/cu121</span>
<span id="cb86-12"><a href="#cb86-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-13"><a href="#cb86-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Install remaining dependencies</span></span>
<span id="cb86-14"><a href="#cb86-14" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install <span class="at">-r</span> requirements.txt</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>requirements.txt contents:</strong></p>
<pre><code>torch&gt;=2.0.0
numpy&gt;=1.24.0
datasets&gt;=2.14.0
transformers&gt;=4.30.0
accelerate&gt;=0.20.0
tensorboard&gt;=2.13.0</code></pre>
</section>
<section id="verify-installation" class="level4">
<h4 class="anchored" data-anchor-id="verify-installation">7.2.4 <strong>Verify Installation</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb88"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check CUDA availability</span></span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> <span class="at">-c</span> <span class="st">"import torch; print(f'CUDA available: {torch.cuda.is_available()}')"</span></span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Expected: CUDA available: True</span></span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Check GPU count</span></span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> <span class="at">-c</span> <span class="st">"import torch; print(f'GPU count: {torch.cuda.device_count()}')"</span></span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Expected: GPU count: 2 (or more)</span></span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Check NCCL support</span></span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> <span class="at">-c</span> <span class="st">"import torch.distributed as dist; print(f'NCCL available: {dist.is_nccl_available()}')"</span></span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Expected: NCCL available: True</span></span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-13"><a href="#cb88-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify GPU details</span></span>
<span id="cb88-14"><a href="#cb88-14" aria-hidden="true" tabindex="-1"></a><span class="ex">nvidia-smi</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Expected nvidia-smi output:</strong></p>
<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
|   0  NVIDIA A100-SXM...  On   | 00000000:00:04.0 Off |                    0 |
|   1  NVIDIA A100-SXM...  On   | 00000000:00:05.0 Off |                    0 |
+-------------------------------+----------------------+----------------------+</code></pre>
<hr>
</section>
</section>
<section id="running-zero-1" class="level3">
<h3 class="anchored" data-anchor-id="running-zero-1">7.3 Running ZeRO-1</h3>
<section id="basic-execution" class="level4">
<h4 class="anchored" data-anchor-id="basic-execution">7.3.1 <strong>Basic Execution</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb90"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run with 2 GPUs</span></span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a><span class="ex">torchrun</span> <span class="at">--nproc_per_node</span><span class="op">=</span>2 zero1.py</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What happens:</strong></p>
<ol type="1">
<li><code>torchrun</code> launches 2 processes (one per GPU)</li>
<li>Each process gets unique <code>LOCAL_RANK</code> (0, 1)</li>
<li>NCCL initializes communication backend</li>
<li>Training runs with regular Adam baseline</li>
<li>Training runs with ZeRO-1 sharded optimizer</li>
<li>Memory comparison printed</li>
<li>Profiler traces saved to <code>./profiler_traces/</code></li>
</ol>
</section>
<section id="expected-output" class="level4">
<h4 class="anchored" data-anchor-id="expected-output">7.3.2 <strong>Expected Output</strong></h4>
<pre><code>GPU 0 - Testing with regular Adam:

GPU 0 - Initial state:
  Model parameters: 2289.05 MB
  Gradients: 2289.05 MB
  Optimizer states: 4578.10 MB
  Total allocated: 6944.14 MB
  Max allocated: 8090.25 MB
----------------------------------------

Step 0 memory:
Before backward: 6947.19 MB
Gradient memory after backward: 2289.05 MB
Peak memory this step: 11528.60 MB

Final peak memory: 11528.60 MB

GPU 0 - Testing with Sharded Adam:

GPU 0 - Initial state:
  Model parameters: 2289.05 MB
  Gradients: 2289.05 MB
  Optimizer states: 2289.05 MB  ‚Üê Sharded! (50% reduction)
  Total allocated: 6944.14 MB
  Max allocated: 8090.25 MB
----------------------------------------

Step 0 memory:
Before backward: 5801.07 MB
Gradient memory after backward: 2289.05 MB
Peak memory this step: 8090.25 MB

Final peak memory: 8090.25 MB

Timing and Communication Stats:
----------------------------------------
Average step time: 0.024s
Average communication time: 0.000s
Average compute time: 0.024s
Communication overhead: 0.0%

Memory Usage Summary:
----------------------------------------
Peak memory with regular Adam: 11528.60 MB
Peak memory with ZeRO-1 (sharded optimizer states): 8090.25 MB
Memory reduction: 3438.35 MB (29.82%)

Profiler traces saved to:
  - ./profiler_traces/regular_adam
  - ./profiler_traces/zero1_adam

View with: tensorboard --logdir=./profiler_traces</code></pre>
</section>
<section id="viewing-profiler-traces" class="level4">
<h4 class="anchored" data-anchor-id="viewing-profiler-traces">7.3.3 <strong>Viewing Profiler Traces</strong></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb92"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Launch TensorBoard</span></span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a><span class="ex">tensorboard</span> <span class="at">--logdir</span><span class="op">=</span>./profiler_traces</span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a><span class="co"># If running on remote server, forward port:</span></span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a><span class="co"># On local machine:</span></span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ssh</span> <span class="at">-L</span> 6006:localhost:6006 user@remote-server</span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Then open browser to:</span></span>
<span id="cb92-10"><a href="#cb92-10" aria-hidden="true" tabindex="-1"></a><span class="ex">http://localhost:6006</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What to look for:</strong></p>
<ul>
<li>Navigate to ‚ÄúPYTORCH_PROFILER‚Äù tab</li>
<li>Compare ‚Äúregular_adam‚Äù vs ‚Äúzero1_adam‚Äù runs</li>
<li>Check ‚ÄúOverview‚Äù for execution breakdown</li>
<li>Check ‚ÄúMemory View‚Äù for peak memory timeline</li>
<li>Check ‚ÄúOperator View‚Äù for communication operations</li>
</ul>
<p>We can run all ZeRO stages in a similar way.</p>
<hr>
</section>
</section>
<section id="comparing-all-three-stages" class="level3">
<h3 class="anchored" data-anchor-id="comparing-all-three-stages">7.4 Comparing All Three Stages</h3>
<section id="run-all-stages-in-sequence" class="level4">
<h4 class="anchored" data-anchor-id="run-all-stages-in-sequence">7.4.1 <strong>Run All Stages in Sequence</strong></h4>
<p>Create a script <code>run_all.sh</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb93"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">"========================================="</span></span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">"Running ZeRO-1"</span></span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">"========================================="</span></span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a><span class="ex">torchrun</span> <span class="at">--nproc_per_node</span><span class="op">=</span>2 zero1.py <span class="dv">2</span><span class="op">&gt;&amp;</span><span class="dv">1</span> <span class="kw">|</span> <span class="fu">tee</span> zero1_output.log</span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">""</span></span>
<span id="cb93-9"><a href="#cb93-9" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">"========================================="</span></span>
<span id="cb93-10"><a href="#cb93-10" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">"Running ZeRO-2"</span></span>
<span id="cb93-11"><a href="#cb93-11" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">"========================================="</span></span>
<span id="cb93-12"><a href="#cb93-12" aria-hidden="true" tabindex="-1"></a><span class="ex">torchrun</span> <span class="at">--nproc_per_node</span><span class="op">=</span>2 zero2.py <span class="dv">2</span><span class="op">&gt;&amp;</span><span class="dv">1</span> <span class="kw">|</span> <span class="fu">tee</span> zero2_output.log</span>
<span id="cb93-13"><a href="#cb93-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-14"><a href="#cb93-14" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">""</span></span>
<span id="cb93-15"><a href="#cb93-15" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">"========================================="</span></span>
<span id="cb93-16"><a href="#cb93-16" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">"Running ZeRO-3"</span></span>
<span id="cb93-17"><a href="#cb93-17" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">"========================================="</span></span>
<span id="cb93-18"><a href="#cb93-18" aria-hidden="true" tabindex="-1"></a><span class="ex">torchrun</span> <span class="at">--nproc_per_node</span><span class="op">=</span>2 zero3.py <span class="dv">2</span><span class="op">&gt;&amp;</span><span class="dv">1</span> <span class="kw">|</span> <span class="fu">tee</span> zero3_output.log</span>
<span id="cb93-19"><a href="#cb93-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-20"><a href="#cb93-20" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">""</span></span>
<span id="cb93-21"><a href="#cb93-21" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">"========================================="</span></span>
<span id="cb93-22"><a href="#cb93-22" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">"Summary"</span></span>
<span id="cb93-23"><a href="#cb93-23" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">"========================================="</span></span>
<span id="cb93-24"><a href="#cb93-24" aria-hidden="true" tabindex="-1"></a><span class="fu">grep</span> <span class="st">"Memory reduction:"</span> zero1_output.log zero2_output.log zero3_output.log</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Make it executable and run:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb94"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="fu">chmod</span> +x run_all.sh</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a><span class="ex">./run_all.sh</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="extracting-results" class="level4">
<h4 class="anchored" data-anchor-id="extracting-results">7.4.2 <strong>Extracting Results</strong></h4>
<p>Create <code>parse_results.py</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Original</span></span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim <span class="im">import</span> Adam</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a><span class="co"># SGD with momentum</span></span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim <span class="im">import</span> SGD</span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-9"><a href="#cb95-9" aria-hidden="true" tabindex="-1"></a><span class="co"># AdamW (weight decay)</span></span>
<span id="cb95-10"><a href="#cb95-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-11"><a href="#cb95-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim <span class="im">import</span> AdamW</span>
<span id="cb95-12"><a href="#cb95-12" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> AdamW(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>, weight_decay<span class="op">=</span><span class="fl">0.01</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Memory impact:</strong></p>
<ul>
<li>SGD with momentum: K = 8 (less than Adam‚Äôs K = 12)</li>
<li>AdamW: Same as Adam (K = 12)</li>
<li>SGD without momentum: K = 4 (minimal optimizer state)</li>
</ul>
<hr>
</section>
</section>
<section id="advanced-experiments" class="level3">
<h3 class="anchored" data-anchor-id="advanced-experiments">7.5 Advanced Experiments</h3>
<section id="measuring-bandwidth-utilization" class="level4">
<h4 class="anchored" data-anchor-id="measuring-bandwidth-utilization">7.5.1 <strong>Measuring Bandwidth Utilization</strong></h4>
<p>Add to <code>zero1.py</code> step function:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> step(<span class="va">self</span>, closure<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>    step_start <span class="op">=</span> time.perf_counter()</span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Measure data transferred</span></span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a>    total_bytes <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> p.grad <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a>            total_bytes <span class="op">+=</span> p.grad.numel() <span class="op">*</span> p.grad.element_size()</span>
<span id="cb96-9"><a href="#cb96-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-10"><a href="#cb96-10" aria-hidden="true" tabindex="-1"></a>    comm_start <span class="op">=</span> time.perf_counter()</span>
<span id="cb96-11"><a href="#cb96-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb96-12"><a href="#cb96-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> p.grad <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb96-13"><a href="#cb96-13" aria-hidden="true" tabindex="-1"></a>            dist.all_reduce(p.grad.data, op<span class="op">=</span>dist.ReduceOp.SUM)</span>
<span id="cb96-14"><a href="#cb96-14" aria-hidden="true" tabindex="-1"></a>            p.grad.data <span class="op">/=</span> get(<span class="st">"ws"</span>)</span>
<span id="cb96-15"><a href="#cb96-15" aria-hidden="true" tabindex="-1"></a>    torch.cuda.synchronize()</span>
<span id="cb96-16"><a href="#cb96-16" aria-hidden="true" tabindex="-1"></a>    comm_time <span class="op">=</span> time.perf_counter() <span class="op">-</span> comm_start</span>
<span id="cb96-17"><a href="#cb96-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-18"><a href="#cb96-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate bandwidth</span></span>
<span id="cb96-19"><a href="#cb96-19" aria-hidden="true" tabindex="-1"></a>    bandwidth_gbps <span class="op">=</span> (total_bytes <span class="op">/</span> <span class="fl">1e9</span>) <span class="op">/</span> comm_time</span>
<span id="cb96-20"><a href="#cb96-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-21"><a href="#cb96-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> get(<span class="st">'rank'</span>) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb96-22"><a href="#cb96-22" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"All-reduce bandwidth: </span><span class="sc">{</span>bandwidth_gbps<span class="sc">:.2f}</span><span class="ss"> GB/s"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Typical values:</strong></p>
<ul>
<li>NVLink (V100): 50-100 GB/s per direction</li>
<li>PCIe 4.0 x16: 15-25 GB/s</li>
<li>Ethernet (100 Gbps): 8-12 GB/s</li>
</ul>
</section>
<section id="profiling-with-different-profiler-settings" class="level4">
<h4 class="anchored" data-anchor-id="profiling-with-different-profiler-settings">7.5.2 <strong>Profiling with Different Profiler Settings</strong></h4>
<p>Modify profiler configuration in <code>zero1.py</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="co"># More detailed profiling</span></span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>profiler_context <span class="op">=</span> profile(</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a>    activities<span class="op">=</span>[ProfilerActivity.CPU, ProfilerActivity.CUDA],</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>    schedule<span class="op">=</span>schedule(</span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>        skip_first<span class="op">=</span><span class="dv">3</span>,   <span class="co"># Skip fewer warmup steps</span></span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a>        wait<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a>        warmup<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb97-8"><a href="#cb97-8" aria-hidden="true" tabindex="-1"></a>        active<span class="op">=</span><span class="dv">10</span>,      <span class="co"># Profile more steps</span></span>
<span id="cb97-9"><a href="#cb97-9" aria-hidden="true" tabindex="-1"></a>        repeat<span class="op">=</span><span class="dv">2</span>        <span class="co"># Repeat profiling cycle</span></span>
<span id="cb97-10"><a href="#cb97-10" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb97-11"><a href="#cb97-11" aria-hidden="true" tabindex="-1"></a>    on_trace_ready<span class="op">=</span>torch.profiler.tensorboard_trace_handler(</span>
<span id="cb97-12"><a href="#cb97-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">"./profiler_traces/detailed"</span></span>
<span id="cb97-13"><a href="#cb97-13" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb97-14"><a href="#cb97-14" aria-hidden="true" tabindex="-1"></a>    record_shapes<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb97-15"><a href="#cb97-15" aria-hidden="true" tabindex="-1"></a>    profile_memory<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb97-16"><a href="#cb97-16" aria-hidden="true" tabindex="-1"></a>    with_stack<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb97-17"><a href="#cb97-17" aria-hidden="true" tabindex="-1"></a>    with_flops<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb97-18"><a href="#cb97-18" aria-hidden="true" tabindex="-1"></a>    with_modules<span class="op">=</span><span class="va">True</span>  <span class="co"># Track module-level info</span></span>
<span id="cb97-19"><a href="#cb97-19" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="testing-with-real-models" class="level4">
<h4 class="anchored" data-anchor-id="testing-with-real-models">7.5.3 <strong>Testing with Real Models</strong></h4>
<p>Replace the simple model with a transformer:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModel</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load a small transformer (e.g., BERT-base)</span></span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModel.from_pretrained(<span class="st">"bert-base-uncased"</span>).to(device)</span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-6"><a href="#cb98-6" aria-hidden="true" tabindex="-1"></a><span class="co"># For larger models (requires more GPUs):</span></span>
<span id="cb98-7"><a href="#cb98-7" aria-hidden="true" tabindex="-1"></a><span class="co"># model = AutoModel.from_pretrained("gpt2-large").to(device)</span></span>
<span id="cb98-8"><a href="#cb98-8" aria-hidden="true" tabindex="-1"></a><span class="co"># model = AutoModel.from_pretrained("facebook/opt-1.3b").to(device)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Important:</strong> You‚Äôll need to adjust the input data shape to match the model‚Äôs expected input.</p>
</section>
</section>
<section id="experiment-ideas" class="level3">
<h3 class="anchored" data-anchor-id="experiment-ideas">7.6 Experiment Ideas</h3>
<section id="scaling-study" class="level4">
<h4 class="anchored" data-anchor-id="scaling-study">7.6.1 <strong>Scaling Study</strong></h4>
<p><strong>Goal:</strong> Measure how memory reduction scales with GPU count</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb99"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run with different GPU counts</span></span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ngpu <span class="kw">in</span> 2 4 8<span class="kw">;</span> <span class="cf">do</span></span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">echo</span> <span class="st">"Testing with </span><span class="va">$ngpu</span><span class="st"> GPUs"</span></span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a>    <span class="ex">torchrun</span> <span class="at">--nproc_per_node</span><span class="op">=</span><span class="va">$ngpu</span> zero3.py <span class="dv">2</span><span class="op">&gt;&amp;</span><span class="dv">1</span> <span class="kw">|</span> <span class="fu">tee</span> zero3_<span class="va">${ngpu}</span>gpu.log</span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a><span class="cf">done</span></span>
<span id="cb99-6"><a href="#cb99-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-7"><a href="#cb99-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare results</span></span>
<span id="cb99-8"><a href="#cb99-8" aria-hidden="true" tabindex="-1"></a><span class="fu">grep</span> <span class="st">"Memory reduction:"</span> zero3_<span class="pp">*</span>gpu.log</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Hypothesis:</strong> Memory reduction should approach theoretical limits:</p>
<ul>
<li>2 GPUs: ~50%</li>
<li>4 GPUs: ~75%</li>
<li>8 GPUs: ~87.5%</li>
</ul>
</section>
<section id="communication-vs.-computation-trade-off" class="level4">
<h4 class="anchored" data-anchor-id="communication-vs.-computation-trade-off">7.6.2 <strong>Communication vs.&nbsp;Computation Trade-off</strong></h4>
<p><strong>Goal:</strong> Find the break-even point where ZeRO overhead becomes negligible</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Vary model size</span></span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>hidden_dims <span class="op">=</span> [<span class="dv">5_000</span>, <span class="dv">10_000</span>, <span class="dv">20_000</span>, <span class="dv">50_000</span>]</span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> hidden_dim <span class="kw">in</span> hidden_dims:</span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create model with this hidden dimension</span></span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Measure communication overhead</span></span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot: Hidden Dim vs Communication Overhead %</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Expected:</strong> Larger models ‚Üí Lower communication overhead percentage</p>
</section>
</section>
<section id="next-steps" class="level3">
<h3 class="anchored" data-anchor-id="next-steps">7.7 Next Steps</h3>
<p>After successfully running the experiments:</p>
<ol type="1">
<li><p><strong>Experiment with your own models</strong>: Replace the simple MLP with your research model</p></li>
<li><p><strong>Profile in detail</strong>: Use TensorBoard to identify bottlenecks specific to your workload</p></li>
<li><p><strong>Scale to more GPUs</strong>: Test how ZeRO performs on 4, 8, or more GPUs</p></li>
<li><p><strong>Combine techniques</strong>: Try ZeRO + checkpointing + mixed precision + offloading</p></li>
<li><p><strong>Contribute</strong>: Share your findings, optimizations, or bug fixes with the community</p></li>
<li><p><strong>Explore ZeRO-R</strong>: Add residual state partitioning (activations, temporary buffers)</p></li>
<li><p><strong>Implement ZeRO-Infinity</strong>: Add NVMe offloading for trillion-parameter models</p></li>
</ol>
<hr>
</section>
<section id="validation-checklist" class="level3">
<h3 class="anchored" data-anchor-id="validation-checklist">7.8 Validation Checklist</h3>
<p>Before concluding your experiments, verify:</p>
<ul class="task-list">
<li><label><input type="checkbox">All three ZeRO stages run without errors</label></li>
<li><label><input type="checkbox">Memory reductions match expected theoretical values (¬±10%)</label></li>
<li><label><input type="checkbox">Communication overhead increases from ZeRO-1 ‚Üí ZeRO-2 ‚Üí ZeRO-3</label></li>
<li><label><input type="checkbox">ZeRO-3 shows the best memory savings (~50%+ reduction)</label></li>
<li><label><input type="checkbox">Profiler traces are generated and viewable in TensorBoard</label></li>
<li><label><input type="checkbox">Bandwidth tests show reasonable interconnect performance</label></li>
<li><label><input type="checkbox">Results are reproducible across multiple runs (same seed)</label></li>
<li><label><input type="checkbox">All GPUs show balanced memory usage (check nvidia-smi)</label></li>
</ul>
<hr>
</section>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">7.9 Summary</h3>
<p>This section covered:</p>
<ol type="1">
<li><strong>Prerequisites</strong>: Hardware, software, and network requirements</li>
<li><strong>Environment setup</strong>: Virtual environment, dependencies, verification</li>
<li><strong>Running ZeRO-1, 2, 3</strong>: Step-by-step execution with expected outputs</li>
<li><strong>Customization</strong>: Changing model size, batch size, GPU count, optimizers</li>
<li><strong>Advanced experiments</strong>: Bandwidth measurement, real models, checkpointing</li>
<li><strong>Troubleshooting</strong>: Common issues and solutions</li>
<li><strong>Benchmarking</strong>: GPU bandwidth testing</li>
<li><strong>Experiment ideas</strong>: Scaling studies, trade-off analysis, real workloads</li>
<li><strong>Reproducing paper results</strong>: Scaling to larger models</li>
<li><strong>Validation</strong>: Checklist for verifying your results</li>
</ol>
<p>You now have everything needed to reproduce our results and conduct your own ZeRO experiments!</p>
<hr>
</section>
</section>
<section id="findings-and-conclusion" class="level2">
<h2 class="anchored" data-anchor-id="findings-and-conclusion">8. Findings and Conclusion</h2>
<section id="key-findings" class="level3">
<h3 class="anchored" data-anchor-id="key-findings">8.1 Key Findings</h3>
<section id="memory-efficiency-achievements" class="level4">
<h4 class="anchored" data-anchor-id="memory-efficiency-achievements">8.1.1 <strong>Memory Efficiency Achievements</strong></h4>
<p>Our experiments with a 2.3B parameter model across 2 GPUs demonstrated the progressive memory optimization of ZeRO stages:</p>
<p><strong>Memory Reduction Results:</strong></p>
<ul>
<li><strong>ZeRO-1</strong>: 29.82% memory reduction (11.5 GB ‚Üí 8.1 GB)
<ul>
<li>Delivers on theoretical promise with minimal gap from theory</li>
<li>Shards only optimizer states while keeping parameters and gradients replicated</li>
</ul></li>
<li><strong>ZeRO-2</strong>: 26.53% memory reduction (11.5 GB ‚Üí 8.5 GB)
<ul>
<li>Gap from theory due to temporary communication buffers</li>
<li>Additional sharding of gradients offset by communication overhead</li>
</ul></li>
<li><strong>ZeRO-3</strong>: 56.34% memory reduction (11.5 GB ‚Üí 5.0 GB)
<ul>
<li><strong>EXCEEDS theory</strong> by avoiding simultaneous parameter storage</li>
<li>Only one layer‚Äôs parameters materialized at a time</li>
<li>Enables training models that wouldn‚Äôt fit otherwise</li>
</ul></li>
</ul>
<p><strong>Theoretical Scaling:</strong> ZeRO-3‚Äôs memory reduction scales linearly with the number of GPUs. With 1024 GPUs, you could train a model 1024√ó larger than what fits on a single GPU.</p>
</section>
<section id="communication-overhead-trade-offs" class="level4">
<h4 class="anchored" data-anchor-id="communication-overhead-trade-offs">8.1.2 <strong>Communication Overhead Trade-offs</strong></h4>
<p>The memory savings come with varying communication costs that scale differently with model size:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 32%">
<col style="width: 29%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th>Stage</th>
<th>Communication Volume</th>
<th>Measured Overhead</th>
<th>Scaling Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Baseline DP</strong></td>
<td>2Œ® (all-reduce)</td>
<td>Reference</td>
<td>-</td>
</tr>
<tr class="even">
<td><strong>ZeRO-1</strong></td>
<td>2Œ® (reduce-scatter + broadcast)</td>
<td><strong>0%</strong></td>
<td>Same as baseline</td>
</tr>
<tr class="odd">
<td><strong>ZeRO-2</strong></td>
<td>2Œ® (reduce-scatter + broadcast)</td>
<td><strong>48.6%</strong></td>
<td>Amortizes with larger batches/GPUs</td>
</tr>
<tr class="even">
<td><strong>ZeRO-3</strong></td>
<td>3Œ® (all-gather per layer)</td>
<td><strong>97.0%</strong></td>
<td>Becomes negligible as model size grows</td>
</tr>
</tbody>
</table>
<p><strong>Critical Insight:</strong> Communication overhead becomes negligible as model size grows. For 100B+ parameter models, compute time dominates and ZeRO-3‚Äôs overhead drops to 10-20%, while enabling training that‚Äôs otherwise impossible.</p>
</section>
<section id="profiler-insights" class="level4">
<h4 class="anchored" data-anchor-id="profiler-insights">8.1.3 <strong>Profiler Insights</strong></h4>
<p>Profiler analysis revealed the distinct execution patterns of each ZeRO stage:</p>
<p><strong>ZeRO-1 Profiler Verdict:</strong> Delivers exactly what it promises‚Äî29.82% memory reduction with zero performance penalty. The profiler confirms that compute efficiency is maintained while memory usage drops dramatically.</p>
<p><strong>ZeRO-2 Profiler Verdict:</strong> Trades peak memory spikes for lower baseline memory. The 48.6% overhead is expected with our small 2-GPU setup. With larger batch sizes and more GPUs (8+), the communication overhead amortizes better and peak spikes become less significant relative to baseline memory savings.</p>
<p><strong>ZeRO-3 Profiler Verdict:</strong> Achieves unprecedented memory efficiency by treating parameters as temporary resources rather than permanent state. The 97% communication overhead is the price for this flexibility, but it enables training models that simply wouldn‚Äôt fit otherwise. With larger models, arithmetic intensity increases and communication becomes a smaller fraction of total time.</p>
</section>
<section id="memory-consumption-fundamentals" class="level4">
<h4 class="anchored" data-anchor-id="memory-consumption-fundamentals">8.1.4 <strong>Memory Consumption Fundamentals</strong></h4>
<p>Understanding where memory goes in deep learning revealed:</p>
<ol type="1">
<li><strong>Model states dominate</strong> memory usage, with Adam requiring 16Œ® bytes for Œ® parameters</li>
<li><strong>Activations are the second largest</strong> consumer, but checkpointing helps significantly</li>
<li><strong>Temporary buffers and fragmentation</strong> add 10-30% overhead</li>
<li><strong>Data parallelism is memory inefficient</strong> due to complete redundancy across GPUs</li>
<li><strong>Standard DP runs out of memory</strong> for models &gt;1.4B parameters on 32GB GPUs</li>
</ol>
</section>
<section id="when-to-use-each-zero-stage" class="level4">
<h4 class="anchored" data-anchor-id="when-to-use-each-zero-stage">8.1.5 <strong>When to Use Each ZeRO Stage</strong></h4>
<p>Based on profiler analysis and experimental results:</p>
<p><strong>Use ZeRO-2 when (DEFAULT RECOMMENDATION):</strong></p>
<ul>
<li><p>Nearly all production training scenarios</p></li>
<li><p>You have 4+ GPUs with reasonable interconnect</p></li>
<li><p>Batch size ‚â• 32 (global)</p></li>
<li><p>You want the best balance of memory savings and performance</p></li>
<li><p><strong>This should be your starting point!</strong></p></li>
</ul>
<p><strong>Use ZeRO-1 when:</strong></p>
<ul>
<li><p>You‚Äôre doing small-scale debugging (2-4 GPUs, tiny batches)</p></li>
<li><p>Very limited interconnect bandwidth (old PCIe Gen3)</p></li>
<li><p>Model comfortably fits and you‚Äôre bandwidth-constrained</p></li>
<li><p>Latency-critical applications where every millisecond counts</p></li>
</ul>
<p><strong>Use ZeRO-3 when:</strong></p>
<ul>
<li>Model absolutely won‚Äôt fit otherwise</li>
<li>You have excellent GPU interconnect (NVLink, InfiniBand)</li>
<li>Training very large models (10B+ parameters)</li>
<li>You‚Äôre willing to trade performance for memory</li>
<li>Scaling to 64+ GPUs where communication amortizes</li>
</ul>
</section>
</section>
<section id="practical-recommendations" class="level3">
<h3 class="anchored" data-anchor-id="practical-recommendations">8.2 Practical Recommendations</h3>
<section id="implementation-best-practices" class="level4">
<h4 class="anchored" data-anchor-id="implementation-best-practices">8.2.1 <strong>Implementation Best Practices</strong></h4>
<p>From our implementation deep dive:</p>
<ol type="1">
<li><strong>Start with ZeRO-2, not ZeRO-1</strong>: Despite our 48.6% overhead measurement, ZeRO-2 is the better default
<ul>
<li>Our 2-GPU, small-batch experiment is a worst-case scenario</li>
<li>With 8 GPUs and batch size ‚â•32, overhead drops to ~3-5%</li>
<li>You get 15-20% more memory than ZeRO-1 for effectively free</li>
<li>Only fall back to ZeRO-1 if bandwidth-constrained</li>
</ul></li>
<li><strong>Profile before scaling</strong>: Use PyTorch profiler to understand your bottlenecks</li>
<li><strong>Test communication bandwidth</strong>: Use provided benchmarks to verify your network</li>
<li><strong>Monitor memory patterns</strong>: Watch for spikes vs baseline consumption</li>
<li><strong>Validate correctness</strong>: Compare final losses across all stages</li>
</ol>
</section>
<section id="hardware-requirements-1" class="level4">
<h4 class="anchored" data-anchor-id="hardware-requirements-1">8.2.2 <strong>Hardware Requirements</strong></h4>
<p>For effective ZeRO deployment:</p>
<ul>
<li><strong>Minimum</strong>: 2 GPUs with PCIe connection (ZeRO-1)</li>
<li><strong>Recommended</strong>: 4-8 GPUs with NVLink/NVSwitch (ZeRO-2)</li>
<li><strong>Optimal</strong>: 16+ GPUs with InfiniBand (ZeRO-3)</li>
</ul>
</section>
<section id="performance-optimization" class="level4">
<h4 class="anchored" data-anchor-id="performance-optimization">8.2.3 <strong>Performance Optimization</strong></h4>
<p>To maximize ZeRO performance:</p>
<ol type="1">
<li><strong>Increase batch size</strong>: Amortizes communication overhead</li>
<li><strong>Use larger models</strong>: Improves arithmetic intensity</li>
<li><strong>Enable NCCL optimizations</strong>: Set appropriate environment variables</li>
<li><strong>Consider mixed-precision</strong>: fp16/bf16 reduces memory and communication</li>
<li><strong>Profile iteratively</strong>: Identify and eliminate bottlenecks systematically</li>
</ol>
</section>
</section>
<section id="broader-impact" class="level3">
<h3 class="anchored" data-anchor-id="broader-impact">8.3 Broader Impact</h3>
<p>ZeRO represents a fundamental shift in distributed training philosophy:</p>
<p><strong>From replication to sharding</strong>: Instead of maintaining complete copies of model state across GPUs, ZeRO treats distributed memory as a unified pool. This enables:</p>
<ul>
<li><strong>Linear scaling</strong>: Memory capacity grows with GPU count</li>
<li><strong>Accessibility</strong>: Researchers can train larger models without massive clusters</li>
<li><strong>Efficiency</strong>: Eliminates redundant memory consumption</li>
<li><strong>Flexibility</strong>: Trade-offs between memory and communication are configurable</li>
</ul>
<p>The techniques demonstrated in this blog‚Äîoptimizer state sharding, gradient sharding, and parameter sharding‚Äîform the foundation of modern large-scale training systems including DeepSpeed, FSDP, and commercial LLM training infrastructure.</p>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">8.4 Conclusion</h3>
<p>ZeRO‚Äôs elegant solution to the memory wall problem demonstrates that careful system design can overcome fundamental scaling bottlenecks. By progressively eliminating redundancy in optimizer states, gradients, and parameters, ZeRO enables training of models that were previously impossible.</p>
<p>Our experimental results validate the theoretical foundations: - <strong>ZeRO-1</strong> provides free memory savings with zero performance cost - <strong>ZeRO-2</strong> offers deeper savings with acceptable overhead at scale - <strong>ZeRO-3</strong> achieves unprecedented memory efficiency for extreme-scale training</p>
<p>The profiler traces reveal the execution patterns underlying these trade-offs, showing how communication overhead amortizes as model size and GPU count increase.</p>
<p>Most importantly, the implementations provided in this blog demonstrate that ZeRO‚Äôs core ideas‚Äîpartition instead of replicate, communicate on-demand, shard everything‚Äîcan be understood and applied by practitioners. Whether you‚Äôre training a 7B model on 2 GPUs or a 175B model on 1000 GPUs, these principles remain the same.</p>
<p><strong>The memory wall is not insurmountable. With ZeRO, we can scale beyond it.</strong></p>
<hr>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<section id="primary-literature" class="level3">
<h3 class="anchored" data-anchor-id="primary-literature">Primary Literature</h3>
<ol type="1">
<li><p><strong>Rajbhandari, S., Rasley, J., Ruwase, O., &amp; He, Y. (2020).</strong> ZeRO: Memory Optimizations Toward Training Trillion Parameter Models. <em>SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</em>. IEEE. <a href="https://arxiv.org/abs/1910.02054">arXiv:1910.02054</a></p></li>
<li><p><strong>Rajbhandari, S., Ruwase, O., Rasley, J., Smith, S., &amp; He, Y. (2021).</strong> ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning. <em>SC21: International Conference for High Performance Computing, Networking, Storage and Analysis</em>. IEEE. <a href="https://arxiv.org/abs/2104.07857">arXiv:2104.07857</a></p></li>
</ol>
</section>
<section id="implementation-code" class="level3">
<h3 class="anchored" data-anchor-id="implementation-code">Implementation &amp; Code</h3>
<ol start="3" type="1">
<li><p><strong>This Blog‚Äôs GitHub Repository:</strong> <a href="https://github.com/Scratch-to-Scale/zero-daddyofadoggy/tree/main">zero-daddyofadoggy</a> - Full implementations of ZeRO-1, ZeRO-2, and ZeRO-3 with profiling and visualization tools</p></li>
<li><p><strong>Microsoft DeepSpeed:</strong> <a href="https://github.com/microsoft/DeepSpeed">https://github.com/microsoft/DeepSpeed</a> - Production implementation of ZeRO optimizations</p></li>
<li><p><strong>PyTorch FSDP Documentation:</strong> <a href="https://pytorch.org/docs/stable/fsdp.html">https://pytorch.org/docs/stable/fsdp.html</a> - PyTorch‚Äôs Fully Sharded Data Parallel, inspired by ZeRO</p></li>
</ol>
</section>
<section id="related-work-background" class="level3">
<h3 class="anchored" data-anchor-id="related-work-background">Related Work &amp; Background</h3>
<ol start="6" type="1">
<li><p><strong>Li, S., Zhao, Y., Varma, R., et al.&nbsp;(2020).</strong> PyTorch Distributed: Experiences on Accelerating Data Parallel Training. <em>Proceedings of the VLDB Endowment</em>, 13(12).</p></li>
<li><p><strong>Narayanan, D., et al.&nbsp;(2021).</strong> Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM. <em>SC21: International Conference for High Performance Computing, Networking, Storage and Analysis</em>. IEEE.</p></li>
<li><p><strong>Brown, T., et al.&nbsp;(2020).</strong> Language Models are Few-Shot Learners. <em>Advances in Neural Information Processing Systems</em>, 33. <a href="https://arxiv.org/abs/2005.14165">arXiv:2005.14165</a></p></li>
</ol>
</section>
<section id="tools-frameworks" class="level3">
<h3 class="anchored" data-anchor-id="tools-frameworks">Tools &amp; Frameworks</h3>
<ol start="9" type="1">
<li><p><strong>PyTorch Documentation:</strong> <a href="https://pytorch.org/docs/stable/index.html">https://pytorch.org/docs/stable/index.html</a></p></li>
<li><p><strong>NVIDIA NCCL:</strong> <a href="https://developer.nvidia.com/nccl">https://developer.nvidia.com/nccl</a> - Collective communication library used for GPU synchronization</p></li>
<li><p><strong>TensorBoard Profiler:</strong> <a href="https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras">https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras</a></p></li>
</ol>
<hr>
<p><em>End of ZeRO Implementation Blog</em></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/your-website-url\.example\.com");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>