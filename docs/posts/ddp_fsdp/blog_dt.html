<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>blog_dt – My Blogs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">My Blogs</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block"></header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>This study presents a comprehensive analysis of distributed training strategies for large language models using PyTorch. Three distributed training approaches—<strong>Data Parallel (DDP)</strong>, <strong>Fully Sharded Data Parallel with full parameter sharding (FSDP-Full/ZeRO-3)</strong>, and <strong>FSDP with gradient sharding only (FSDP-Grad/ZeRO-2)</strong>—were implemented and evaluated against a single-GPU baseline. Experiments conducted on a <strong>2x H100 SXM5</strong> Lambda instance training <strong>GPT-2 Large (774M parameters)</strong> reveal that FSDP-Grad achieves <strong>97.71% compute efficiency</strong>, nearly matching single-GPU performance while providing memory savings and enabling larger-scale training.</p>
<p>Training large language models has become increasingly challenging as model sizes grow from millions to billions of parameters. A GPT-2 Large model with 774M parameters requires approximately <strong>12GB of GPU memory</strong> for training on a single GPU. This memory consumption breaks down as follows: <strong>model parameters</strong> stored in FP32 format consume ~3GB (774M × 4 bytes), <strong>gradients</strong> of the same size add another ~3GB, <strong>optimizer states</strong> (momentum and variance for AdamW) require ~6GB (2× parameters in FP32), totaling ~12GB for the model states alone. Additionally, <strong>activation tensors</strong> saved during the forward pass for backpropagation require extra memory that scales with batch size and sequence length. As models scale to billions or trillions of parameters, this linear memory growth makes single-GPU training infeasible, necessitating distributed training strategies.</p>
<p>This comprehensive study aims to: (1) <strong>Implement and compare</strong> three distributed training strategies: DDP, FSDP-Full (ZeRO-3), and FSDP-Grad (ZeRO-2); (2) <strong>Analyze memory consumption</strong>, throughput, and communication patterns for each approach; (3) <strong>Profile GPU utilization</strong>, compute vs.&nbsp;communication time, and operator-level performance; and (4) <strong>Provide practical recommendations</strong> for choosing the optimal strategy based on model size and hardware constraints.</p>
<p>The experimental setup utilized a <strong>GPU 2x H100 SXM5 Lambda</strong> instance with 2x NVIDIA H100 GPUs (80GB HBM3 each) connected via NVLink/NVSwitch for high-bandwidth GPU-to-GPU communication. The model configuration consisted of GPT-2 Large architecture with 774,439,808 parameters, sequence length of 1024 tokens, global batch size of 32, micro batch size of 8 per GPU, 20 training steps, and learning rate of 3e-4 with cosine annealing. The software stack included PyTorch 2.x with CUDA support, <code>uv</code> for Python package management, NCCL (NVIDIA Collective Communications Library) as the distributed backend, and PyTorch Profiler with Chrome trace export for profiling.</p>
<hr>
</section>
<section id="challenges-in-distributed-training-vs.-single-gpu-training" class="level1">
<h1>Challenges in Distributed Training vs.&nbsp;Single GPU Training</h1>
<section id="memory-constraints" class="level2">
<h2 class="anchored" data-anchor-id="memory-constraints">Memory Constraints</h2>
<section id="single-gpu-training" class="level3">
<h3 class="anchored" data-anchor-id="single-gpu-training">Single GPU Training</h3>
<p>Training a neural network on a single GPU requires storing:</p>
<ol type="1">
<li><strong>Model Parameters (W)</strong>: Original weights</li>
<li><strong>Gradients (∇W)</strong>: Same size as parameters</li>
<li><strong>Optimizer States (O)</strong>: For Adam/AdamW, stores first and second moments (2x parameter size)</li>
<li><strong>Activations (A)</strong>: Intermediate outputs saved for backward pass</li>
</ol>
<p>For GPT-2 Small (<strong>124,439,808 parameters</strong>, or ~124M) with mixed precision training:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>Parameters:       <span class="fl">474.70</span> MB  (<span class="dv">124</span><span class="er">M</span> × <span class="dv">4</span> <span class="bu">bytes</span> <span class="kw">in</span> FP32)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>Gradients:        <span class="fl">474.70</span> MB  (same size <span class="im">as</span> parameters)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>Optimizer States: <span class="fl">949.40</span> MB  (FP32: momentum <span class="op">+</span> variance <span class="op">=</span> <span class="dv">2</span>× parameters)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>Total Estimated:  <span class="dv">1</span>,<span class="fl">898.80</span> MB</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>Actual Peak:      <span class="dv">3</span>,<span class="fl">076.27</span> MB (includes activations <span class="kw">and</span> overhead)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The parameter memory calculation: 124,439,808 parameters × 4 bytes (FP32) = 497,759,232 bytes ≈ 474.70 MB. Despite using mixed precision (FP16 for forward/backward passes), the optimizer maintains an FP32 master copy of weights, so the base parameter storage is FP32.</p>
<p>The <strong>1,177.47 MB difference</strong> between estimated and actual memory comes from:</p>
<ul>
<li><strong>Activation tensors</strong> stored during forward pass (~800-900 MB)</li>
<li><strong>Temporary buffers</strong> for operations</li>
<li><strong>PyTorch framework overhead</strong></li>
<li><strong>Memory fragmentation</strong></li>
</ul>
</section>
<section id="distributed-training-challenges" class="level3">
<h3 class="anchored" data-anchor-id="distributed-training-challenges">Distributed Training Challenges</h3>
<p>When scaling to multiple GPUs, new challenges emerge:</p>
<p><strong>1. Data Parallelism Memory Replication</strong> Traditional DDP replicates the entire model on each GPU, which doesn’t reduce per-GPU memory usage—it only enables larger batch sizes.</p>
<p><strong>2. Communication Overhead</strong> Distributed training introduces collective communication operations: - <strong>All-Reduce</strong>: Synchronize gradients across all GPUs (DDP) - <strong>All-Gather</strong>: Collect sharded parameters/gradients from all GPUs (FSDP) - <strong>Reduce-Scatter</strong>: Reduce and distribute results across GPUs (FSDP)</p>
<p><strong>3. Synchronization Barriers</strong> GPUs must wait for each other at communication points, potentially causing idle time if workloads are imbalanced.</p>
<p><strong>4. Network Bandwidth</strong> The <strong>data transfer rate</strong> between GPUs, measured in GB/s (gigabytes per second). Higher bandwidth enables faster gradient/parameter synchronization. H100s use NVLink (900 GB/s bidirectional), allowing near-simultaneous compute and communication. Lower bandwidth (e.g., PCIe: 32 GB/s) creates bottlenecks where GPUs wait idle during all-reduce/all-gather operations.</p>
<p><strong>5. Load Balancing</strong> Uneven data distribution or computational load can cause some GPUs to finish early and wait idle.</p>
</section>
</section>
<section id="communication-vs.-computation-trade-off" class="level2">
<h2 class="anchored" data-anchor-id="communication-vs.-computation-trade-off">Communication vs.&nbsp;Computation Trade-off</h2>
<p>The fundamental challenge in distributed training is <strong>hiding communication latency behind computation</strong>. Ideal distributed training achieves:</p>
<pre><code>Compute Efficiency = (Compute Time) / (Total Time) × 100%</code></pre>
<p>Where <code>Total Time = Compute Time + Communication Time + Idle Time</code>.</p>
<p><strong>Single GPU Baseline:</strong> - Compute: 97.87% - Communication: 1.16% (memory transfers, kernel overhead) - Idle: 0.97%</p>
<p><strong>Distributed Training Goal:</strong> Match or approach baseline efficiency while enabling: - Training models that don’t fit on a single GPU - Faster training through data parallelism - Flexibility to scale to larger clusters</p>
</section>
<section id="gradient-accumulation-and-synchronization" class="level2">
<h2 class="anchored" data-anchor-id="gradient-accumulation-and-synchronization">Gradient Accumulation and Synchronization</h2>
<section id="single-gpu-approach" class="level3">
<h3 class="anchored" data-anchor-id="single-gpu-approach">Single GPU Approach</h3>
<p>Gradient accumulation simply accumulates gradients over multiple micro-batches before updating weights:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> micro_batch <span class="kw">in</span> <span class="bu">range</span>(grad_accumulation_steps):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> forward_backward(micro_batch)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    loss.backward()  <span class="co"># Accumulate gradients</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>optimizer.step()  <span class="co"># Update weights</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>optimizer.zero_grad()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="distributed-training-synchronization" class="level3">
<h3 class="anchored" data-anchor-id="distributed-training-synchronization">Distributed Training Synchronization</h3>
<p>Must decide when to synchronize gradients:</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Warning</span>The Synchronization Problem
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Synchronizing every micro-batch <strong>wastes bandwidth</strong></li>
<li>Using <code>no_sync()</code> context manager <strong>prevents unnecessary all-reduce operations</strong></li>
<li>Only synchronize on the <strong>last micro-batch</strong> of gradient accumulation</li>
</ul>
</div>
</div>
<p>Here’s the optimized approach:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, micro_batch <span class="kw">in</span> <span class="bu">enumerate</span>(batches):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    is_last <span class="op">=</span> (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> grad_accumulation_steps <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> is_last:</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> model.no_sync():  <span class="co"># Skip gradient synchronization</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        loss.backward()  <span class="co"># Synchronize gradients across GPUs</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This optimization reduces communication overhead significantly. The experimental results show that DDP with proper gradient accumulation achieves <strong>97.36% compute efficiency</strong>.</p>
</section>
</section>
<section id="memory-sharding-strategies" class="level2">
<h2 class="anchored" data-anchor-id="memory-sharding-strategies">Memory Sharding Strategies</h2>
<p>To overcome memory limitations, <strong>Fully Sharded Data Parallel (FSDP)</strong> implements the ZeRO (Zero Redundancy Optimizer) approach:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 27%">
<col style="width: 28%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>DDP (No Sharding)</th>
<th>FSDP-Grad (ZeRO-2)</th>
<th>FSDP-Full (ZeRO-3)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Parameters</td>
<td>Replicated</td>
<td>Replicated</td>
<td>Sharded</td>
</tr>
<tr class="even">
<td>Gradients</td>
<td>Synchronized</td>
<td>Sharded</td>
<td>Sharded</td>
</tr>
<tr class="odd">
<td>Optimizer States</td>
<td>Replicated</td>
<td>Sharded</td>
<td>Sharded</td>
</tr>
<tr class="even">
<td><strong>Memory per GPU</strong></td>
<td>100%</td>
<td>~67%</td>
<td>~33%</td>
</tr>
<tr class="odd">
<td><strong>Communication</strong></td>
<td>All-Reduce</td>
<td>All-Gather (grads)</td>
<td>All-Gather (params) + Reduce-Scatter (grads)</td>
</tr>
</tbody>
</table>
<div data-align="center">
<em>Table 1: Comparison of Memory Sharding Strategies for Distributed Training</em>
</div>
<p>For GPT-2 Large on 2 GPUs: - <strong>DDP</strong>: 12GB per GPU (no savings) - <strong>FSDP-Grad</strong>: ~8GB per GPU (1.5x reduction) - <strong>FSDP-Full</strong>: ~4GB per GPU (3x reduction)</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>The Trade-off
</div>
</div>
<div class="callout-body-container callout-body">
<p>More aggressive sharding reduces memory but increases communication frequency and complexity.</p>
</div>
</div>
<hr>
</section>
</section>
<section id="distributed-training-approaches" class="level1">
<h1>Distributed Training Approaches</h1>
<section id="baseline-single-gpu-training" class="level2">
<h2 class="anchored" data-anchor-id="baseline-single-gpu-training">Baseline: Single GPU Training</h2>
<p>The baseline implementation serves as the reference point for performance comparison.</p>
<p><strong>Implementation</strong> (<code>train_baseline.py</code>):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Device setup</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Model initialization</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> AutoConfig.from_pretrained(<span class="st">"gpt2-large"</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MyGPT2LMHeadModel(config)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>model.init_weights()</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.to(device)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient accumulation to match distributed global batch size</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>global_batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>micro_batch_size <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>grad_acc_steps <span class="op">=</span> global_batch_size <span class="op">//</span> micro_batch_size  <span class="co"># 4 steps</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop with gradient accumulation</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch <span class="kw">in</span> dataloader:</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> model(batch)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    scaled_loss <span class="op">=</span> loss <span class="op">/</span> grad_acc_steps</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    scaled_loss.backward()</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (step <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> grad_acc_steps <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Key Characteristics:</strong></p>
<ul>
<li>Simple, straightforward implementation</li>
<li>No communication overhead</li>
<li>Limited by single GPU memory capacity</li>
<li>Serves as performance upper bound (97.87% compute efficiency)</li>
</ul>
<p><strong>Performance Metrics:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>Training Time: <span class="fl">112.3</span><span class="er">s</span> <span class="cf">for</span> <span class="dv">20</span> steps</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>Peak Memory: <span class="dv">3</span>,<span class="dv">0</span><span class="er">76</span> MB</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>Compute Efficiency: <span class="fl">97.87</span><span class="op">%</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>Throughput: <span class="dv">32</span>,<span class="dv">582</span> tokens<span class="op">/</span>sec (batch size <span class="dv">8</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="data-parallel-ddp" class="level2">
<h2 class="anchored" data-anchor-id="data-parallel-ddp">Data Parallel (DDP)</h2>
<section id="concept" class="level3">
<h3 class="anchored" data-anchor-id="concept">Concept</h3>
<p>DDP replicates the model on each GPU and synchronizes gradients after the backward pass using an all-reduce collective operation.</p>
<p><strong>How It Works:</strong></p>
<ol type="1">
<li>Each GPU maintains a complete copy of the model</li>
<li>Each GPU processes a different subset of the data (data parallelism)</li>
<li>After backward pass, gradients are averaged across all GPUs using all-reduce</li>
<li>All GPUs update their models identically using synchronized gradients</li>
</ol>
</section>
<section id="implementation" class="level3">
<h3 class="anchored" data-anchor-id="implementation">Implementation</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.distributed <span class="im">as</span> dist</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.parallel <span class="im">import</span> DistributedDataParallel <span class="im">as</span> DDP</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize distributed process group</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>dist.init_process_group(backend<span class="op">=</span><span class="st">'nccl'</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>rank <span class="op">=</span> dist.get_rank()</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>world_size <span class="op">=</span> dist.get_world_size()</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>local_rank <span class="op">=</span> <span class="bu">int</span>(os.environ[<span class="st">'LOCAL_RANK'</span>])</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Set device for this process</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>torch.cuda.set_device(local_rank)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="ss">f'cuda:</span><span class="sc">{</span>local_rank<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Critical: All ranks must use same seed for identical initialization</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>torch.cuda.manual_seed(<span class="dv">42</span>)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Create model and wrap with DDP</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MyGPT2LMHeadModel(config)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>model.init_weights()</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.to(device)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> DDP(model, device_ids<span class="op">=</span>[local_rank])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="key-implementation-detail-gradient-synchronization-with-no_sync" class="level3">
<h3 class="anchored" data-anchor-id="key-implementation-detail-gradient-synchronization-with-no_sync">Key Implementation Detail: Gradient Synchronization with <code>no_sync()</code></h3>
<p>From <code>distributed_trainer.py</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> training_step(<span class="va">self</span>, inputs, targets, should_sync: <span class="bu">bool</span>):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Training step with DDP no_sync support.</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Key concept: For gradient accumulation, we only sync gradients</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co">    on the last micro-batch to avoid unnecessary communication overhead.</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward pass</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> <span class="va">self</span>.model(inputs)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> nn.functional.cross_entropy(</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        logits.view(<span class="op">-</span><span class="dv">1</span>, logits.size(<span class="op">-</span><span class="dv">1</span>)),</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        targets.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backward with conditional synchronization</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    scaled_loss <span class="op">=</span> loss <span class="op">/</span> <span class="va">self</span>.grad_accumulation_steps</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="va">self</span>.ddp_enabled <span class="kw">and</span> <span class="kw">not</span> should_sync:</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Don't sync gradients for intermediate micro-batches</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="va">self</span>.model.no_sync():</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>            scaled_loss.backward()</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sync gradients on last micro-batch (triggers all-reduce)</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>        scaled_loss.backward()</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Why <code>no_sync()</code> Matters
</div>
</div>
<div class="callout-body-container callout-body">
<p>Without this optimization, DDP would perform all-reduce after every <code>backward()</code> call:</p>
<ul>
<li>With 4 micro-batches per step: <strong>4 × all-reduce operations</strong></li>
<li>With <code>no_sync()</code>: <strong>Only 1 all-reduce operation</strong> (on the last micro-batch)</li>
<li>Result: <strong>4x reduction in communication overhead</strong></li>
</ul>
</div>
</div>
</section>
<section id="communication-pattern" class="level3">
<h3 class="anchored" data-anchor-id="communication-pattern">Communication Pattern</h3>
<ul>
<li><strong>Operation</strong>: All-Reduce (averages gradients across all ranks)</li>
<li><strong>Frequency</strong>: Once per gradient accumulation cycle (every 2 micro-batches with 2 GPUs)</li>
<li><strong>Data Volume</strong>: All gradient tensors (~475 MB)</li>
<li><strong>NCCL Implementation</strong>: Ring all-reduce algorithm</li>
</ul>
</section>
<section id="performance-metrics" class="level3">
<h3 class="anchored" data-anchor-id="performance-metrics">Performance Metrics</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>Training Time: <span class="fl">58.3</span><span class="er">s</span> (<span class="fl">1.93</span><span class="er">x</span> speedup over baseline)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>Communication Overhead: <span class="fl">105.3</span> ms</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>Compute Efficiency: <span class="fl">97.36</span><span class="op">%</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>Communication Overlap: <span class="fl">92.50</span><span class="op">%</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="advantages-limitations" class="level3">
<h3 class="anchored" data-anchor-id="advantages-limitations">Advantages &amp; Limitations</h3>
<p><strong>Advantages:</strong></p>
<p>✅ Simple to implement (single <code>DDP()</code> wrapper)</p>
<p>✅ Excellent performance when model fits in memory</p>
<p>✅ Minimal code changes from single-GPU training</p>
<p><strong>Limitations:</strong></p>
<p>❌ No memory savings (full model replication)</p>
<p>❌ Highest communication overhead among strategies tested</p>
<p>❌ Doesn’t scale to models larger than single GPU capacity</p>
</section>
</section>
<section id="fully-sharded-data-parallel---full-sharding-fsdp-full-zero-3" class="level2">
<h2 class="anchored" data-anchor-id="fully-sharded-data-parallel---full-sharding-fsdp-full-zero-3">Fully Sharded Data Parallel - Full Sharding (FSDP-Full / ZeRO-3)</h2>
<section id="concept-1" class="level3">
<h3 class="anchored" data-anchor-id="concept-1">Concept</h3>
<p>FSDP-Full implements ZeRO Stage 3, sharding parameters, gradients, and optimizer states across all GPUs. Each GPU only stores a fraction of the model, reconstructing full parameters on-demand during forward/backward passes.</p>
<p><strong>How It Works:</strong></p>
<ol type="1">
<li>Model parameters are sharded across GPUs (each GPU holds 1/N of weights)</li>
<li>Before each layer’s forward pass: <strong>all-gather</strong> to reconstruct full parameters</li>
<li>After forward pass: parameters are freed, only shards retained</li>
<li>Before backward pass: <strong>all-gather</strong> parameters again</li>
<li>After backward pass: <strong>reduce-scatter</strong> to shard gradients across GPUs</li>
<li>Optimizer updates only local parameter shards</li>
</ol>
</section>
<section id="implementation-1" class="level3">
<h3 class="anchored" data-anchor-id="implementation-1">Implementation</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.distributed.fsdp <span class="im">import</span> FullyShardedDataParallel <span class="im">as</span> FSDP</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.distributed.fsdp <span class="im">import</span> ShardingStrategy</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> setup_fsdp_model(model, device, sharding_strategy_name<span class="op">=</span><span class="st">'FULL_SHARD'</span>):</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Wrap model with FSDP using per-layer wrapping.</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Sharding strategies:</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co">    - FULL_SHARD: Parameters, gradients, optimizer states all sharded</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co">      -&gt; Max memory savings, more communication</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co">    - SHARD_GRAD_OP: Only gradients and optimizer states sharded</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co">      -&gt; Moderate savings, less communication</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> model.to(device)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    strategy_map <span class="op">=</span> {</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>        <span class="st">'FULL_SHARD'</span>: ShardingStrategy.FULL_SHARD,</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>        <span class="st">'SHARD_GRAD_OP'</span>: ShardingStrategy.SHARD_GRAD_OP,</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>        <span class="st">'NO_SHARD'</span>: ShardingStrategy.NO_SHARD</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    sharding_strategy <span class="op">=</span> strategy_map[sharding_strategy_name]</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Per-layer wrapping for fine-grained control</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Each transformer block is wrapped individually</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, block <span class="kw">in</span> <span class="bu">enumerate</span>(model.transformer.h):</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>        model.transformer.h[i] <span class="op">=</span> FSDP(</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>            block,</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>            sharding_strategy<span class="op">=</span>sharding_strategy</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Wrap entire model</span></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> FSDP(model, sharding_strategy<span class="op">=</span>sharding_strategy)</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Per-Layer Wrapping is Critical
</div>
</div>
<div class="callout-body-container callout-body">
<p>Per-layer wrapping enables:</p>
<ol type="1">
<li><strong>Granular Memory Management</strong>: Each transformer block can independently gather/shard parameters</li>
<li><strong>Better Overlap</strong>: While layer N is computing, layer N+1 can prefetch parameters</li>
<li><strong>Reduced Peak Memory</strong>: Only one layer’s full parameters need to be materialized at a time</li>
<li><strong>Communication Efficiency</strong>: Smaller, more frequent all-gathers overlap better with computation</li>
</ol>
</div>
</div>
</section>
<section id="communication-pattern-1" class="level3">
<h3 class="anchored" data-anchor-id="communication-pattern-1">Communication Pattern</h3>
<p>For a 36-layer GPT-2 Large model:</p>
<ul>
<li><p><strong>All-Gather Operations</strong>: ~73 calls (forward + backward for each layer)</p>
<ul>
<li>Each call gathers 1/N parameters from all GPUs</li>
<li>Total duration: 50,074 μs (50.1 ms)</li>
<li>Average per call: ~686 μs</li>
</ul></li>
<li><p><strong>Reduce-Scatter Operations</strong>: Shard and reduce gradients</p></li>
</ul>
</section>
<section id="memory-breakdown-per-gpu-2-gpus-total" class="level3">
<h3 class="anchored" data-anchor-id="memory-breakdown-per-gpu-2-gpus-total">Memory Breakdown (per GPU, 2 GPUs total)</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>Parameters:  <span class="dv">6</span> GB <span class="op">/</span> <span class="dv">2</span> <span class="op">=</span> <span class="dv">3</span> GB per GPU (sharded)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>Gradients:   <span class="dv">6</span> GB <span class="op">/</span> <span class="dv">2</span> <span class="op">=</span> <span class="dv">3</span> GB per GPU (sharded)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>Optimizer:   <span class="dv">2</span> GB <span class="op">/</span> <span class="dv">2</span> <span class="op">=</span> <span class="dv">1</span> GB per GPU (sharded)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>Activations: <span class="op">~</span><span class="fl">1.5</span> GB per GPU (<span class="kw">not</span> sharded)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>Peak Memory: <span class="op">~</span><span class="dv">4</span> GB per GPU (<span class="dv">3</span><span class="er">x</span> reduction vs. baseline)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="performance-metrics-1" class="level3">
<h3 class="anchored" data-anchor-id="performance-metrics-1">Performance Metrics</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>Training Time: <span class="fl">58.8</span><span class="er">s</span> (<span class="fl">1.91</span><span class="er">x</span> speedup)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>Communication Overhead: <span class="fl">94.7</span> ms</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>Compute Efficiency: <span class="fl">96.98</span><span class="op">%</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>Communication Overlap: <span class="fl">95.14</span><span class="op">%</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>Idle Time: <span class="fl">117.2</span> ms (highest due to frequent synchronization)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="advantages-trade-offs" class="level3">
<h3 class="anchored" data-anchor-id="advantages-trade-offs">Advantages &amp; Trade-offs</h3>
<p><strong>Advantages:</strong></p>
<p>✅ Maximum memory savings (3x with 2 GPUs, scales to 8x+ with more GPUs)</p>
<p>✅ Enables training models that don’t fit on a single GPU</p>
<p>✅ Lower communication overhead than DDP despite more operations</p>
<p><strong>Trade-offs:</strong></p>
<p>⚠️ More complex implementation</p>
<p>⚠️ Higher idle time (117ms) from 73 synchronization points</p>
<p>⚠️ Parameter gathering overhead during forward/backward passes</p>
</section>
</section>
<section id="fully-sharded-data-parallel---gradient-sharding-fsdp-grad-zero-2" class="level2">
<h2 class="anchored" data-anchor-id="fully-sharded-data-parallel---gradient-sharding-fsdp-grad-zero-2">Fully Sharded Data Parallel - Gradient Sharding (FSDP-Grad / ZeRO-2)</h2>
<section id="concept-2" class="level3">
<h3 class="anchored" data-anchor-id="concept-2">Concept</h3>
<p>FSDP-Grad implements ZeRO Stage 2, sharding only gradients and optimizer states while keeping parameters replicated. This represents the <strong>sweet spot</strong> between memory savings and performance.</p>
<p><strong>How It Works:</strong></p>
<ol type="1">
<li><p>Full model parameters are replicated on each GPU (like DDP)</p></li>
<li><p>Gradients are sharded across GPUs during backward pass</p></li>
<li><p>Optimizer states are sharded (each GPU only stores 1/N of Adam moments)</p></li>
<li><p>After backward: <strong>all-gather</strong> to collect full gradients for optimizer step</p></li>
<li><p>Each GPU updates only its shard of optimizer states</p></li>
</ol>
</section>
<section id="implementation-2" class="level3">
<h3 class="anchored" data-anchor-id="implementation-2">Implementation</h3>
<p>The implementation is identical to FSDP-Full, but uses a different sharding strategy:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Same setup_fsdp_model function, different strategy</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> setup_fsdp_model(model, device, sharding_strategy_name<span class="op">=</span><span class="st">'SHARD_GRAD_OP'</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># ShardingStrategy.SHARD_GRAD_OP tells FSDP to:</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co"># - Keep parameters replicated (no all-gather for params)</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># - Shard gradients (reduce-scatter after backward)</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># - Shard optimizer states (memory savings)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-fsdp-grad-excels" class="level3">
<h3 class="anchored" data-anchor-id="why-fsdp-grad-excels">Why FSDP-Grad Excels</h3>
<ol type="1">
<li><strong>No Parameter Gathering Overhead</strong>: Parameters always available for forward/backward</li>
<li><strong>Fewer Synchronization Points</strong>: Only 37 all-gather calls (vs.&nbsp;73 for FSDP-Full)</li>
<li><strong>Consolidated Communication</strong>: Gradient gathering happens during optimizer step (natural idle time)</li>
<li><strong>Better Overlap Opportunity</strong>: Fewer synchronization points = fewer pipeline bubbles</li>
</ol>
</section>
<section id="communication-pattern-2" class="level3">
<h3 class="anchored" data-anchor-id="communication-pattern-2">Communication Pattern</h3>
<ul>
<li><strong>All-Gather Operations</strong>: 37 calls (gathering gradients + optimizer states)
<ul>
<li>Total duration: 106,828 μs (106.8 ms)</li>
<li>Average per call: ~2,887 μs</li>
</ul></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>The Paradox
</div>
</div>
<div class="callout-body-container callout-body">
<p>FSDP-Grad has <strong>higher total communication time</strong> (106.8ms) than FSDP-Full (50.1ms), but achieves <strong>better performance</strong> due to 98.06% overlap vs.&nbsp;95.14%!</p>
<p><strong>Why?</strong></p>
<pre><code>Exposed Communication = Total Communication × (1 - Overlap %)

FSDP-Grad: 106.8ms × 1.94% = ~2ms exposed communication
FSDP-Full: 50.1ms × 4.86% = ~2.5ms exposed communication
DDP:       43.4ms × 7.5% = ~3.2ms exposed communication</code></pre>
<p><strong>Overlap efficiency matters more than raw communication time!</strong></p>
</div>
</div>
</section>
<section id="memory-breakdown-per-gpu-2-gpus-total-1" class="level3">
<h3 class="anchored" data-anchor-id="memory-breakdown-per-gpu-2-gpus-total-1">Memory Breakdown (per GPU, 2 GPUs total)</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>Parameters:  <span class="dv">6</span> GB per GPU (replicated <span class="op">-</span> full copy on each GPU)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>Gradients:   <span class="dv">6</span> GB <span class="op">/</span> <span class="dv">2</span> <span class="op">=</span> <span class="dv">3</span> GB per GPU (sharded)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>Optimizer:   <span class="dv">2</span> GB <span class="op">/</span> <span class="dv">2</span> <span class="op">=</span> <span class="dv">1</span> GB per GPU (sharded)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>Activations: <span class="op">~</span><span class="fl">1.5</span> GB per GPU (<span class="kw">not</span> sharded)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>Peak Memory: <span class="op">~</span><span class="dv">8</span> GB per GPU (<span class="fl">1.5</span><span class="er">x</span> reduction vs. baseline)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="performance-metrics-2" class="level3">
<h3 class="anchored" data-anchor-id="performance-metrics-2">Performance Metrics</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>Training Time: <span class="fl">58.6</span><span class="er">s</span> (<span class="fl">1.92</span><span class="er">x</span> speedup)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>Communication Overhead: <span class="fl">87.1</span> ms (lowest among distributed strategies)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>Compute Efficiency: <span class="fl">97.71</span><span class="op">%</span> (closest to baseline)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>Communication Overlap: <span class="fl">98.06</span><span class="op">%</span> (highest <span class="op">-</span> near perfect)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>Idle Time: <span class="fl">72.6</span> ms (only <span class="dv">5</span><span class="er">ms</span> more than baseline)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="advantages-trade-offs-1" class="level3">
<h3 class="anchored" data-anchor-id="advantages-trade-offs-1">Advantages &amp; Trade-offs</h3>
<p><strong>Advantages:</strong></p>
<p>✅ Best performance-memory trade-off</p>
<p>✅ Highest compute efficiency among distributed strategies (97.71%)</p>
<p>✅ Near-perfect communication overlap (98.06%)</p>
<p>✅ Minimal idle time overhead (+5ms vs.&nbsp;baseline)</p>
<p>✅ Enables training models 1.5-2x larger than single GPU</p>
<p><strong>Trade-offs:</strong></p>
<p>⚠️ Less memory savings than FSDP-Full (1.5x vs.&nbsp;3x)</p>
<p>⚠️ Still requires parameters to fit on each GPU</p>
<p>⚠️ Not suitable for extremely large models (&gt;100B parameters on 80GB GPUs)</p>
<hr>
</section>
</section>
</section>
<section id="performance-analysis" class="level1">
<h1>Performance Analysis</h1>
<section id="throughput-analysis" class="level2">
<h2 class="anchored" data-anchor-id="throughput-analysis">Throughput Analysis</h2>
<section id="single-gpu-batch-size-scaling" class="level3">
<h3 class="anchored" data-anchor-id="single-gpu-batch-size-scaling">Single GPU Batch Size Scaling</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Batch Size</th>
<th>Tokens/sec</th>
<th>Memory (MB)</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>16,521</td>
<td>15,619</td>
<td>✓</td>
</tr>
<tr class="even">
<td>4</td>
<td>30,819</td>
<td>15,619</td>
<td>✓</td>
</tr>
<tr class="odd">
<td>8</td>
<td>32,585</td>
<td>16,104</td>
<td>✓</td>
</tr>
<tr class="even">
<td>16</td>
<td>33,900</td>
<td>30,224</td>
<td>✓</td>
</tr>
<tr class="odd">
<td>32</td>
<td>34,560</td>
<td>58,438</td>
<td>✓</td>
</tr>
<tr class="even">
<td>64</td>
<td>-</td>
<td>-</td>
<td>OOM</td>
</tr>
</tbody>
</table>
<div data-align="center">
<em>Table 2: Throughput and Memory Usage Across Different Batch Sizes on Single H100 GPU</em>
</div>
<p><strong>Key Observations:</strong></p>
<ol type="1">
<li><p><strong>Throughput Scaling</strong>: Tokens/sec increases with batch size but plateaus around batch size 16-32</p>
<ul>
<li>Batch 1→4: +87% throughput increase</li>
<li>Batch 4→8: +6% throughput increase</li>
<li>Batch 8→16: +4% throughput increase</li>
<li>Batch 16→32: +2% throughput increase</li>
</ul></li>
<li><p><strong>Memory Efficiency</strong>: Small batches underutilize GPU (15.6GB for batch 1-4, only 19% of 80GB capacity)</p></li>
<li><p><strong>OOM Boundary</strong>: Batch size 64 exceeds memory capacity, confirming ~60GB limit for this model configuration</p></li>
<li><p><strong>Optimal Batch Size</strong>: Batch size 8-16 offers best throughput-memory trade-off</p></li>
</ol>
</section>
<section id="distributed-training-speedup" class="level3">
<h3 class="anchored" data-anchor-id="distributed-training-speedup">Distributed Training Speedup</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 22%">
<col style="width: 31%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th>Strategy</th>
<th>Training Time</th>
<th>Speedup vs Baseline</th>
<th>Effective Throughput</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Baseline</td>
<td>112.3s</td>
<td>1.0x</td>
<td>32,582 tokens/sec</td>
</tr>
<tr class="even">
<td>DDP</td>
<td>58.3s</td>
<td>1.93x</td>
<td>62,870 tokens/sec</td>
</tr>
<tr class="odd">
<td>FSDP-Full</td>
<td>58.8s</td>
<td>1.91x</td>
<td>62,244 tokens/sec</td>
</tr>
<tr class="even">
<td>FSDP-Grad</td>
<td>58.6s</td>
<td>1.92x</td>
<td>62,495 tokens/sec</td>
</tr>
</tbody>
</table>
<div data-align="center">
<em>Table 3: Training Time and Throughput Comparison Across Distributed Strategies</em>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>All distributed strategies achieve <strong>near-linear speedup (1.9x with 2 GPUs)</strong>, indicating excellent scaling efficiency.</p>
</div>
</div>
</section>
</section>
<section id="memory-profiling-analysis" class="level2">
<h2 class="anchored" data-anchor-id="memory-profiling-analysis">Memory Profiling Analysis</h2>
<p>Detailed memory profiling was performed using PyTorch’s memory snapshot visualization (<code>torch.cuda.memory._record_memory_history()</code>).</p>
<section id="memory-usage-patterns-baseline-single-gpu" class="level3">
<h3 class="anchored" data-anchor-id="memory-usage-patterns-baseline-single-gpu">Memory Usage Patterns (Baseline Single GPU)</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>Estimated Memory:</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>  Parameters:       <span class="fl">474.70</span> MB (FP16)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>  Gradients:        <span class="fl">474.70</span> MB (FP16)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>  Optimizer States: <span class="fl">949.40</span> MB (FP32: momentum <span class="op">+</span> variance <span class="cf">for</span> AdamW)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>  Total Estimated:  <span class="dv">1</span>,<span class="fl">898.80</span> MB</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>Actual Peak Memory:</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>  Peak Allocated:   <span class="dv">3</span>,<span class="fl">076.27</span> MB</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>  Reserved:         <span class="dv">15</span>,<span class="fl">890.00</span> MB</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>Memory Overhead:</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>  Allocated vs Estimated: <span class="dv">1</span>,<span class="fl">177.47</span> MB (<span class="dv">62</span><span class="op">%</span> overhead)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>  Reserved vs Allocated: <span class="dv">12</span>,<span class="fl">813.73</span> MB (unused reserved memory)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What Causes the 1,177 MB Overhead?</strong></p>
<ol type="1">
<li><p><strong>Activations</strong>: ~800-900 MB</p>
<ul>
<li>Each transformer layer saves activations for backward pass</li>
<li>For 36 layers with batch size 8 and sequence length 1024</li>
<li>Not included in parameter/gradient/optimizer estimation</li>
</ul></li>
<li><p><strong>Temporary Buffers</strong>: ~200-300 MB</p>
<ul>
<li>Intermediate computation results</li>
<li>Attention score matrices (batch × heads × seq × seq)</li>
<li>Layer norm statistics</li>
</ul></li>
<li><p><strong>Framework Overhead</strong>: ~100-150 MB</p>
<ul>
<li>PyTorch’s internal bookkeeping</li>
<li>Autograd graph metadata</li>
<li>CUDA context and kernel caches</li>
</ul></li>
</ol>
</section>
<section id="active-memory-timeline-memory-usage-across-training" class="level3">
<h3 class="anchored" data-anchor-id="active-memory-timeline-memory-usage-across-training">Active Memory Timeline: Memory Usage Across Training</h3>
<p><strong>Overview</strong>: This visualization shows the stacked memory allocations over 5,093 memory snapshot entries spanning the training process.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/active_memory_timeline.png" class="img-fluid figure-img"></p>
<figcaption>Active Memory Timeline</figcaption>
</figure>
</div>
<div data-align="center">
<em>Figure 1: Active Memory Timeline showing three training iterations with characteristic sawtooth pattern indicating clean memory allocation and deallocation cycles</em>
</div>
<p><strong>Key Observations:</strong></p>
<p><strong>Three Distinct Training Phases</strong>:</p>
<ul>
<li><strong>Phase 1 (Left)</strong>: Initial training iteration with peak ~12GB memory usage</li>
<li><strong>Phase 2 (Middle)</strong>: Second iteration showing similar peak with different allocation pattern</li>
<li><strong>Phase 3 (Right)</strong>: Third iteration with memory pattern variations</li>
</ul>
<p><strong>Memory Valleys Between Peaks</strong>:</p>
<ul>
<li>Memory drops to ~2-3GB baseline between iterations</li>
<li>This indicates <strong>good memory cleanup</strong> - tensors from forward/backward passes are being freed</li>
<li>The persistent 2-3GB represents model parameters, optimizer states, and framework overhead</li>
</ul>
<p><strong>Color-Coded Allocation Categories</strong>: The stacked colored bands represent different memory categories:</p>
<ul>
<li><strong>Large cyan/teal sections</strong>: Likely activations from forward pass</li>
<li><strong>Large pink/red sections</strong>: Different allocation pattern, possibly gradient accumulation</li>
<li><strong>Large orange/brown sections</strong>: Third pattern variation</li>
<li>Multiple smaller colored bands: Temporary buffers, intermediate tensors, framework allocations</li>
</ul>
<p><strong>Memory Allocation Efficiency</strong>:</p>
<ul>
<li>Clean sawtooth pattern indicates efficient memory reuse</li>
<li>No concerning memory leaks (would show as rising baseline)</li>
<li>Valleys confirm proper tensor deallocation</li>
</ul>
</section>
<section id="active-cache-segment-timeline-memory-fragmentation-view" class="level3">
<h3 class="anchored" data-anchor-id="active-cache-segment-timeline-memory-fragmentation-view">Active Cache Segment Timeline: Memory Fragmentation View</h3>
<p><strong>Overview</strong>: This visualization shows how PyTorch’s caching allocator manages memory segments over time.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/active_cache_segment_Timeline.png" class="img-fluid figure-img"></p>
<figcaption>Active Cache Segment Timeline</figcaption>
</figure>
</div>
<div data-align="center">
<em>Figure 2: Active Cache Segment Timeline demonstrating linear memory growth with no fragmentation, showing PyTorch’s efficient caching allocator managing memory segments sequentially</em>
</div>
<p><strong>Key Observations:</strong></p>
<p><strong>Linear Memory Growth Pattern</strong>:</p>
<ul>
<li>Memory usage grows <strong>linearly and smoothly</strong> from ~0 to ~16GB</li>
<li>This is the view from a <strong>single training iteration</strong></li>
<li>Each colored band represents a <strong>cached memory segment</strong> allocated by PyTorch</li>
</ul>
<p><strong>Excellent Memory Allocation Behavior</strong>:</p>
<ul>
<li><strong>No visible fragmentation</strong>: The smooth, sequential stacking indicates allocations are contiguous</li>
<li><strong>No gaps or holes</strong>: PyTorch’s allocator is efficiently reusing freed memory</li>
<li><strong>Clean segment boundaries</strong>: Each color represents a separate cached block</li>
</ul>
<p><strong>Why This Matters for H100</strong>:</p>
<ul>
<li>H100 has 80GB HBM3 memory with high bandwidth (3+ TB/s)</li>
<li>The ~16GB peak usage leaves substantial headroom (64GB free)</li>
<li>Contiguous allocations enable optimal memory coalescing</li>
<li>No fragmentation means no performance penalty from scattered memory access</li>
</ul>
</section>
<section id="allocator-state-history-detailed-fragmentation-analysis" class="level3">
<h3 class="anchored" data-anchor-id="allocator-state-history-detailed-fragmentation-analysis">Allocator State History: Detailed Fragmentation Analysis</h3>
<p><strong>Overview</strong>: This is the most detailed view showing individual memory segments and their allocation/deallocation patterns over time.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/allocator_state_story.png" class="img-fluid figure-img"></p>
<figcaption>Allocator State History</figcaption>
</figure>
</div>
<div data-align="center">
<em>Figure 3: Allocator State History revealing individual memory segment lifecycle with frequent allocation/deallocation patterns (colorful periods) and persistent model state (large blocks at bottom)</em>
</div>
<p><strong>Key Observations:</strong></p>
<p><strong>Top Section - Active Allocations</strong>:</p>
<ul>
<li>Multiple horizontal bars represent different memory segments</li>
<li><strong>Colorful periods</strong>: Memory is actively allocated and in use</li>
<li><strong>White/empty periods</strong>: Memory has been freed and returned to cache</li>
<li>Pattern shows <strong>frequent allocation and deallocation</strong> of many small-to-medium tensors</li>
</ul>
<p><strong>Bottom Section - Large Persistent Allocations</strong>:</p>
<ul>
<li><strong>Large teal/cyan block</strong>: A single large allocation that persists</li>
<li>This is likely the <strong>model parameters</strong> or <strong>optimizer state</strong></li>
<li>Remains allocated throughout training (as expected)</li>
</ul>
<p><strong>PyTorch Allocator Efficiency</strong>:</p>
<ul>
<li>The allocator is <strong>successfully coalescing</strong> freed memory</li>
<li><strong>No visible memory holes</strong> that would indicate fragmentation</li>
<li>The pattern suggests the allocator’s <strong>caching strategy is working well</strong></li>
</ul>
</section>
<section id="cross-analysis-memory-usage-vs-communication-overhead" class="level3">
<h3 class="anchored" data-anchor-id="cross-analysis-memory-usage-vs-communication-overhead">Cross-Analysis: Memory Usage vs Communication Overhead</h3>
<p>Combining memory profiling with performance metrics across all strategies:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 19%">
<col style="width: 22%">
<col style="width: 16%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th>Strategy</th>
<th>Peak Memory</th>
<th>Communication</th>
<th>Overlap %</th>
<th>Memory Efficiency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Baseline</strong></td>
<td>~12GB</td>
<td>0ms</td>
<td>N/A</td>
<td>100% (single GPU)</td>
</tr>
<tr class="even">
<td><strong>DDP</strong></td>
<td>~12GB (×2 ranks)</td>
<td>105ms</td>
<td>92.50%</td>
<td>100% (full replication)</td>
</tr>
<tr class="odd">
<td><strong>FSDP_FULL</strong></td>
<td>~4GB (×2 ranks)</td>
<td>95ms</td>
<td>95.14%</td>
<td>300% (3× memory savings)</td>
</tr>
<tr class="even">
<td><strong>FSDP_GRAD</strong></td>
<td>~8GB (×2 ranks)</td>
<td>87ms</td>
<td>98.06%</td>
<td>150% (1.5× memory savings)</td>
</tr>
</tbody>
</table>
<div data-align="center">
<em>Table 12: Memory-Performance Trade-off Analysis Across Distributed Strategies</em>
</div>
<p><strong>Key Insights:</strong></p>
<ol type="1">
<li><p><strong>FSDP_FULL’s 4GB per rank</strong> (vs 12GB baseline) comes from sharding:</p>
<ul>
<li>Parameters: Sharded across 2 ranks (6GB → 3GB per rank)</li>
<li>Gradients: Sharded (3GB → 1.5GB per rank)</li>
<li>Optimizer states: Sharded (2GB → 1GB per rank)</li>
<li>Activations: NOT sharded (~1.5GB per rank)</li>
</ul></li>
<li><p><strong>FSDP_GRAD’s 8GB per rank</strong> keeps full parameters but shards:</p>
<ul>
<li>Parameters: Full on each rank (6GB per rank)</li>
<li>Gradients: Sharded (3GB → 1.5GB per rank)</li>
<li>Optimizer states: Sharded (2GB → 1GB per rank)</li>
<li>Activations: NOT sharded (~1.5GB per rank)</li>
</ul></li>
<li><p><strong>Memory-Performance Trade-off</strong>:</p>
<ul>
<li>FSDP_FULL: Maximum memory savings but more communication (73 all-gathers)</li>
<li>FSDP_GRAD: Balanced savings with minimal communication (37 all-gathers)</li>
<li>Baseline 12GB → 8GB with FSDP_GRAD represents the sweet spot</li>
</ul></li>
</ol>
<p><strong>Memory Efficiency on H100:</strong></p>
<p>Peak usage of 12GB on 80GB H100 = <strong>15% utilization</strong></p>
<p>This leaves substantial headroom for:</p>
<ul>
<li>5x larger batch sizes (12GB × 5 = 60GB)</li>
<li>Larger models without changes</li>
<li>Activation checkpointing (if needed, could trade 20-30% speed for 50% memory savings)</li>
</ul>
</section>
</section>
<section id="profiling-results-compute-vs.-communication-breakdown" class="level2">
<h2 class="anchored" data-anchor-id="profiling-results-compute-vs.-communication-breakdown">Profiling Results: Compute vs.&nbsp;Communication Breakdown</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Understanding the Metrics
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Compute Time</strong> represents the duration spent executing actual computational kernels on the GPU - matrix multiplications, activations, and other operations that perform the forward and backward passes. <strong>Non-Compute Time</strong> (Communication Time) captures GPU communication kernels like NCCL all-reduce and all-gather operations, along with CPU-side overhead. <strong>Kernel Time</strong> is the total GPU active execution time (Compute + Non-Compute). <strong>Idle Time</strong> represents periods when the GPU is completely idle - waiting for other GPUs at synchronization barriers. The complete picture: <strong>Total Time = Kernel Time + Idle Time = Compute Time + Non-Compute Time + Idle Time</strong>.</p>
</div>
</div>
<section id="summary-comparison-table" class="level3">
<h3 class="anchored" data-anchor-id="summary-comparison-table">Summary Comparison Table</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 8%">
<col style="width: 13%">
<col style="width: 15%">
<col style="width: 19%">
<col style="width: 15%">
<col style="width: 6%">
<col style="width: 9%">
<col style="width: 12%">
</colgroup>
<thead>
<tr class="header">
<th>Strategy</th>
<th>Idle Time (ms)</th>
<th>Compute Time (ms)</th>
<th>Non-Compute Time (ms)</th>
<th>Kernel Time (ms)</th>
<th>Idle %</th>
<th>Compute %</th>
<th>Non-Compute %</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>BASELINE</strong></td>
<td>67.4</td>
<td>6,768.0</td>
<td>80.0</td>
<td>6,915.4</td>
<td>0.97%</td>
<td>97.87%</td>
<td>1.16%</td>
</tr>
<tr class="even">
<td><strong>DDP</strong></td>
<td>79.9</td>
<td>6,825.1</td>
<td>105.3</td>
<td>7,010.3</td>
<td>1.14%</td>
<td>97.36%</td>
<td>1.50%</td>
</tr>
<tr class="odd">
<td><strong>FSDP_FULL</strong></td>
<td>117.2</td>
<td>6,808.9</td>
<td>94.7</td>
<td>7,020.7</td>
<td>1.67%</td>
<td>96.98%</td>
<td>1.35%</td>
</tr>
<tr class="even">
<td><strong>FSDP_GRAD</strong></td>
<td>72.6</td>
<td>6,828.7</td>
<td>87.1</td>
<td>6,988.4</td>
<td>1.04%</td>
<td>97.71%</td>
<td>1.25%</td>
</tr>
</tbody>
</table>
<div data-align="center">
<em>Table 4: Temporal Breakdown of Training Time Across All Strategies (Rank 0 Only)</em>
</div>
</section>
<section id="key-metrics" class="level3">
<h3 class="anchored" data-anchor-id="key-metrics">Key Metrics</h3>
<p><strong>1. Compute Efficiency</strong> (Higher is better): - Baseline: 97.87% (reference) - FSDP_Grad: 97.71% (-0.16% vs baseline) ← <strong>Best distributed strategy</strong> - DDP: 97.36% (-0.51% vs baseline) - FSDP_Full: 96.98% (-0.89% vs baseline)</p>
<p><strong>2. Communication Overhead</strong> (Lower is better): - Baseline: 80.0 ms (memory transfers, no inter-GPU communication) - FSDP_Grad: 87.1 ms ← <strong>Best distributed strategy</strong> - FSDP_Full: 94.7 ms - DDP: 105.3 ms</p>
<p><strong>3. Idle Time</strong> (Lower is better): - Baseline: 67.4 ms - FSDP_Grad: 72.6 ms (+5.2ms) ← <strong>Best distributed strategy</strong> - DDP: 79.9 ms (+12.5ms) - FSDP_Full: 117.2 ms (+49.8ms)</p>
</section>
<section id="overhead-vs.-baseline" class="level3">
<h3 class="anchored" data-anchor-id="overhead-vs.-baseline">Overhead vs.&nbsp;Baseline</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 20%">
<col style="width: 22%">
<col style="width: 20%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th>Strategy</th>
<th>Additional Idle</th>
<th>Additional Comm</th>
<th>Total Overhead</th>
<th>Performance Loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>DDP</td>
<td>+12.5 ms</td>
<td>+25.3 ms</td>
<td>+94.9 ms</td>
<td>0.51%</td>
</tr>
<tr class="even">
<td>FSDP_FULL</td>
<td>+49.8 ms</td>
<td>+14.7 ms</td>
<td>+105.3 ms</td>
<td>0.89%</td>
</tr>
<tr class="odd">
<td>FSDP_GRAD</td>
<td>+5.2 ms</td>
<td>+7.1 ms</td>
<td>+73.0 ms</td>
<td>0.16%</td>
</tr>
</tbody>
</table>
<div data-align="center">
<em>Table 5: Additional Overhead Introduced by Distributed Strategies Compared to Baseline</em>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>FSDP_Grad achieves the <strong>lowest total overhead (73ms)</strong> and <strong>performance loss (0.16%)</strong>—only 5ms more idle time than baseline!</p>
</div>
</div>
</section>
</section>
<section id="communication-computation-overlap-analysis" class="level2">
<h2 class="anchored" data-anchor-id="communication-computation-overlap-analysis">Communication-Computation Overlap Analysis</h2>
<section id="overlap-percentage" class="level3">
<h3 class="anchored" data-anchor-id="overlap-percentage">Overlap Percentage</h3>
<p>The overlap metric indicates how much communication happens simultaneously with computation (higher is better):</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Strategy</th>
<th>Overlap %</th>
<th>Quality</th>
<th>Exposed Communication</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>DDP</strong></td>
<td>92.50%</td>
<td>Good</td>
<td>7.9 ms</td>
</tr>
<tr class="even">
<td><strong>FSDP_FULL</strong></td>
<td>95.14%</td>
<td>Excellent</td>
<td>4.6 ms</td>
</tr>
<tr class="odd">
<td><strong>FSDP_GRAD</strong></td>
<td>98.06%</td>
<td>Outstanding</td>
<td>1.7 ms</td>
</tr>
</tbody>
</table>
<div data-align="center">
<em>Table 6: Communication-Computation Overlap Efficiency (Rank 0 Only)</em>
</div>
<p><strong>What This Means:</strong></p>
<pre><code>Exposed Communication = Communication Time × (1 - Overlap %)</code></pre>
<ul>
<li><strong>DDP</strong>: 105.3ms × 7.5% = 7.9ms of communication that blocks computation</li>
<li><strong>FSDP_Full</strong>: 94.7ms × 4.86% = 4.6ms exposed</li>
<li><strong>FSDP_Grad</strong>: 87.1ms × 1.94% = 1.7ms exposed ← <strong>Minimal impact</strong></li>
</ul>
</section>
<section id="why-fsdp_grad-achieves-98-overlap" class="level3">
<h3 class="anchored" data-anchor-id="why-fsdp_grad-achieves-98-overlap">Why FSDP_Grad Achieves 98% Overlap</h3>
<ol type="1">
<li><strong>Fewer Synchronization Points</strong>: 37 all-gather calls vs.&nbsp;73 (FSDP_Full) or 109 (DDP)</li>
<li><strong>Larger Transfers, Better Pipelining</strong>: Larger transfers amortize communication setup costs</li>
<li><strong>Natural Overlap Opportunity</strong>: Gradient all-gather happens during optimizer step</li>
<li><strong>H100 NVLink Bandwidth</strong>: 900 GB/s enables simultaneous compute and communication</li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Industry Context
</div>
</div>
<div class="callout-body-container callout-body">
<p>Typical FSDP overlap on older hardware (V100/A100): 70-85%</p>
<p>Our results (95-98%) represent <strong>state-of-the-art distributed training efficiency</strong>!</p>
</div>
</div>
</section>
</section>
<section id="operator-level-profiling-what-changes-between-strategies" class="level2">
<h2 class="anchored" data-anchor-id="operator-level-profiling-what-changes-between-strategies">Operator-Level Profiling: What Changes Between Strategies</h2>
<section id="baseline-vs.-ddp-adding-gradient-synchronization" class="level3">
<h3 class="anchored" data-anchor-id="baseline-vs.-ddp-adding-gradient-synchronization">Baseline vs.&nbsp;DDP: Adding Gradient Synchronization</h3>
<p>DDP introduces 3 new operations for gradient all-reduce:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 26%">
<col style="width: 35%">
<col style="width: 16%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th>Operation</th>
<th>Duration (μs)</th>
<th>Count</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>ncclDevKernel_AllReduce_Sum_f32_RING_LL</code></td>
<td>43,392</td>
<td>109</td>
<td>All-reduce gradients across ranks</td>
</tr>
<tr class="even">
<td><code>ncclDevKernel_Broadcast_RING_LL</code></td>
<td>120</td>
<td>1</td>
<td>Broadcast initial state</td>
</tr>
<tr class="odd">
<td><code>CatArrayBatchedCopy</code></td>
<td>75</td>
<td>1</td>
<td>Concatenate gradient buckets</td>
</tr>
</tbody>
</table>
<div data-align="center">
<em>Table 7: New NCCL Communication Operations Introduced by DDP</em>
</div>
<p><strong>Total Communication</strong>: ~43.5ms for all-reduce operations</p>
<p><strong>Impact on Compute</strong>: - Memory copies: +2,732μs (gradient bucketing) - Vectorized kernels: +2,231μs (bucketing overhead) - GEMM operations: +2,170μs (slight increase from scheduling around communication)</p>
</section>
<section id="ddp-vs.-fsdp_full-switching-to-parameter-sharding" class="level3">
<h3 class="anchored" data-anchor-id="ddp-vs.-fsdp_full-switching-to-parameter-sharding">DDP vs.&nbsp;FSDP_Full: Switching to Parameter Sharding</h3>
<p>FSDP_Full replaces all-reduce with all-gather operations and introduces new memory management kernels:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 17%">
<col style="width: 20%">
<col style="width: 16%">
<col style="width: 19%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th>Operation</th>
<th>DDP Duration</th>
<th>FSDP Duration</th>
<th>Difference</th>
<th>Count Change</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>ncclDevKernel_AllGather_RING_LL</code></td>
<td>0</td>
<td>50,074μs</td>
<td>+50,074</td>
<td>+73 calls</td>
<td>✨ NEW</td>
</tr>
<tr class="even">
<td><code>void at::native::(anonymous namespace)::CatArrayBatchedCopy</code></td>
<td>0</td>
<td>2,511μs</td>
<td>+2,511</td>
<td>+37 calls</td>
<td>✨ NEW</td>
</tr>
<tr class="odd">
<td><code>void at::native::elementwise_kernel&lt;128, 2&gt;</code></td>
<td>0</td>
<td>1,879μs</td>
<td>+1,879</td>
<td>0→37 calls</td>
<td>📊 MODIFIED</td>
</tr>
</tbody>
</table>
<div data-align="center">
<em>Table 8: Operator Changes from DDP to FSDP_Full (Parameter Sharding)</em>
</div>
<p><strong>Operations Removed</strong>:</p>
<ul>
<li>❌ <code>ncclDevKernel_AllReduce_Sum_f32_RING_LL</code> (43,392μs, 109 calls) → Replaced by all-gather</li>
</ul>
<p><strong>Why More Communication?</strong></p>
<ul>
<li><strong>73 new all-gather calls</strong> for parameter reconstruction during forward/backward passes</li>
<li><strong>37 new CatArrayBatchedCopy calls</strong> for concatenating parameter shards</li>
<li><strong>37 new elementwise kernel invocations</strong> handle parameter sharding/unsharding overhead (1.9ms)</li>
</ul>
</section>
<section id="ddp-vs.-fsdp_grad-minimal-communication-pattern" class="level3">
<h3 class="anchored" data-anchor-id="ddp-vs.-fsdp_grad-minimal-communication-pattern">DDP vs.&nbsp;FSDP_Grad: Minimal Communication Pattern</h3>
<p>FSDP_Grad adds all-gather operations for gradient synchronization with minimal compute impact:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 16%">
<col style="width: 24%">
<col style="width: 15%">
<col style="width: 18%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th>Operation</th>
<th>DDP Duration</th>
<th>FSDP_GRAD Duration</th>
<th>Difference</th>
<th>Count Change</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>ncclDevKernel_AllGather_RING_LL</code></td>
<td>0</td>
<td>106,828μs</td>
<td>+106,828</td>
<td>+37 calls</td>
<td>✨ NEW</td>
</tr>
<tr class="even">
<td><code>void at::native::(anonymous namespace)::CatArrayBatchedCopy</code></td>
<td>0</td>
<td>2,515μs</td>
<td>+2,515</td>
<td>+37 calls</td>
<td>✨ NEW</td>
</tr>
<tr class="odd">
<td><code>sm80_xmma_gemm_f32f32_f32f32_f32_tn_n_tilesize1</code></td>
<td>varies</td>
<td>6,378μs</td>
<td>+6,378</td>
<td>0→varies</td>
<td>📊 MODIFIED</td>
</tr>
</tbody>
</table>
<div data-align="center">
<em>Table 9: Operator Changes from DDP to FSDP_Grad (Gradient Sharding Only)</em>
</div>
<p><strong>Operations Removed</strong>:</p>
<ul>
<li>❌ <code>ncclDevKernel_AllReduce_Sum_f32_RING_LL</code> (43,392μs, 109 calls) → Replaced by all-gather</li>
</ul>
<p><strong>Surprising Finding</strong>: FSDP_Grad all-gather duration (106.8ms) &gt; FSDP_Full (50.1ms)!</p>
<p><strong>Why?</strong></p>
<ul>
<li><strong>37 new all-gather calls</strong> for gradient + optimizer state synchronization (larger transfers per call)</li>
<li><strong>37 new CatArrayBatchedCopy calls</strong> for assembling gradient shards</li>
<li><strong>Modified GEMM operations</strong>: +6,378μs from memory bandwidth contention during gradient gathering</li>
</ul>
<p><strong>But FSDP_Grad still performs better due to 98% overlap</strong> (most of 106.8ms is hidden behind computation).</p>
</section>
<section id="communication-pattern-summary" class="level3">
<h3 class="anchored" data-anchor-id="communication-pattern-summary">Communication Pattern Summary</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 23%">
<col style="width: 21%">
<col style="width: 9%">
<col style="width: 18%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th>Strategy</th>
<th>Primary Operation</th>
<th>Total Duration</th>
<th>Calls</th>
<th>Avg per Call</th>
<th>Overlap %</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>DDP</td>
<td>All-Reduce</td>
<td>43.4ms</td>
<td>109</td>
<td>398μs</td>
<td>92.50%</td>
</tr>
<tr class="even">
<td>FSDP_FULL</td>
<td>All-Gather</td>
<td>50.1ms</td>
<td>73</td>
<td>686μs</td>
<td>95.14%</td>
</tr>
<tr class="odd">
<td>FSDP_GRAD</td>
<td>All-Gather</td>
<td>106.8ms</td>
<td>37</td>
<td>2,887μs</td>
<td>98.06%</td>
</tr>
</tbody>
</table>
<div data-align="center">
<em>Table 10: Summary of NCCL Communication Patterns Across All Distributed Strategies</em>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>The Paradox Explained
</div>
</div>
<div class="callout-body-container callout-body">
<pre><code>Communication Duration: FSDP_GRAD (106.8ms) &gt; FSDP_FULL (50.1ms) &gt; DDP (43.4ms)
BUT
Actual Performance:     FSDP_GRAD (97.71%) &gt; DDP (97.36%) &gt; FSDP_FULL (96.98%)</code></pre>
<p><strong>Why?</strong></p>
<ul>
<li>FSDP_Grad’s fewer, larger transfers pipeline better with computation</li>
<li>Happens during optimizer step (natural idle time)</li>
<li>Only ~1.7ms of exposed communication impacts performance</li>
<li><strong>Communication pattern and overlap matter more than raw duration</strong></li>
</ul>
</div>
</div>
<hr>
</section>
</section>
</section>
<section id="recommendations" class="level1">
<h1>Recommendations</h1>
<section id="key-findings" class="level2">
<h2 class="anchored" data-anchor-id="key-findings">Key Findings</h2>
<ol type="1">
<li><p><strong>FSDP-Grad achieves 97.71% compute efficiency</strong> — only 0.16% slower than single-GPU baseline. This is exceptional for distributed training!</p></li>
<li><p><strong>Communication pattern and overlap matter more than raw communication volume</strong> — FSDP-Grad’s 106.8ms of communication time outperforms DDP’s 43.4ms because it overlaps 98% of communication with computation, resulting in only 1.7ms of exposed overhead. This is the key insight that drives performance.</p></li>
<li><p><strong>All distributed strategies achieve excellent scaling</strong> — 1.9x speedup with 2 GPUs (95% scaling efficiency) demonstrates H100’s capabilities.</p></li>
<li><p><strong>Memory profiling reveals clean allocation patterns</strong> — No leaks, no fragmentation, 15% utilization leaves room for 5x larger batches.</p></li>
<li><p><strong>Per-layer FSDP wrapping is critical</strong> — Enables granular parameter gathering, better overlap through prefetching, and reduced peak memory.</p></li>
</ol>
</section>
<section id="strategy-selection-guide" class="level2">
<h2 class="anchored" data-anchor-id="strategy-selection-guide">Strategy Selection Guide</h2>
<section id="choose-fsdp-grad-zero-2-when" class="level3">
<h3 class="anchored" data-anchor-id="choose-fsdp-grad-zero-2-when">Choose FSDP-Grad (ZeRO-2) When:</h3>
<p>✅ Model fits on a single GPU but you want faster training</p>
<p>✅ You want best performance with memory savings</p>
<p>✅ Training models in 7B-70B parameter range</p>
<p>✅ You have high-bandwidth interconnect (NVLink, InfiniBand)</p>
<p>⭐ <strong>Recommendation</strong>: Default choice for most distributed training scenarios</p>
</section>
<section id="choose-fsdp-full-zero-3-when" class="level3">
<h3 class="anchored" data-anchor-id="choose-fsdp-full-zero-3-when">Choose FSDP-Full (ZeRO-3) When:</h3>
<p>✅ Model absolutely won’t fit with FSDP-Grad</p>
<p>✅ You need maximum memory savings (3x+)</p>
<p>✅ Training extremely large models (&gt;100B parameters)</p>
<p>⚠️ You can tolerate ~1% performance loss</p>
</section>
<section id="choose-ddp-when" class="level3">
<h3 class="anchored" data-anchor-id="choose-ddp-when">Choose DDP When:</h3>
<p>✅ Model comfortably fits in GPU memory</p>
<p>✅ You want simplest implementation</p>
<p>✅ Training smaller models (&lt;7B parameters)</p>
<p>✅ Debugging distributed training issues</p>
</section>
<section id="stay-with-single-gpu-when" class="level3">
<h3 class="anchored" data-anchor-id="stay-with-single-gpu-when">Stay with Single GPU When:</h3>
<p>✅ Model fits comfortably with room for larger batches</p>
<p>✅ You don’t need faster training</p>
<p>✅ Simplicity is paramount</p>
</section>
</section>
<section id="performance-optimization-insights" class="level2">
<h2 class="anchored" data-anchor-id="performance-optimization-insights">Performance Optimization Insights</h2>
<p><strong>Achieved Optimizations:</strong></p>
<ol type="1">
<li>✅ Gradient Accumulation with <code>no_sync()</code>: Reduces communication by 4x</li>
<li>✅ Per-Layer FSDP Wrapping: Enables parameter prefetching and better overlap</li>
<li>✅ NCCL Ring All-Reduce/All-Gather: Efficient collective communication</li>
<li>✅ NVLink Bandwidth: 900 GB/s enables near-perfect overlap</li>
</ol>
<p><strong>Future Optimization Opportunities:</strong></p>
<ol type="1">
<li><strong>Activation Checkpointing</strong>: Trade 20-30% compute for 50% memory savings</li>
<li><strong>Gradient Compression</strong>: Potential 2-4x communication reduction</li>
<li><strong>Pipeline Parallelism</strong>: For models &gt;100B parameters</li>
<li><strong>Flash Attention</strong>: Reduce activation memory, increase speed</li>
</ol>
</section>
<section id="scaling-beyond-2-gpus" class="level2">
<h2 class="anchored" data-anchor-id="scaling-beyond-2-gpus">Scaling Beyond 2 GPUs</h2>
<section id="expected-scaling-behavior" class="level3">
<h3 class="anchored" data-anchor-id="expected-scaling-behavior">Expected Scaling Behavior</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 7%">
<col style="width: 27%">
<col style="width: 17%">
<col style="width: 18%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>GPUs</th>
<th>FSDP_Grad Memory/GPU</th>
<th>DDP Speedup</th>
<th>FSDP Speedup</th>
<th>Communication Overhead</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>12 GB</td>
<td>1.0x</td>
<td>1.0x</td>
<td>0%</td>
</tr>
<tr class="even">
<td>2</td>
<td>8 GB</td>
<td>1.9x</td>
<td>1.9x</td>
<td>1.25%</td>
</tr>
<tr class="odd">
<td>4</td>
<td>6 GB</td>
<td>3.6x</td>
<td>3.7x</td>
<td>2.5% (estimated)</td>
</tr>
<tr class="even">
<td>8</td>
<td>4.5 GB</td>
<td>6.8x</td>
<td>7.2x</td>
<td>4.0% (estimated)</td>
</tr>
</tbody>
</table>
<div data-align="center">
<em>Table 11: Projected Scaling Behavior for GPT-2 Large Beyond 2 GPUs</em>
</div>
<p><strong>Scaling Considerations:</strong></p>
<ul>
<li>FSDP_Grad should scale better than DDP (lower per-GPU communication)</li>
<li>Communication overhead grows sub-linearly due to NCCL’s efficient algorithms</li>
<li>NVLink enables near-linear scaling up to 8 GPUs per node</li>
<li>Multi-node scaling requires InfiniBand for optimal performance</li>
</ul>
</section>
</section>
<section id="final-recommendations" class="level2">
<h2 class="anchored" data-anchor-id="final-recommendations">Final Recommendations</h2>
<section id="for-this-experiment-gpt-2-large-2x-h100" class="level3">
<h3 class="anchored" data-anchor-id="for-this-experiment-gpt-2-large-2x-h100">For This Experiment (GPT-2 Large, 2x H100)</h3>
<p><strong>Use FSDP_Grad</strong> with the current configuration:</p>
<ul>
<li>Global batch size: 32</li>
<li>Micro batch size per GPU: 8</li>
<li>Gradient accumulation: 2 steps</li>
<li>Per-layer wrapping: Enabled</li>
<li>Sharding strategy: <code>SHARD_GRAD_OP</code></li>
</ul>
<p><strong>Why?</strong></p>
<ul>
<li>97.71% compute efficiency (essentially matching baseline)</li>
<li>98.06% overlap (only 1.7ms exposed communication)</li>
<li>1.5x memory reduction (room to scale up)</li>
<li>Best overall performance-memory trade-off</li>
</ul>
</section>
<section id="monitoring-metrics" class="level3">
<h3 class="anchored" data-anchor-id="monitoring-metrics">Monitoring Metrics</h3>
<p>Track these metrics to ensure optimal performance:</p>
<ol type="1">
<li><strong>Compute Efficiency</strong>: Should remain &gt;97%</li>
<li><strong>Communication Overlap</strong>: Should stay &gt;95%</li>
<li><strong>Idle Time</strong>: Should be &lt;100ms per iteration</li>
<li><strong>Memory Utilization</strong>: Can increase batch size if needed</li>
</ol>
</section>
<section id="configuration-checklist" class="level3">
<h3 class="anchored" data-anchor-id="configuration-checklist">Configuration Checklist</h3>
<ul class="task-list">
<li><label><input type="checkbox" checked="">Use NCCL backend for distributed communication</label></li>
<li><label><input type="checkbox" checked="">Enable per-layer FSDP wrapping</label></li>
<li><label><input type="checkbox" checked="">Use <code>no_sync()</code> for gradient accumulation</label></li>
<li><label><input type="checkbox" checked="">Set deterministic seeds across ranks for reproducibility</label></li>
<li><label><input type="checkbox" checked="">Profile with PyTorch Profiler to verify overlap</label></li>
<li><label><input type="checkbox" checked="">Monitor memory usage to avoid OOM</label></li>
</ul>
<hr>
</section>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>This comprehensive experimental analysis of distributed training strategies demonstrates that <strong>FSDP with gradient sharding (ZeRO-2)</strong> achieves exceptional performance on modern GPU hardware. With <strong>97.71% compute efficiency</strong> and <strong>98.06% communication-computation overlap</strong>, FSDP_Grad delivers near-baseline performance while enabling memory savings and faster training through data parallelism.</p>
<p>The experiments demonstrate state-of-the-art distributed training efficiency, leveraging H100’s NVLink bandwidth, PyTorch’s optimized FSDP implementation, and careful architectural choices like per-layer wrapping and conditional gradient synchronization. These results provide a strong foundation for scaling to larger models and multi-node clusters while maintaining excellent performance.</p>
<p><strong>Bottom Line</strong>: For distributed training on H100 clusters, <strong>FSDP_Grad is the recommended approach to begin with</strong>, delivering 97.71% of single-GPU performance with memory savings and near-linear speedup.</p>
<hr>
</section>
<section id="appendix-running-the-experiments" class="level1">
<h1>Appendix: Running the Experiments</h1>
<section id="single-gpu-baseline" class="level3">
<h3 class="anchored" data-anchor-id="single-gpu-baseline">Single GPU Baseline</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb20"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Memory analysis</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="va">CUDA_VISIBLE_DEVICES</span><span class="op">=</span>0 <span class="ex">uv</span> run python memory_analysis.py</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Throughput measurement</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="va">CUDA_VISIBLE_DEVICES</span><span class="op">=</span>0 <span class="ex">uv</span> run python throughput.py</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Training</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="va">CUDA_VISIBLE_DEVICES</span><span class="op">=</span>0 <span class="ex">uv</span> run python train_baseline.py</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="ddp-training" class="level3">
<h3 class="anchored" data-anchor-id="ddp-training">DDP Training</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb21"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="ex">uv</span> run torchrun <span class="at">--nproc_per_node</span><span class="op">=</span>2 train_ddp.py</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="fsdp-training" class="level3">
<h3 class="anchored" data-anchor-id="fsdp-training">FSDP Training</h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb22"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># FSDP-Full (ZeRO-3)</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="ex">uv</span> run torchrun <span class="at">--nproc_per_node</span><span class="op">=</span>2 train_fsdp.py <span class="at">--strategy</span> FULL_SHARD</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co"># FSDP-Grad (ZeRO-2)</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="ex">uv</span> run torchrun <span class="at">--nproc_per_node</span><span class="op">=</span>2 train_fsdp.py <span class="at">--strategy</span> SHARD_GRAD_OP</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Resources
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Code Repository</strong>: Full implementation available on <a href="https://github.com/daddyofadoggy/pytorch-distributed">GitHub</a></p>
<p><strong>Further Reading</strong>:</p>
<ul>
<li><a href="https://pytorch.org/docs/stable/fsdp.html">PyTorch FSDP Documentation</a></li>
<li><a href="https://arxiv.org/abs/1910.02054">ZeRO Paper (Microsoft Research)</a></li>
<li><a href="https://pytorch.org/memory_viz">PyTorch Memory Visualizer</a></li>
<li><a href="https://docs.nvidia.com/deeplearning/nccl/">NCCL Documentation</a></li>
</ul>
</div>
</div>
<hr>
<p><em>Questions or comments? We’d love to hear about your experiences with distributed training! Feel free to reach out or leave a comment below.</em></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/your-website-url\.example\.com");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>